[{"content":"","date":"12 September 2023","permalink":"/tags/bpftrace/","section":"Tags","summary":"","title":"bpftrace"},{"content":"","date":"12 September 2023","permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"","date":"12 September 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"12 September 2023","permalink":"/tags/tcp/","section":"Tags","summary":"","title":"TCP"},{"content":"🙋‍♂️Hello,我是👨‍💻FeixiangFu\n🎮资深DOTA2云玩家🇨🇳CN Dota Best Dota💯💯💯\n🤝感兴趣可以一起交流学习 😊\n","date":"12 September 2023","permalink":"/","section":"Welcome to feixiang's blog 🎉","summary":"🙋‍♂️Hello,我是👨‍💻FeixiangFu\n🎮资深DOTA2云玩家🇨🇳CN Dota Best Dota💯💯💯\n🤝感兴趣可以一起交流学习 😊","title":"Welcome to feixiang's blog 🎉"},{"content":"","date":"12 September 2023","permalink":"/posts/","section":"博客","summary":"","title":"博客"},{"content":"背景 # 服务pod实例访问对方接口频繁超时，引起客诉。我们通过专线与对方机房互联，并且专线接入侧部署有专用物理设备(vpn加密通道)，架构图如下 问题分析 # 客户端请求抓包 # 对服务pod实例抓包分析，发现超时的http请求均为发送syn包无响应导致tcp连接建立失败超时。同时专线网络监控比较稳，可以排除专线网络丢包的原因 对方服务器抓包 # 联系对方协助在server机器上抓包，分析对方提供的抓包文件:\n客户端请求经过防火墙SNAT后，对方server看到的客户端IP都一样 服务器上收到了SYN包，但是回复的是ACK包，而不是SYN+ACK包 超时重传都发生在客户端请求源端口复用时(十几秒~九十几秒) client复用端口时，对比server发送的ACK包和相同五元组TCP连接断开时发送的最后一个ACK包，Acknowledgment number是相同的 HTTP response header中包含Connection: close，server只支持http1.0短连接，客户端请求完成后server主动断开TCP连接并进入TIME-WAIT状态 根据以上分析推断，对于新请求server端复用了之前的TCP连接，TCP连接处于TIME-WAIT状态\n还有一个问题，server回复了ACK包，客户端为什么没有收到？ 通过在专用设备上抓包分析，发现收到了ACK回包，推测是被防火墙drop。因为防火墙TCP连接TIME-WAIT状态timeout时间为1s，对于客户端新请求SYN包，会新建tcp session，收到server ACK回包后，认为非法就drop了\n解决方案 # 支持HTTP1.1长连接 # 接口支持http1.1长连接，客户端请求完成后会主动断开TCP连接，TIME-WAIT状态维持在客户端。对方反馈无法支持\n增加防火墙SNAT地址池 # 当前业务的防火墙策略的SNAT IP Pool只有一个3.3.10.10，尝试再增加一个3.3.10.11，发现防火墙并没有轮询使用。查看官方文档，由于当前在用的IP Pool类型为overload，会根据客户端IP分配snat地址。比起巧合的是当前客户端四个pod ip 计算出来都会使用3.3.10.10。所以客户端pod IP随机分配，最终使用snat地址也是不确定的，考虑把SNAT IP Pool切换为 one-to-one类型\none-to-one模式是: 内部 IP 地址和外部（转换后的）IP 地址一对一匹配\n配置一个28位的新地址池之后，超时请求率大幅降低。但是one-to-one IP pool并不是静态SNAT，所以每次客户端请求结束后，会释放snat IP到池子里，还是有一定几率超时。\n如果要彻底解决，需要物理机或虚机部署固定IP，然后配置防火墙配置静态SNAT\n内核源码看TIME_WAIT连接处理 # 为什么会返回ACK? 从内核源码中找答案，内核版本3.10.0-1062\ntcp_v4_rcv() # tcp_v4_rcv函数负责接收和处理传入的TCP ipv4 数据包\nint tcp_v4_rcv(struct sk_buff *skb) { const struct iphdr *iph; const struct tcphdr *th; struct sock *sk; /* 中间省略 */ process: // TCP连接处于TIMT_WAIT状态时，跳转到do_time_wait if (sk-\u0026gt;sk_state == TCP_TIME_WAIT) goto do_time_wait; /* 中间省略 */ do_time_wait: /* 中间省略 */ // 根据tcp_timewait_state_process函数返回值，确定下一步动作 switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) { // 合法SYN包，可以复用TCP连接，tcp状态可以切换至SYN_RECV case TCP_TW_SYN: { struct sock *sk2 = inet_lookup_listener(dev_net(skb-\u0026gt;dev), \u0026amp;tcp_hashinfo, iph-\u0026gt;saddr, th-\u0026gt;source, iph-\u0026gt;daddr, th-\u0026gt;dest, inet_iif(skb)); if (sk2) { inet_twsk_deschedule(inet_twsk(sk), \u0026amp;tcp_death_row); inet_twsk_put(inet_twsk(sk)); sk = sk2; goto process; } /* Fall through to ACK */ } /* 发送ACK包 1.TCP_FIN_WAIT2状态收到FIN包，发送最后一次挥手的ACK包 2.TCP_TIME_WAIT状态下发送上一次的ACK包 */ case TCP_TW_ACK: tcp_v4_timewait_ack(sk, skb); break; // 发送RESET包 case TCP_TW_RST: tcp_v4_send_reset(sk, skb); inet_twsk_deschedule(inet_twsk(sk), \u0026amp;tcp_death_row); inet_twsk_put(inet_twsk(sk)); goto discard_it; // 直接丢弃，不做处理 case TCP_TW_SUCCESS:; } goto discard_it; } 结合上面的场景，server处理流程应该是: 收到SYN包，tcp状态为TCP_TIME_WAIT，tcp_timewait_state_process()函数处理并返回了TCP_TW_ACK\ntcp_timewait_state_process() # tcp_timewait_state_process() 函数处理\ntcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb, const struct tcphdr *th) { struct tcp_options_received tmp_opt; struct tcp_timewait_sock *tcptw = tcp_twsk((struct sock *)tw); bool paws_reject = false; tmp_opt.saw_tstamp = 0; // tcp header中开启了options且当前tcp连接开启了时间戳选项 if (th-\u0026gt;doff \u0026gt; (sizeof(*th) \u0026gt;\u0026gt; 2) \u0026amp;\u0026amp; tcptw-\u0026gt;tw_ts_recent_stamp) { tcp_parse_options(skb, \u0026amp;tmp_opt, 0, NULL); if (tmp_opt.saw_tstamp) { if (tmp_opt.rcv_tsecr) tmp_opt.rcv_tsecr -= tcptw-\u0026gt;tw_ts_offset; tmp_opt.ts_recent\t= tcptw-\u0026gt;tw_ts_recent; tmp_opt.ts_recent_stamp\t= tcptw-\u0026gt;tw_ts_recent_stamp; /* 如果开启了TCP timestamp可选项，判断是否发生时间戳回绕 1. paws_reject = false 时间戳没有回绕 2. paws_reject = true 时间戳回绕 */ paws_reject = tcp_paws_reject(\u0026amp;tmp_opt, th-\u0026gt;rst); } } // tcp状态是FIN_WAIT2的处理逻辑 if (tw-\u0026gt;tw_substate == TCP_FIN_WAIT2) { /* 中间省略 */ } // 时间戳没有回绕 if (!paws_reject \u0026amp;\u0026amp; // 报文序列号 等于 预期接收的序列号 (TCP_SKB_CB(skb)-\u0026gt;seq == tcptw-\u0026gt;tw_rcv_nxt \u0026amp;\u0026amp; /* TCP_SKB_CB(skb)-\u0026gt;end_seq = (TCP_SKB_CB(skb)-\u0026gt;seq + th-\u0026gt;syn + th-\u0026gt;fin + skb-\u0026gt;len - th-\u0026gt;doff * 4); 没有携带数据报文，只有tcp header且没有syn/fin标志位，只能是reset或者ack */ (TCP_SKB_CB(skb)-\u0026gt;seq == TCP_SKB_CB(skb)-\u0026gt;end_seq || th-\u0026gt;rst))) { /* In window segment, it may be only reset or bare ack. */ if (th-\u0026gt;rst) { // net.ipv4.tcp_rfc1337内核参数等于0时 if (sysctl_tcp_rfc1337 == 0) { kill: //清理TIME_WAIT连接 inet_twsk_deschedule(tw, \u0026amp;tcp_death_row); inet_twsk_put(tw); return TCP_TW_SUCCESS; } } else { // 重置TIME_WAIT状态timeout时间 inet_twsk_schedule(tw, \u0026amp;tcp_death_row, TCP_TIMEWAIT_LEN, TCP_TIMEWAIT_LEN); } if (tmp_opt.saw_tstamp) { tcptw-\u0026gt;tw_ts_recent\t= tmp_opt.rcv_tsval; tcptw-\u0026gt;tw_ts_recent_stamp = get_seconds(); } inet_twsk_put(tw); return TCP_TW_SUCCESS; } // SYN包且没有rst/ack标志位，时间戳没有回绕 if (th-\u0026gt;syn \u0026amp;\u0026amp; !th-\u0026gt;rst \u0026amp;\u0026amp; !th-\u0026gt;ack \u0026amp;\u0026amp; !paws_reject \u0026amp;\u0026amp; // 序列号没有回绕 或者 (after(TCP_SKB_CB(skb)-\u0026gt;seq, tcptw-\u0026gt;tw_rcv_nxt) || // 请求报文中开启timestamp可选项并且时间戳没有回绕 (tmp_opt.saw_tstamp \u0026amp;\u0026amp; (s32)(tcptw-\u0026gt;tw_ts_recent - tmp_opt.rcv_tsval) \u0026lt; 0))) { u32 isn = tcptw-\u0026gt;tw_snd_nxt + 65535 + 2; if (isn == 0) isn++; TCP_SKB_CB(skb)-\u0026gt;tcp_tw_isn = isn; return TCP_TW_SYN; } if (paws_reject) // 更新 /proc/net/netstat NET_INC_STATS_BH(twsk_net(tw), LINUX_MIB_PAWSESTABREJECTED); if (!th-\u0026gt;rst) { /* In this case we must reset the TIMEWAIT timer. * * If it is ACKless SYN it may be both old duplicate * and new good SYN with random sequence number \u0026lt;rcv_nxt. * Do not reschedule in the last case. */ if (paws_reject || th-\u0026gt;ack) // 重置TIME_WAIT状态timeout时间 inet_twsk_schedule(tw, \u0026amp;tcp_death_row, TCP_TIMEWAIT_LEN, TCP_TIMEWAIT_LEN); /* if (*last_oow_ack_time) { s32 elapsed = (s32)(tcp_time_stamp - *last_oow_ack_time); // 计算当前的tcp timestamp(jiffies)和上次发送ack时的tcp timestamp 差值 // 如果小于内核参数net.ipv4.tcp_invalid_ratelimit 表示速率过快，需要限速 if (0 \u0026lt;= elapsed \u0026amp;\u0026amp; elapsed \u0026lt; sysctl_tcp_invalid_ratelimit) { // 更新 /proc/net/netstat NET_INC_STATS(net, mib_idx); return true; } } 如果dupacks没有限速 return TCP_TW_ACK，否则return TCP_TW_SUCCESS */ return tcp_timewait_check_oow_rate_limit( tw, skb, LINUX_MIB_TCPACKSKIPPEDTIMEWAIT); } inet_twsk_put(tw); return TCP_TW_SUCCESS; } th-\u0026gt;doff 数据偏移字段，4-bit的无符号整型(数据范围0~15) 表示tcp header的长度，单位是32-bit(即4 bytes)，范围5~15 tcp header(no options)长度为5，大小 5 * 32-bit (4 bytes) = 20 bytes\nsizeof(*th) \u0026raquo; 2 tcphdr结构体大小是20 bytes，20 \u0026raquo; 2 = 20 / 4 = 5\n时间戳回绕判断 # static inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt, int rst) { // tcp_paws_check()函数判断是否通过检查 if (tcp_paws_check(rx_opt, 0)) return false; if (rst \u0026amp;\u0026amp; get_seconds() \u0026gt;= rx_opt-\u0026gt;ts_recent_stamp + TCP_PAWS_MSL) return false; return true; } static inline bool tcp_paws_check(const struct tcp_options_received *rx_opt, int paws_win) { if ((s32)(rx_opt-\u0026gt;ts_recent - rx_opt-\u0026gt;rcv_tsval) \u0026lt;= paws_win) return true; if (unlikely(get_seconds() \u0026gt;= rx_opt-\u0026gt;ts_recent_stamp + TCP_PAWS_24DAYS)) return true; if (!rx_opt-\u0026gt;ts_recent) return true; return false; } (s32)(rx_opt-\u0026gt;ts_recent - rx_opt-\u0026gt;rcv_tsval) \u0026lt;= 0 当前tcp连接server上一次发送的tcp报文中的TSecr和tcp报文选中的TSval的差值小于等于0，检查通过\ntcp timestamp是u32无符号整型，数据范围0~2^32-1 s32是有符合整型，数据范围是-2^31~2^31-1 u32无符号整型减法，如果|x|\u0026gt;y，则y-x = 2^32-|x-y| 对u32差值做强制类型转换的话，如果超过2^31就会溢出。所以rcv_tsval - ts_recent \u0026gt; 2^31时，时间戳回绕检查不会通过 tcp timestamp取值来源于jiffies，和服务器运行时长有关，记录的是从开机到现在时间戳时钟更新了多少次，rfc规定更新频率是1ms~1s，服务器重启后jiffies重置\nCentos中查看内核时间戳时钟频率，1s/1000=1ms，那么2^31/1000/(3600*24)=24.8天后回绕\n$ cat /boot/config-3.10.0-1062.el7.x86_64 |grep -w \u0026#34;CONFIG_HZ\u0026#34; CONFIG_HZ=1000 get_seconds() \u0026gt;= rx_opt-\u0026gt;ts_recent_stamp + TCP_PAWS_24DAYS\n#define TCP_PAWS_24DAYS (60 * 60 * 24 * 24) ，单位是s\n当前系统时间戳在tcp连接上一次收到rcv_tsval时的系统时间戳的24天之后的话，则时间戳回绕检查通过\n时钟频率是1ms，那么rcv_tsval增加到溢出的时间是2^31/1000/(3600*24)=24.8天\n所以增加了一个超过TCP_PAWS_24DAYS的判断，直接检查通过\n序列号回绕判断 # 对于SYN包，会比较tcp报文的序列号是否大于tcp连接记录的要预期接收的序列号\nafter(TCP_SKB_CB(skb)-\u0026gt;seq, tcptw-\u0026gt;tw_rcv_nxt)\n// after函数定义 static inline bool before(__u32 seq1, __u32 seq2) { return (__s32)(seq1-seq2) \u0026lt; 0; } #define after(seq2, seq1) before(seq1, seq2) (__s32)(seq1-seq2) 也是对u32差值强制类型转换，同样存在溢出情况。所以如果SYN包的seq比tcptw-\u0026gt;tw_rcv_nxt大超过2^31，则认为序列号回绕，不能复用TIME_WAIT连接\n结合上文抓包分析,\nseq=2123642848， ack=554887750\nts_recent = 869333586，rcv_tsval = 28962444\n代码运行结果显示，时间戳发生回绕，所以paws_reject = true，返回了TCP_TW_ACK\n问题场景模拟 # 测试环境搭建 # 服务器都是基于同一物理机上kvm虚拟化部署，内核版本 3.10.0-1062.el7.x86_64\nclient 请求 http://172.16.10.8:80 ,路由转发到firewall，filrewall服务器上使用iptables配置snat和dnat，再路由转发到gateway，gateway再转发到server\nrole device ip special route system client-1 eth0 172.16.9.131 172.16.10.8 via 172.16.9.133 dev eth0 centos 7.7 client-2 eth0 172.16.9.132 172.16.10.8 via 172.16.9.133 dev eth0 centos 7.7 firewall eth0 172.16.9.133 centos 7.7 eth1 192.168.102.3 192.168.10.9 via 192.168.102.2 dev eth1 physical gw\n(专用设备) eth0 192.168.102.2 3.3.10.10 via 192.168.102.3 dev eth0 centos 7.7 eth1 192.168.10.8 server eth0 192.168.10.9 3.3.10.10 via 192.168.10.8 dev eth0 centos 7.7 firewall配置 # iptables配置snat和dnat\n#dnat 收到访问172.16.10.8:80请求路由转发到192.168.10.9:80 $ iptables -t nat -A PREROUTING -d 172.16.10.8/32 -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 192.168.10.9:80 #snat 访问192.168.10.9:80的请求，源地址转换为3.3.10.10 $ iptables -t nat -A POSTROUTING -d 192.168.10.9/32 -o eth1 -p tcp -m tcp --dport 80 -j SNAT --to-source 3.3.10.10 内核参数\n$ sysctl -w net.ipv4.ip_forward=1 # 参考物理防火墙配置 $ systcl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=2 gateway配置 # 专用设备是用于双方建立vpn加密通道，基于iptables对请求打标记、转发到vpn网卡。测试环境只加了conntrack记录\n$ iptables -t nat -A PREROUTING -m conntrack --ctstate INVALID -j LOG --log-prefix \u0026#34;Conntrack INVALID: \u0026#34; --log-level 7 # conntrack日志记录文件 $ grep conntrack /etc/rsyslog.conf kern.* /var/log/conntrack.log $ sysctl -w net.ipv4.ip_forward=1 # 参照专用设备配置 $ systcl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=60 server配置 # # SimpleHTTPServer默认是HTTP/1.0 $ nohup python -m SimpleHTTPServer 80 \u0026amp; 问题复现 # client-1请求 # [root@node1] ~$ curl --local-port 54321 172.16.10.8:80 \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 3.2 Final//EN\u0026#34;\u0026gt;\u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Directory listing for /\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Directory listing for /\u0026lt;/h2\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;nohup.out\u0026#34;\u0026gt;nohup.out\u0026lt;/a\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;test.file\u0026#34;\u0026gt;test.file\u0026lt;/a\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 同时server抓包\n[root@localhost] $ tcpdump -ieth0 port 80 -nnS 17:08:16.798635 IP 3.3.10.10.54321 \u0026gt; 192.168.10.9.80: Flags [S], seq 2851797271, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 9], length 0 17:08:16.798704 IP 192.168.10.9.80 \u0026gt; 3.3.10.10.54321: Flags [S.], seq 912495348, ack 2851797272, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 9], length 0 ...省略... 17:08:16.806065 IP 3.3.10.10.54321 \u0026gt; 192.168.10.9.80: Flags [F.], seq 2851797347, ack 912495758, win 60, length 0 17:08:16.806073 IP 192.168.10.9.80 \u0026gt; 3.3.10.10.54321: Flags [.], ack 2851797348, win 58, length 0 client-2模拟请求 # server端的抓包记录可以看出，tcptw-\u0026gt;tw_rcv_nxt = 2851797348。使用python scapy，发送一个seq小于2851797348的SYN包\n[root@node2] /data0/fake$ python send_test.py 2851797340 [root@node2] /data0/fake$ cat send_test.py #!/usr/bin/python #-*- coding:utf-8 -*- from scapy.all import * import sys if len(sys.argv) \u0026lt; 2: print \u0026#39;lack args\u0026#39; sys.exit() # 创建一个IP数据包，设置源IP、目标IP ip_packet = IP(src=\u0026#34;172.16.9.132\u0026#34;, dst=\u0026#34;172.16.10.8\u0026#34;) # 创建一个TCP数据包，设置源端口、目标端口和序列号 tcp_packet = TCP(sport=54321, dport=80,flags=\u0026#39;S\u0026#39;, seq=int(sys.argv[1])) # 组装数据包 syn = ip_packet / tcp_packet send(syn)server端抓包记录可以看到，回复了seq为2851797348ACK包，而不是SYN+ACK 17:08:24.023783 IP 3.3.10.10.54321 \u0026gt; 192.168.10.9.80: Flags [S], seq 2851797340, win 8192, length 0 17:08:24.023842 IP 192.168.10.9.80 \u0026gt; 3.3.10.10.54321: Flags [.], ack 2851797348, win 58, length 0 server端使用bpftrace脚本查看tcp_timewait_state_process()函数收到SYN包的内核处理流程\n[root@localhost] $ bpftrace trace-time_wait.bt Attaching 4 probes... tcp_timewait_state_process: 3.3.10.10:54321 -\u0026gt; 192.168.10.9:80 [syn:1,ack:0,fin:0,rst:0], seq:1351613457, ack:0, win:32120, tw_rcv_nxt:1125813282 tw_substate: TCP_TIME_WAIT tcp_tw_status: TCP_TW_SYN ################################ tcp sock处于TCP_TIME_WAIT状态，收到的SYN包seq=1351613457 \u0026gt; tw_rcv_nxt=1125813282，函数返回TCP_TW_SYN\n问题延伸 # 客户端发送SYN包后，收到ACK包会怎么处理?\n查看内核源码，客户端发送SYN后，状态切换为SYN_SENT。收到tcp回包后的处理流程: tcp_v4_rcv -\u0026gt; tcp_v4_do_rcv -\u0026gt; tcp_rcv_state_process -\u0026gt; tcp_rcv_synsent_state_process\n看一下tcp_rcv_synsent_state_process函数，如果收到回包设置了ACK标志位，判断该数据包seq和timestamp是否符合预期\n(!after(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_una)\nsnd_una是对应发送窗口记录的未被确认的seq号，如果ack_seq =\u0026lt; snd_una，返回reset\nafter(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_nxt))\nsnd_nxt是对应发送窗口记录的下一次发送的seq号，如果ack_seq \u0026gt; snd_nxt，返回reset\n检查通过后，如果数据包设置了RST标志位，直接丢弃，tcp状态切换为CLOSE。如果没有设置，继续检查是否设置了SYN标志位，如果没有，直接丢弃。\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th, unsigned int len) { struct inet_connection_sock *icsk = inet_csk(sk); struct tcp_sock *tp = tcp_sk(sk); struct tcp_fastopen_cookie foc = { .len = -1 }; int saved_clamp = tp-\u0026gt;rx_opt.mss_clamp; tcp_parse_options(skb, \u0026amp;tp-\u0026gt;rx_opt, 0, \u0026amp;foc); if (tp-\u0026gt;rx_opt.saw_tstamp \u0026amp;\u0026amp; tp-\u0026gt;rx_opt.rcv_tsecr) tp-\u0026gt;rx_opt.rcv_tsecr -= tp-\u0026gt;tsoffset; if (th-\u0026gt;ack) { /* rfc793: * \u0026#34;If the state is SYN-SENT then * first check the ACK bit * If the ACK bit is set *\tIf SEG.ACK =\u0026lt; ISS, or SEG.ACK \u0026gt; SND.NXT, send * a reset (unless the RST bit is set, if so drop * the segment and return)\u0026#34; */ if (!after(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_una) || after(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_nxt)) goto reset_and_undo; if (tp-\u0026gt;rx_opt.saw_tstamp \u0026amp;\u0026amp; tp-\u0026gt;rx_opt.rcv_tsecr \u0026amp;\u0026amp; !between(tp-\u0026gt;rx_opt.rcv_tsecr, tp-\u0026gt;retrans_stamp, tcp_time_stamp)) { NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_PAWSACTIVEREJECTED); goto reset_and_undo; } /* Now ACK is acceptable. * * \u0026#34;If the RST bit is set * If the ACK was acceptable then signal the user \u0026#34;error: * connection reset\u0026#34;, drop the segment, enter CLOSED state, * delete TCB, and return.\u0026#34; */ if (th-\u0026gt;rst) { tcp_reset(sk); goto discard; } /* rfc793: * \u0026#34;fifth, if neither of the SYN or RST bits is set then * drop the segment and return.\u0026#34; * * See note below! * --ANK(990513) */ if (!th-\u0026gt;syn) goto discard_and_undo; ...省略... 构造请求 # 可以借助 Scapy_Tcpsession指定tcp sequence发送http请求，来复现\nclient-1 发送tcp seqence为1的http请求 [root@nod[root@node1] /data0$ python scapy_http.py 1 [root@nod[root@node1] /data0$ cat scapy_http.py #!/usr/bin/python #-*- coding:utf-8 -*- from Scapy_Tcpsession import TcpSession import sys if len(sys.argv) \u0026lt; 2: print \u0026#39;lack args\u0026#39; sys.exit() sess = TcpSession((\u0026#39;172.16.10.8\u0026#39;,80,int(sys.argv[1]),54321)) sess.connect() http_get=( \u0026#34;GET / HTTP/1.0\\r\\n\u0026#34; \u0026#34;Host: 172.16.10.8:80\\r\\n\u0026#34; \u0026#34;User-Agent: Python Scapy\\r\\n\u0026#34; \u0026#34;Connection: close\\r\\n\\r\\n\u0026#34; ) sess.send(http_get) client-2 请求172.16.10.8:80发现返回正常 [root@node2] ~$ curl --local-port 54321 172.16.10.8:80 、\u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD HTML 3.2 Final//EN\u0026#34;\u0026gt;\u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Directory listing for /\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Directory listing for /\u0026lt;/h2\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;nohup.out\u0026#34;\u0026gt;nohup.out\u0026lt;/a\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;test.file\u0026#34;\u0026gt;test.file\u0026lt;/a\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bpftrace跟踪内核处理过程 # 使用bpftrace工具观察下client-2 收到ACK包的处理流程\n[root@node2] /data0/kernel$ bpftrace synsent.bt #!/usr/bin/env bpftrace #include \u0026lt;linux/tcp.h\u0026gt; #include \u0026lt;net/tcp.h\u0026gt; BEGIN { @tcp_sk_status[2] = \u0026#34;TCP_SYN_SENT\u0026#34;; @tcp_sk_status[9] = \u0026#34;TCP_LAST_ACK\u0026#34;; } kprobe:tcp_rcv_state_process { $sk=(struct sock *)arg0; $tp=(struct tcp_sock *)$sk; $skb = (struct sk_buff *)arg1; $iphd = ((struct iphdr *)($skb-\u0026gt;head + $skb-\u0026gt;network_header)); $th = ((struct tcphdr *)($skb-\u0026gt;head + $skb-\u0026gt;transport_header)); $dport = $th-\u0026gt;dest; $dport = ($dport \u0026gt;\u0026gt; 8) | (($dport \u0026lt;\u0026lt; 8) \u0026amp; 0x00FF00); if ($dport == 54321) { $ack = $th-\u0026gt;ack_seq; $ack = ($ack \u0026gt;\u0026gt; 24) | (($ack \u0026amp; 0x00FF0000) \u0026gt;\u0026gt; 8) | (( $ack \u0026amp; 0x00FF00) \u0026lt;\u0026lt; 8) | (($ack \u0026amp; 0xFF) \u0026lt;\u0026lt; 24); $skc_state=$sk-\u0026gt;__sk_common.skc_state; printf(\u0026#34;%s:\\ntcp_sk_status: %s, syn:%u,ack:%u,fin:%u,ack_seq:%u, snd_una:%u, snd_nxt:%u\\n\u0026#34;,func,@tcp_sk_status[$skc_state],$th-\u0026gt;syn,$th-\u0026gt;ack,$th-\u0026gt;fin,$ack,$tp-\u0026gt;snd_una,$tp-\u0026gt;snd_nxt); $una = $tp-\u0026gt;snd_una - $ack; $int_una = (int32) $una; // (!after(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_una) if (!((int32)($tp-\u0026gt;snd_una - $ack) \u0026lt; 0)) { printf(\u0026#34;%d, SEG.ACK =\u0026lt; ISS\\n\u0026#34;,$int_una); } $nxt = $tp-\u0026gt;snd_nxt - $ack; $int_nxt = (int32) $nxt; // after(TCP_SKB_CB(skb)-\u0026gt;ack_seq, tp-\u0026gt;snd_nxt)) if ((int32)($tp-\u0026gt;snd_nxt - $ack) \u0026lt; 0) { printf(\u0026#34;%d, SEG.ACK \u0026gt; SND.NXT\\n\u0026#34;,$int_nxt); } } } END { clear(@tcp_sk_status); } [root@node2] /data0/kernel$ bpftrace synsent.bt tcp_rcv_state_process: tcp_sk_status: TCP_SYN_SENT, syn:0,ack:1,fin:0,ack_seq:88, snd_una:2387012236, snd_nxt:2387012237 -1907955147, SEG.ACK \u0026gt; SND.NXT tcp_rcv_state_process: tcp_sk_status: TCP_SYN_SENT, syn:1,ack:1,fin:0,ack_seq:2387012237, snd_una:2387012236, snd_nxt:2387012237 tcp_rcv_state_process: tcp_sk_status: TCP_LAST_ACK, syn:0,ack:1,fin:0,ack_seq:2387012313, snd_una:2387012312, snd_nxt:2387012313 可以看到，client-2收到server的ACK回包后\n内核tcp_rcv_state_process函数判断 ack_seq = 88 \u0026gt; snd_una = 2387012236 (u32差值强制int32转换，相差2^31溢出导致)，发送server reset数据包 server端收到reset包后，清理tcp TIME_WAIT连接 client-2重发syn包，server端返回syn+ack，握手成功 同样使用bpftrace工具观察server的处理过程，\n[root@localhost] $ cat trace-time_wait.bt #!/usr/bin/env bpftrace #include \u0026lt;linux/tcp.h\u0026gt; #include \u0026lt;net/tcp.h\u0026gt; #include \u0026lt;net/inet_timewait_sock.h\u0026gt; BEGIN { @tw_substate[5] = \u0026#34;TCP_FIN_WAIT2\u0026#34;; @tw_substate[6] = \u0026#34;TCP_TIME_WAIT\u0026#34;; @tcp_tw_status[0] = \u0026#34;TCP_TW_SUCCESS\u0026#34;; @tcp_tw_status[2] = \u0026#34;TCP_TW_ACK\u0026#34;; @tcp_tw_status[3] = \u0026#34;TCP_TW_SYN\u0026#34;; } kprobe:tcp_timewait_state_process { $inet = (struct inet_timewait_sock *)arg0; $tw = (struct tcp_timewait_sock *)$inet; $skb = (struct sk_buff *)arg1; $th = (struct tcphdr *)arg2; $iphd = ((struct iphdr *)($skb-\u0026gt;head + $skb-\u0026gt;network_header)); if ($iphd-\u0026gt;protocol == IPPROTO_TCP) { $srcaddr = $iphd-\u0026gt;saddr; $dstaddr = $iphd-\u0026gt;daddr; $srcip = ntop($iphd-\u0026gt;saddr); $dstip = ntop($iphd-\u0026gt;daddr); // $th = ((struct tcphdr *)($skb-\u0026gt;head + $skb-\u0026gt;transport_header)); $dport = $th-\u0026gt;dest; $dport = ($dport \u0026gt;\u0026gt; 8) | (($dport \u0026lt;\u0026lt; 8) \u0026amp; 0x00FF00); if ($dport == 80) { $sport = $th-\u0026gt;source; $sport = ($sport \u0026gt;\u0026gt; 8) | (($sport \u0026lt;\u0026lt; 8) \u0026amp; 0x00FF00); // $st = *$th; $seq = $th-\u0026gt;seq; $seq = ($seq \u0026gt;\u0026gt; 24) | (($seq \u0026amp; 0x00FF0000) \u0026gt;\u0026gt; 8) | (( $seq \u0026amp; 0x00FF00) \u0026lt;\u0026lt; 8) | (($seq \u0026amp; 0xFF) \u0026lt;\u0026lt; 24); $ack = $th-\u0026gt;ack_seq; $ack = ($ack \u0026gt;\u0026gt; 24) | (($ack \u0026amp; 0x00FF0000) \u0026gt;\u0026gt; 8) | (( $ack \u0026amp; 0x00FF00) \u0026lt;\u0026lt; 8) | (($ack \u0026amp; 0xFF) \u0026lt;\u0026lt; 24); $win = $th-\u0026gt;window; $win = ($win \u0026gt;\u0026gt; 8) | (($win \u0026lt;\u0026lt; 8) \u0026amp; 0x00FF00); $tw_rcv_nxt = $tw-\u0026gt;tw_rcv_nxt; printf(\u0026#34;%s:\\n\u0026#34;,func); printf(\u0026#34;%s:%d -\u0026gt; %s:%d\\n\u0026#34;,$srcip, $sport, $dstip, $dport); /* TCP_FIN_WAIT2状态收到fin包: tcptw-\u0026gt;tw_rcv_nxt = TCP_SKB_CB(skb)-\u0026gt;end_seq; 回复对方的ack包中 ack=tw_rcv_nxt=seq+1 TCP_SKB_CB(skb)-\u0026gt;end_seq = (TCP_SKB_CB(skb)-\u0026gt;seq + th-\u0026gt;syn + th-\u0026gt;fin + skb-\u0026gt;len - th-\u0026gt;doff * 4); */ printf(\u0026#34;[syn:%d,ack:%d,fin:%d,rst:%d], seq:%-u, ack:%u, win:%u, tw_rcv_nxt:%u\\n\u0026#34;,$th-\u0026gt;syn, $th-\u0026gt;ack, $th-\u0026gt;fin, $th-\u0026gt;rst, $seq, $ack, $win, $tw_rcv_nxt); // printf(\u0026#34;%d compare %d\\n\u0026#34;,$th-\u0026gt;doff,sizeof($st)\u0026gt;\u0026gt;2); $state = $inet-\u0026gt;tw_substate; $statestr = @tw_substate[$state]; printf(\u0026#34;tw_substate: %s\\n\u0026#34;,$statestr); } } } kretprobe:tcp_timewait_state_process { $ret = retval; $statestr = @tcp_tw_status[$ret]; printf(\u0026#34;tcp_tw_status: %s\\n\u0026#34;,$statestr); print(\u0026#34;################################\u0026#34;); } END { clear(@tw_substate); clear(@tcp_tw_status); } [root@localhost] $ bpftrace trace-time_wait.bt tcp_timewait_state_process: 3.3.10.10:54321 -\u0026gt; 192.168.10.9:80 [syn:1,ack:0,fin:0,rst:0], seq:2387012236, ack:0, win:32120, tw_rcv_nxt:88 tw_substate: TCP_TIME_WAIT tcp_tw_status: TCP_TW_ACK ################################ tcp_timewait_state_process: 3.3.10.10:54321 -\u0026gt; 192.168.10.9:80 [syn:0,ack:0,fin:0,rst:1], seq:88, ack:0, win:0, tw_rcv_nxt:88 tw_substate: TCP_TIME_WAIT tcp_tw_status: TCP_TW_SUCCESS ################################ 可以看到，server收到SYN包后\ntcp_timewait_state_process函数判断sequence回绕（seq \u0026lt; tw_rcv_nxt），返回给client-2 ACK包 收到client-2发送的reset包，直接清理tcp TIME_WAIT连接 测试环境搭建后发现server端TIME_WAIT状态持续时间为60s，这是因为内核版本的差异。参考这篇文章 从 Linux 源码看 TIME_WAIT 状态的持续时间\n","date":"12 September 2023","permalink":"/2023/09/timeout-time_wait/","section":"博客","summary":"背景 # 服务pod实例访问对方接口频繁超时，引起客诉。我们通过专线与对方机房互联，并且专线接入侧部署有专用物理设备(vpn加密通道)，架构图如下 问题分析 # 客户端请求抓包 # 对服务pod实例抓包分析，发现超时的http请求均为发送syn包无响应导致tcp连接建立失败超时。同时专线网络监控比较稳，可以排除专线网络丢包的原因 对方服务器抓包 # 联系对方协助在server机器上抓包，分析对方提供的抓包文件:\n客户端请求经过防火墙SNAT后，对方server看到的客户端IP都一样 服务器上收到了SYN包，但是回复的是ACK包，而不是SYN+ACK包 超时重传都发生在客户端请求源端口复用时(十几秒~九十几秒) client复用端口时，对比server发送的ACK包和相同五元组TCP连接断开时发送的最后一个ACK包，Acknowledgment number是相同的 HTTP response header中包含Connection: close，server只支持http1.0短连接，客户端请求完成后server主动断开TCP连接并进入TIME-WAIT状态 根据以上分析推断，对于新请求server端复用了之前的TCP连接，TCP连接处于TIME-WAIT状态\n还有一个问题，server回复了ACK包，客户端为什么没有收到？ 通过在专用设备上抓包分析，发现收到了ACK回包，推测是被防火墙drop。因为防火墙TCP连接TIME-WAIT状态timeout时间为1s，对于客户端新请求SYN包，会新建tcp session，收到server ACK回包后，认为非法就drop了\n解决方案 # 支持HTTP1.1长连接 # 接口支持http1.1长连接，客户端请求完成后会主动断开TCP连接，TIME-WAIT状态维持在客户端。对方反馈无法支持","title":"一次请求超时问题排查记录"},{"content":"","date":"16 August 2022","permalink":"/tags/kvm/","section":"Tags","summary":"","title":"KVM"},{"content":"1.资料准备 # 1.1 iso文件下载 # 可以通过访问官方页面下载( 链接)，但是下载速度比较慢。 我这里选择使用p2p下载工具mldonkey，Centos7系统安装可以参考 Centos7部署mldonkey，iso文件e2dk下载链接可以从 msdn.itellyou.cn网站获取\ned2k://|file|cn_windows_server_2019_updated_march_2019_x64_dvd_c1ffb46c.iso|5347280896|49FCF8C558517608537E0396854560D6|/ 1.2 virtio驱动程序下载 # kvm安装运行windows server系统需要安装磁盘scsi、网卡驱动\nyum安装 wget https://fedorapeople.org/groups/virt/virtio-win/virtio-win.repo -O /etc/yum.repos.d/virtio-win.repo yum install -y virtio-win 手动下载驱动文件 virtio-win-0.1.173_x86.vfd 或者 virtio-win-0.1.173.iso 下载链接 2.kvm制作windows server基础镜像 # 2.1创建kvm镜像文件 # [root@kvm-1 ~]# qemu-img create -f qcow2 win2019-dc.qcow2 50G Formatting \u0026#39;win2019-dc.qcow2\u0026#39;, fmt=qcow2 size=53687091200 cluster_size=65536 lazy_refcounts=off refcount_bits=16 2.2安装windows server # 有两种方式安装虚拟化驱动程序\n2.2.1 方法一:使用VFD软盘驱动镜像 # virt-install启动windows server kvm实例，开启vnc远程访问，监听端口5920\n[root@kvm-1 windows]virt-install \\ --name win2019dc \\ --memory 8192 \\ --vcpus sockets=1,cores=2,threads=2 \\ --os-type=windows \\ --os-variant=auto \\ --cdrom=/data/windows/cn_windows_server_2019.iso \\ --disk /data/windows/win2019-dc.qcow2,bus=virtio,size=50 \\ --disk /data/windows/virtio-win-0.1.173_x86.vfd,device=floppy \\ --network bridge=bridge0,model=virtio \\ --graphics vnc,listen=\u0026#39;0.0.0.0\u0026#39;,port=5920 \\ --hvm \\ --virt-type kvm WARNING 需要图形显示，但未设置 DISPLAY。不能运行 virt-viewer。 WARNING 没有控制台用于启动客户机，默认为 --wait -1 开始安装...... ERROR unsupported format character \u0026#39;�\u0026#39; (0xffffffe7) at index 47 域安装失败，您可以运行下列命令重启您的域： \u0026#39;virsh start virsh --connect qemu:///system start win2019dc\u0026#39; 否则请重新开始安装。 通过VNC Viewer连接访问kvm实例win2019dc,安装windows系统 手动安装磁盘SCSI和网卡驱动 继续按提示完成安装 2.2.2 方法二:手动挂载virtio驱动iso镜像 # 创建windows虚拟机\nvirt-install \\ --name win2019dc \\ --memory 8192 \\ --vcpus sockets=1,cores=2,threads=2 \\ --os-type=windows \\ --os-variant=auto \\ --cdrom=/data/windows/cn_windows_server_2019.iso \\ --disk /data/windows/win2019-dc.qcow2,bus=virtio,size=50 \\ --network bridge=bridge0,model=virtio \\ --graphics vnc,listen=\u0026#39;0.0.0.0\u0026#39;,port=5930 \\ --hvm \\ --virt-type kvm WARNING 需要图形显示，但未设置 DISPLAY。不能运行 virt-viewer。 WARNING 没有控制台用于启动客户机，默认为 --wait -1 开始安装...... ERROR unsupported format character \u0026#39;�\u0026#39; (0xffffffe7) at index 47 域安装失败，您可以运行下列命令重启您的域： \u0026#39;virsh start virsh --connect qemu:///system start win2019dc\u0026#39; 否则请重新开始安装。 和方法一一样，通过VNC Viewer连接访问kvm实例安装windows系统，加载驱动程序的话需要先手动挂载 virtio-win-0.1.173.iso。 先查看cdrom设备对应虚拟机的dev名称 dev='hda'\n[root@kvm-1 windows]# virsh dumpxml win2019dc|grep device.*cdrom -A5|grep \u0026#34;\u0026#34; \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;cdrom\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;/data/windows/cn_windows_server_2019.iso\u0026#39;/\u0026gt; \u0026lt;backingStore/\u0026gt; \u0026lt;target dev=\u0026#39;hda\u0026#39; bus=\u0026#39;ide\u0026#39;/\u0026gt; \u0026lt;readonly/\u0026gt; [root@kvm-1 windows]# virsh change-media win2019dc hda /usr/share/virtio-win/virtio-win.iso Successfully updated media. 然后手动安装磁盘SCSI和网卡驱动 切换回系统iso镜像，并继续按提示完成安装\n[root@kvm-1 windows]# virsh change-media win2019dc hda /data/windows/cn_windows_server_2019.iso Successfully updated media. #卸载iso文件的方法 virsh change-media win2019dc --eject virtio-win.iso 2.2.3 配置管理员密码 # 2.3 windows配置修改 # 2.3.1 电源和睡眠 # ` ` 2.3.2 关闭交互式登录Ctrl+Atl+Del # 2.3.3 开启远程桌面访问 # 2.3.4 IPv4地址持久化配置 # 在 C:\\Windows\\目录新建脚本ipconfig.bat\n@echo off setlocal enabledelayedexpansion set /a v=0 set ipaddr=\u0026#34;\u0026#34; set netmask=\u0026#34;\u0026#34; set gateway=\u0026#34;\u0026#34; @for /f \u0026#34;tokens=2 delims==\u0026#34; %%a in (C:\\Windows\\ip.txt) do ( set /a v+=1 if !v!==1 set ipaddr=%%a if !v!==2 set netmask=%%a if !v!==3 set gateway=%%a ) netsh interface ipv4 add address \u0026#34;以太网\u0026#34; !ipaddr! !netmask! !gateway! netsh interface ipv4 add dnsservers \u0026#34;以太网\u0026#34; 192.168.9.3 index=2 no endlocal 把脚本ipconfig.bat配置开机自启动\n2.3.5 安装KMS密钥 # 本文使用KMS客户端密钥，即 Microsoft 通用批量许可证密钥 (GVLK)。\nC:\\Users\\Administrator\u0026gt; slmgr /upk C:\\Users\\Administrator\u0026gt; slmgr /cpky C:\\Users\\Administrator\u0026gt; slmgr /ckms C:\\Users\\Administrator\u0026gt; slmgr /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG C:\\Users\\Administrator\u0026gt; slmgr /dlv 请自行获取或搭建私有可用KMS服务器 C:\\Users\\Administrator\u0026gt; slmgr /skms your.kms.server C:\\Users\\Administrator\u0026gt; slmgr /ato C:\\Users\\Administrator\u0026gt; slmgr /dlv 微软官网获取通用批量许可证密钥， 密钥管理服务 (KMS) 客户端激活和产品密钥\n2.4 压缩镜像 # 配置完成之后关机，基础镜像win2019-dc.qcow2就完成了，当前大小9.1G\n[root@kvm-1 windows]# qemu-img info win2019-dc.qcow2 image: win2019-dc.qcow2 file format: qcow2 virtual size: 50G (53687091200 bytes) disk size: 9.1G cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false copy模式压缩镜像，压缩之后只有4.9G\n#临时copy文件空间为virtual size,需要注意主机磁盘空间，默认为/tmp #可以通过--tmp /somedir或者环境变量export TMPDIR=/somedir [root@kvm-1 windows]# virt-sparsify --compress --convert qcow2 win2019-dc.qcow2 win2019-dc-compress.qcow2 [ 0.1] Create overlay file in /tmp to protect source disk [ 0.2] Examine source disk [ 1.9] Fill free space in /dev/sda1 with zero 100% ⟦▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒⟧ 00:00 [ 9.9] Fill free space in /dev/sda2 with zero 100% ⟦▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒⟧ 00:00 [ 620.8] Copy to destination and make sparse [1150.4] Sparsify operation completed with no errors. virt-sparsify: Before deleting the old disk, carefully check that the target disk boots and works correctly. [root@kvm-1 windows]# qemu-img info win2019-dc-compress.qcow2 image: win2019-dc-compress.qcow2 file format: qcow2 virtual size: 50G (53687091200 bytes) disk size: 4.9G cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false 3. 使用基础镜像启动windows server # 定义一个虚拟机\n[root@kvm-1 ~]# qemu-img create -f qcow2 win2019dc-1.qcow2 50G Formatting \u0026#39;win2019dc-1.qcow2\u0026#39;, fmt=qcow2 size=53687091200 cluster_size=65536 lazy_refcounts=off refcount_bits=16 [root@kvm-1 win2019]# virt-install \\ --name win2019dc-1 \\ --memory 8192 \\ --vcpus sockets=1,cores=2,threads=2 \\ --os-type=windows \\ --os-variant=win10 \\ --disk /data/windows/win2019/win2019dc-1.qcow2,bus=virtio,size=50 \\ --network bridge=bridge0,model=virtio \\ --hvm \\ --virt-type kvm \\ --print-xml \u0026gt; win2019dc-1.xml 创建IP配置文件ip.txt，文件名与基础镜像中ipconfig.bat指定的一致\n[root@kvm-1 win2019]# cat ip.txt IPADDR=192.168.9.12 NETMASK=255.255.255.0 GATEWAY=192.168.9.1 拷贝ip.txt文件到镜像，位置与基础镜像中ipconfig.bat指定的一致\n[root@kvm-1 win2019]# yum install libguestfs-winsupport -y -q [root@kvm-1 win2019]# unix2dos ip.txt [root@kvm-1 win2019]# virt-copy-in -a ./win2019dc-1.qcow2 ip.txt /\u0026#34;Windows\u0026#34;/ 启动虚拟机\n[root@kvm-1 win2019]# virsh define win2019dc-1.xml [root@kvm-1 win2019]# virsh start win2019dc-1 通过windows远程桌面访问192.168.9.12，administrator密码是步骤2.2.3中配置的密码\n因为使用的相同镜像，新虚拟机的主机名、SID(安全标识符)是相同的 如果需要SID唯一，定义虚拟机时要开启vnc，虚拟机启动后使用Sysprep（系统准备工具）生成新的SID，重启后，vnc连接虚拟机并进入OOBE界面进行配置\n","date":"16 August 2022","permalink":"/2022/08/kvm-windowsserver2019/","section":"博客","summary":"1.资料准备 # 1.1 iso文件下载 # 可以通过访问官方页面下载( 链接)，但是下载速度比较慢。 我这里选择使用p2p下载工具mldonkey，Centos7系统安装可以参考 Centos7部署mldonkey，iso文件e2dk下载链接可以从 msdn.itellyou.cn网站获取\ned2k://|file|cn_windows_server_2019_updated_march_2019_x64_dvd_c1ffb46c.iso|5347280896|49FCF8C558517608537E0396854560D6|/ 1.2 virtio驱动程序下载 # kvm安装运行windows server系统需要安装磁盘scsi、网卡驱动\nyum安装 wget https://fedorapeople.org/groups/virt/virtio-win/virtio-win.repo -O /etc/yum.repos.d/virtio-win.repo yum install -y virtio-win 手动下载驱动文件 virtio-win-0.1.173_x86.vfd 或者 virtio-win-0.1.173.iso 下载链接 2.","title":"kvm部署WindowsServer2019"},{"content":"","date":"16 August 2022","permalink":"/tags/windowsserver/","section":"Tags","summary":"","title":"WindowsServer"},{"content":"1. mkdonkey是什么 # MLDonkey是一个GPL开源免费、跨平台(Linux、Solaris、Mac OS X、Windows 以及 MorphOS)、多协议的P2P共享软件。其支持包括eDonkey电驴协议在内的多种P2P协议，并能运行于类Unix/Linux、Mac OS X、Windows等操作系统。主要使用OCaml语言编写,,同时有些部分使用了一些C语言以及汇编语言的代码，从而保证了它的高效能。可接受Magnet URI，能搭配各种GUI。\nMLDonkey最早只支持eDonkey2000协议（ED2K），后来逐步加入了overnet、kad、BT、HTTP、FTP等协议的支持。\n引用https://baike.baidu.com/item/mlDonkey/2257164?fr=ge_ala\n2. Centos7源码安装 # ​\t源码托管在 github，可以release中获取下载链接\n[root@keeping] ~$ wget https://github.com/ygrek/mldonkey/releases/download/release-3-1-7/mldonkey-3.1.7.tar.bz2 [root@keeping] ~$ tar xf mldonkey-3.1.7.tar.bz2 [root@keeping] ~$ cd mldonkey-3.1.7/ [root@keeping] ~$ ./configure [root@keeping] ~$ make # 命令行启动服务 [root@keeping] ~$ ./mlnet 3. 配置mldonkey # 默认配置目录在~/.mldonkey。修改web页面访问的ip白名单~/.mldonkey/downloads.ini\n# 按需配置，这里放开了所有访问权限，不建议 allowed_ips = [ \u0026#34;0.0.0.0/0\u0026#34;;] 3.1 初始化admin密码 # web页面监听端口是http4080，配置好访问白名单后启动服务。首次访问需要初始化admin账户，文本框中输入useradd admin yourpasswd，然后点击input按钮\n这里有个安全问题是密码初始化是GET请求/submit?q=useradd+admin+yourpassword，用户密码都是明文。因为我是通过公网访问，所以在前面部署了nginx代理通过https访问\n自签名证书，参考 如何使用openssl工具创建私有CA\nserver { listen 34080 ssl; server_name _; ssl_certificate myserver.pem; ssl_certificate_key myserver.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM; ssl_prefer_server_ciphers on; location / { proxy_pass http://127.0.0.1:4080; } } 修改密码后登录\n3.2 Options配置修改 # 点击Options，修改一下配置:\nED2K-max_connected_servers 200 max_hard_upload_rate 20 max_hard_download_rate 20480 3.3 导入服务器列表 # 默认服务列表比较少，推荐手动导入下server.met\n点击Servers-\u0026gt;Import Server.met，贴上链接，可以试试一下两个地址\nhttp://ed2k.2x4u.de/index.html 主页中有相关链接\nhttp://upd.emule-security.org/server.met\n3.4 创建下载任务 # 在文本框贴入ed2k资源链接，点击Input按钮完成导入。点击Transfers-\u0026gt;Downloads可以查看下载进度\n文件下载完成后默认存储位置~/.mldonkey/incoming/files/，可以在downloads.ini中指定\n","date":"6 August 2022","permalink":"/2022/08/mldonkey/","section":"博客","summary":"1. mkdonkey是什么 # MLDonkey是一个GPL开源免费、跨平台(Linux、Solaris、Mac OS X、Windows 以及 MorphOS)、多协议的P2P共享软件。其支持包括eDonkey电驴协议在内的多种P2P协议，并能运行于类Unix/Linux、Mac OS X、Windows等操作系统。主要使用OCaml语言编写,,同时有些部分使用了一些C语言以及汇编语言的代码，从而保证了它的高效能。可接受Magnet URI，能搭配各种GUI。\nMLDonkey最早只支持eDonkey2000协议（ED2K），后来逐步加入了overnet、kad、BT、HTTP、FTP等协议的支持。\n引用https://baike.baidu.com/item/mlDonkey/2257164?fr=ge_ala\n2. Centos7源码安装 # ​\t源码托管在 github，可以release中获取下载链接\n[root@keeping] ~$ wget https://github.com/ygrek/mldonkey/releases/download/release-3-1-7/mldonkey-3.1.7.tar.bz2 [root@keeping] ~$ tar xf mldonkey-3.1.7.tar.bz2 [root@keeping] ~$ cd mldonkey-3.","title":"Centos7部署mldonkey"},{"content":"","date":"8 November 2019","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes"},{"content":"","date":"8 November 2019","permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"prometheus"},{"content":"","date":"8 November 2019","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"8 November 2019","permalink":"/series/%E9%AB%98%E5%8F%AF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","section":"Series","summary":"","title":"高可用prometheus监控集群部署"},{"content":" Prometheus可以定义警报规则，满足条件时触发警报，并推送到Alertmanager服务。Alertmanager支持高可用的集群部署，负责管理、整合和分发警报到不通目的地，并能做到告警收敛 本例采用statefulset方式部署3个实例的Alertmanager高可用集群\nAlertmanager集群部署 # 创建alertmanager数据目录 # $ mkdir /data/alertmanager 创建storageclass # $ cat storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: alertmanager-lpv provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 创建local volume # $ cat pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: alertmanager-pv-0 spec: capacity: storage: 30Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: alertmanager-lpv local: path: /data/alertmanager nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.50 --- apiVersion: v1 kind: PersistentVolume metadata: name: alertmanager-pv-1 spec: capacity: storage: 30Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: alertmanager-lpv local: path: /data/alertmanager nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.51 --- apiVersion: v1 kind: PersistentVolume metadata: name: alertmanager-pv-2 spec: capacity: storage: 30Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: alertmanager-lpv local: path: /data/alertmanager nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.52 创建alertmanager.yml的configmap # 通过Alertmanager提供的webhook支持，可以配置钉钉告警。需要定义基于webhook的告警接收器receiver。alertmanager.yml文件定义告警方式、模板、告警的分发策略和告警抑制策略等\n#cat configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: alertmanager-config namespace: kube-system labels: kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: EnsureExists data: alertmanager.yml: | global: resolve_timeout: 3m #解析的超时时间 route: group_by: [\u0026#39;example\u0026#39;] group_wait: 60s group_interval: 60s repeat_interval: 12h receiver: \u0026#39;DingDing\u0026#39; receivers: #定义基于webhook的告警接收器receiver - name: \u0026#39;DingDing\u0026#39; webhook_configs: - url: http://192.168.1.50:8060/dingtalk/webhook/send send_resolved: true # 发送已解决通知 部署prometheus-webhook-dingtalk # 这里使用docker部署\n$ docker pull timonwong/prometheus-webhook-dingtalk $ docker run -d -p 8060:8060 --name webhook timonwong/prometheus-webhook --ding.profile=\u0026#34;webhook=https://oapi.dingtalk.com/robot/send?access_token={自己的dingding token} 钉钉群添加自定义机器人，可以生成Webhook地址https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxx\n创建headless service # $ cat service-statefulset.yaml apiVersion: v1 kind: Service metadata: name: alertmanager namespace: kube-system spec: ports: - name: altermanager port: 9093 targetPort: 9093 selector: k8s-app: alertmanager clusterIP: None 定制alertmanager镜像 # alertmanager集群高可用采用statefulset方式部署，在实例启动时，需要在启动参数cluster.listen-address指定各peer的ip，可以在pod启动时通过变量传入，自定义启动脚本来解决 1.自定义启动脚本start.sh\n$ cat start.sh #!/bin/bash IFS=\u0026#39;,\u0026#39; read -r -a peer_ips \u0026lt;\u0026lt;\u0026lt; \u0026#34;${PEER_ADDRESS}\u0026#34; str=\u0026#34;--cluster.peer=\u0026#34; args=\u0026#34;\u0026#34; for ip in ${peer_ips[@]}; do args=${args}${str}${ip}\u0026#34; \u0026#34; done /app/bin/alertmanager \\ --config.file=/etc/config/alertmanager.yml \\ --storage.path=/data \\ --web.external-url=/ \\ --cluster.listen-address=${NODE_NAME}:9094 \\ $args tail -f /app/bin/hold.txt 2.定制镜像\n$ cat Dockerfile FROM prom/alertmanager:v0.15.3 as alertmanager FROM selfflying/centos7.2:latest COPY start.sh / RUN chmod +x /start.sh ENTRYPOINT [\u0026#34;sh\u0026#34;,\u0026#34;/start.sh\u0026#34;] $ docker build -t 192.168.1.50:5000/library/centos7.2_alertmanager:v0.15.3 . $ docker push 192.168.1.50:5000/library/centos7.2_alertmanager:v0.15.3 创建statefulset # apiVersion: apps/v1 kind: StatefulSet metadata: name: alertmanager namespace: kube-system labels: k8s-app: alertmanager kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile version: v0.15.3 spec: serviceName: \u0026#34;alertmanager\u0026#34; podManagementPolicy: \u0026#34;Parallel\u0026#34; replicas: 3 selector: matchLabels: k8s-app: alertmanager version: v0.15.3 template: metadata: labels: k8s-app: alertmanager version: v0.15.3 annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: tolerations: - key: \u0026#34;CriticalAddonsOnly\u0026#34; operator: \u0026#34;Exists\u0026#34; - effect: NoSchedule key: node-role.kubernetes.io/master affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: k8s-app operator: In values: - alertmanager topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; priorityClassName: system-cluster-critical hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: prometheus-alertmanager image: \u0026#34;192.168.1.50:5000/library/centos7.2_alertmanager:v0.15.3\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: PEER_ADDRESS value: \u0026#34;192.168.1.50:9094,192.168.1.51:9094,192.168.1.52:9094\u0026#34; readinessProbe: httpGet: path: /#/status port: 9093 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - name: config-volume mountPath: /etc/config - name: storage-volume mountPath: \u0026#34;/data\u0026#34; subPath: \u0026#34;\u0026#34; resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 10m memory: 50Mi - name: prometheus-alertmanager-configmap-reload image: \u0026#34;jimmidyson/configmap-reload:v0.1\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; args: - --volume-dir=/etc/config - --webhook-url=http://localhost:9093/-/reload volumeMounts: - name: config-volume mountPath: /etc/config readOnly: true resources: limits: cpu: 10m memory: 10Mi requests: cpu: 10m memory: 10Mi volumes: - name: config-volume configMap: name: alertmanager-config volumeClaimTemplates: - metadata: name: storage-volume spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;alertmanager-lpv\u0026#34; resources: requests: storage: 20Gi 访问alertmanager web页面 # 集群创建成功http://192.168.1.52:9093/#/statusAlertmanager\nprometheus-federate配置告警规则 # 修改prometheus-federate的configmap.yaml\n#cat configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-federate-config namespace: kube-system data: alertmanager_rules.yaml: | groups: - name: example rules: - alert: InstanceDown expr: up == 0 for: 1m labels: severity: page annotations: summary: \u0026#34;Instance {{ $labels.instance }} down\u0026#34; description: \u0026#34;{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minutes.\u0026#34; - alert: NodeMemoryUsage expr: (node_memory_MemTotal_bytes -(node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes )) / node_memory_MemTotal_bytes * 100 \u0026gt; 55 for: 1m labels: team: node annotations: summary: \u0026#34;cluster:{{ $labels.cluster }} {{ $labels.instance }}: High Memory usage detected\u0026#34; description: \u0026#34;{{ $labels.instance }}: Memory usage is above 55% (current value is: {{ $value }}\u0026#34; prometheus.yml: | global: scrape_interval: 30s evaluation_interval: 30s #remote_write: # - url: \u0026#34;http://29.20.18.160:9201/write\u0026#34; alerting: alertmanagers: - static_configs: - targets: - alertmanager-0.alertmanager:9093 - alertmanager-1.alertmanager:9093 - alertmanager-2.alertmanager:9093 rule_files: - \u0026#34;/etc/prometheus/alertmanager_rules.yaml\u0026#34; scrape_configs: - job_name: \u0026#39;federate\u0026#39; scrape_interval: 30s honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; params: \u0026#39;match[]\u0026#39;: - \u0026#39;{job=~\u0026#34;kubernetes.*\u0026#34;}\u0026#39; - \u0026#39;{job=\u0026#34;prometheus\u0026#34;}\u0026#39; static_configs: - targets: - \u0026#39;prometheus-0.prometheus:9090\u0026#39; - \u0026#39;prometheus-1.prometheus:9090\u0026#39; 从prometheus federate web 可以看到添加的两条告警规则\nPrometheus web上可以看到状态为FIRING表示告警已触发\n告警信息生命周期的3中状态\ninactive：表示当前报警信息即不是firing状态也不是pending状态 pending：表示在设置的阈值时间范围内被激活的 firing：表示超过设置的阈值时间被激活的 alertmanger web页面可以看到此告警，表示已成功推送\n同时，钉钉群也收到了告警信息\n","date":"8 November 2019","permalink":"/2019/11/k8s-prometheus-4/","section":"博客","summary":"Prometheus可以定义警报规则，满足条件时触发警报，并推送到Alertmanager服务。Alertmanager支持高可用的集群部署，负责管理、整合和分发警报到不通目的地，并能做到告警收敛 本例采用statefulset方式部署3个实例的Alertmanager高可用集群\nAlertmanager集群部署 # 创建alertmanager数据目录 # $ mkdir /data/alertmanager 创建storageclass # $ cat storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: alertmanager-lpv provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 创建local volume # $ cat pv.","title":"高可用prometheus监控集群搭建(四)"},{"content":" Prometheus的联邦模式，支持了集群的分层扩展及跨服务扩展。 分层扩展允许Prometheus扩展到多数据中心、大规模主机集群，树型拓扑 跨服务扩展是不同类别的监控指标项由不同的prometheus server分别收集 在多k8s集群模式下，每个集群部署prometheus server用于收集该集群相关指标,借助prometheus联邦模式，实现监控数据的统一收集展现及告警通知\n联邦模式部署配置 # 创建prometheus-federate数据目录 # #分别在主机192.168.1.51和192.168.1.52上执行 $ groupadd -g 65534 nfsnobody $ useradd -g 65534 -u 65534 -s /sbin/nologin nfsnobody $ chown nfsnobody. /data/prometheus-federate/ 创建storageclass # $ cat storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: prometheus-federate-lpv provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 创建local volume # $ cat pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: prometheus-federate-lpv-0 spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: prometheus-federate-lpv local: path: /data/prometheus-federate nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.51 --- apiVersion: v1 kind: PersistentVolume metadata: name: prometheus-federate-lpv-1 spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: prometheus-federate-lpv local: path: /data/prometheus-federate nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.52 创建prometheus.yml的configmap # $ cat configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-federate-config namespace: kube-system data: prometheus.yml: | global: scrape_interval: 30s evaluation_interval: 30s scrape_configs: - job_name: \u0026#39;federate\u0026#39; scrape_interval: 30s honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; params: \u0026#39;match[]\u0026#39;: - \u0026#39;{job=~\u0026#34;kubernetes.*\u0026#34;}\u0026#39; - \u0026#39;{job=\u0026#34;prometheus\u0026#34;}\u0026#39; static_configs: - targets: - \u0026#39;prometheus-0.prometheus:9090\u0026#39; - \u0026#39;prometheus-1.prometheus:9090\u0026#39; 创建headless service # $ cat service-statefulset.yaml apiVersion: v1 kind: Service metadata: name: prometheus-federate namespace: kube-system spec: ports: - name: prometheus-federate port: 9091 targetPort: 9091 selector: k8s-app: prometheus-federate 创建statefulset # $ cat prometheus-federate-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: prometheus-federate namespace: kube-system labels: k8s-app: prometheus-federate kubernetes.io/cluster-service: \u0026#34;true\u0026#34; spec: serviceName: \u0026#34;prometheus-federate\u0026#34; podManagementPolicy: \u0026#34;Parallel\u0026#34; replicas: 2 selector: matchLabels: k8s-app: prometheus-federate template: metadata: labels: k8s-app: prometheus-federate annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: k8s-app operator: In values: - prometheus-federate topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; priorityClassName: system-cluster-critical hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: prometheus-federate-configmap-reload image: \u0026#34;jimmidyson/configmap-reload:v0.1\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; args: - --volume-dir=/etc/config - --webhook-url=http://localhost:9091/-/reload volumeMounts: - name: config-volume mountPath: /etc/config readOnly: true resources: limits: cpu: 10m memory: 10Mi requests: cpu: 10m memory: 10Mi - image: prom/prometheus:v2.11.0 imagePullPolicy: IfNotPresent name: prometheus command: - \u0026#34;/bin/prometheus\u0026#34; args: - \u0026#34;--web.listen-address=0.0.0.0:9091\u0026#34; - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; - \u0026#34;--storage.tsdb.path=/prometheus\u0026#34; - \u0026#34;--storage.tsdb.retention=24h\u0026#34; - \u0026#34;--web.console.libraries=/etc/prometheus/console_libraries\u0026#34; - \u0026#34;--web.console.templates=/etc/prometheus/consoles\u0026#34; - \u0026#34;--web.enable-lifecycle\u0026#34; ports: - containerPort: 9091 protocol: TCP volumeMounts: - mountPath: \u0026#34;/prometheus\u0026#34; name: prometheus-federate-data - mountPath: \u0026#34;/etc/prometheus\u0026#34; name: config-volume readinessProbe: httpGet: path: /-/ready port: 9091 initialDelaySeconds: 30 timeoutSeconds: 30 livenessProbe: httpGet: path: /-/healthy port: 9091 initialDelaySeconds: 30 timeoutSeconds: 30 resources: requests: cpu: 100m memory: 100Mi limits: cpu: 1000m memory: 2500Mi serviceAccountName: prometheus volumes: - name: config-volume configMap: name: prometheus-federate-config volumeClaimTemplates: - metadata: name: prometheus-federate-data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;prometheus-federate-lpv\u0026#34; resources: requests: storage: 20Gi 访问prometheus web UI # prometheus server的job都已成功抓取；查询up指标，可以获取到相关metric，同时都具有标签cluster=\u0026ldquo;01\u0026rdquo;，可用于区别不同集群的指标；此标签是prometheus server在配置文件中external_labels指定 ","date":"8 November 2019","permalink":"/2019/11/k8s-prometheus-3/","section":"博客","summary":"\u003cblockquote\u003e\n\u003cp\u003ePrometheus的联邦模式，支持了集群的分层扩展及跨服务扩展。\n\u003ccode\u003e分层扩展\u003c/code\u003e允许Prometheus扩展到多数据中心、大规模主机集群，树型拓扑\n\u003ccode\u003e跨服务扩展\u003c/code\u003e是不同类别的监控指标项由不同的prometheus server分别收集\n在多k8s集群模式下，每个集群部署prometheus server用于收集该集群相关指标,借助prometheus联邦模式，实现监控数据的统一收集展现及告警通知\u003c/p\u003e\n\u003c/blockquote\u003e","title":"高可用prometheus监控集群搭建(三)"},{"content":" Prometheus使用exporter工具来暴露主机和应用程序上的指标，目前有很多可用于各种目的的exporter( https://prometheus.io/docs/instrumenting/exporters/)\nNode Exporter部署 # Node exporter可提供用于收集各种主机指标数据（包括CPU、内存、网络和磁盘），在k8s集群中可以使用daemonset的方式部署。需要将宿主机根映射到容器中，来正确获取主机文件系统监控数据\n$ cat node-exporter.yaml apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: node-exporter namespace: kube-system labels: k8s-app: node-exporter spec: template: metadata: labels: k8s-app: node-exporter spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master containers: - image: k8s.gcr.io/prom/node-exporter:latest imagePullPolicy: IfNotPresent name: prometheus-node-exporter ports: - containerPort: 9100 hostPort: 9100 protocol: TCP name: metrics volumeMounts: - mountPath: /host/proc name: proc - mountPath: /host/sys name: sys - mountPath: /host name: rootfs args: - --path.procfs=/host/proc - --path.sysfs=/host/sys - --path.rootfs=/host volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys - name: rootfs hostPath: path: / hostNetwork: true hostPID: true --- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; labels: k8s-app: node-exporter name: node-exporter namespace: kube-system spec: ports: - name: http port: 9100 protocol: TCP selector: k8s-app: node-exporter $ kubectl apply -f node-exporter.yaml node-exporter的service中指定了annotation: prometheus.io/scrape: \u0026quot;true\u0026quot;, job:kubernetes-service-endpoints可以自动发现，如下图所示\n访问graph查询页面，可以看到node_开头的metrics也已经获取到\nkube-state-metrics部署 # cAdvisor(kubelet)可以收集容器及systemd托管服务的CPU、内存、网络、容器数量，却不能获取到有关kubennetes资源对象指标的信息，比如：集群运行了多少个POD，分别是什么状态，pod重启了多少次，pod中的容器启动失败的原因是什么，deployment的部署是否成功，deployment的运行副本数是否和元数据中的一致\nkube-state-metrics通过轮询kubernetes API，提供有关资源对象指标的metrics，包括：CronJob、DaemonSet、Deployment、Job、LimitRange、Node、PersistentVolume 、PersistentVolumeClaim、Pod、Pod Disruption Budget、ReplicaSet、ReplicationController、ResourceQuota、Service、StatefulSet、Namespace、Horizontal Pod Autoscaler、Endpoint、Secret、ConfigMap、Ingress、CertificateSigningRequest\nrbac资源初始化 # $ cat kube-state-metrics-rbac.yaml kind: Role metadata: namespace: kube-system name: kube-state-metrics-resizer rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - pods verbs: [\u0026#34;get\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: - deployments resourceNames: [\u0026#34;kube-state-metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: - deployments resourceNames: [\u0026#34;kube-state-metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;update\u0026#34;] --- kind: RoleBinding metadata: name: kube-state-metrics namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kube-state-metrics-resizer subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system --- kind: ClusterRole metadata: name: kube-state-metrics rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - configmaps - secrets - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: - daemonsets - deployments - replicasets - ingresses verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: - daemonsets - deployments - replicasets - statefulsets verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;batch\u0026#34;] resources: - cronjobs - jobs verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;autoscaling\u0026#34;] resources: - horizontalpodautoscalers verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;policy\u0026#34;] resources: - poddisruptionbudgets verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;certificates.k8s.io\u0026#34;] resources: - certificatesigningrequests verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- kind: ClusterRoleBinding metadata: name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-state-metrics namespace: kube-system 创建deployment # $ cat kube-state-metrics-deployment.yaml kind: Deployment metadata: name: kube-state-metrics namespace: kube-system spec: selector: matchLabels: k8s-app: kube-state-metrics replicas: 1 template: metadata: labels: k8s-app: kube-state-metrics spec: serviceAccountName: kube-state-metrics containers: - name: kube-state-metrics image: quay.io/coreos/kube-state-metrics:v1.6.0 ports: - name: http-metrics containerPort: 8080 - name: telemetry containerPort: 8081 readinessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 timeoutSeconds: 5 - name: addon-resizer image: k8s.gcr.io/addon-resizer:1.8.4 resources: limits: cpu: 150m memory: 50Mi requests: cpu: 150m memory: 50Mi env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace command: - /pod_nanny - --container=kube-state-metrics - --cpu=100m - --extra-cpu=1m - --memory=100Mi - --extra-memory=2Mi - --threshold=5 - --deployment=kube-state-metrics --- apiVersion: v1 kind: Service metadata: name: kube-state-metrics namespace: kube-system labels: k8s-app: kube-state-metrics annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; spec: ports: - name: http-metrics port: 8080 targetPort: http-metrics protocol: TCP - name: telemetry port: 8081 targetPort: telemetry protocol: TCP selector: k8s-app: kube-state-metrics kube-state-metrics的service中指定了annotation: prometheus.io/scrape: \u0026quot;true\u0026quot;, job: kubernetes-service-endpoints可以自动发现\n","date":"6 November 2019","permalink":"/2019/11/k8s-prometheus-2/","section":"博客","summary":"\u003cblockquote\u003e\n\u003cp\u003ePrometheus使用exporter工具来暴露主机和应用程序上的指标，目前有很多可用于各种目的的exporter(\u003ca href=\"https://prometheus.io/docs/instrumenting/exporters/\"   target=\"_blank\"\u003e\n    https://prometheus.io/docs/instrumenting/exporters/\u003c/a\u003e)\u003c/p\u003e\n\u003c/blockquote\u003e","title":"高可用prometheus监控集群搭建(二)"},{"content":"简介 # Prometheus是一个开源的监控系统，通过抓取或拉取应用程序中暴露的时间序列数据来工作。时间序列数据通常由应用程序本身通过客户端库或者成为exporter的代理来作为HTTP端点暴露。目前已经存在很多exporter和客户端库，支持多种编程语言、框架和开源应用程序。\n官方架构图 # Prometheus通过配置target，来抓取对应主机、进程、服务或应用程序的指标。一组具有相同角色的target被称为job。比如，定义kubernetes-nodes的job，来抓取集群所有主机的相关指标 服务发现（target） 静态资源列表 基于文件的发现，可使用自动化配置管理工具自动更新 自动发现，支持基于AWS/Consul/dns/kubernetes/marathon等的服务发现 Prometheus将收集时间序列数据存储在本地，也可以配置remote_write存储到其它时间序列数据库，比如OpenTSDB、InfluxDB、M3DB等 可视化: Prometheus内置了web UI，也可以和功能强大的开源仪表板Grafana集成，可自定义dashboard满足各种定制化需求 聚合和告警: Prometheus可以查询和聚合时间序列数据，并创建规则来记录常用的查询和聚合。还可以定义警报规则，满足条件时触发警报，并推送到Alertmanager服务。Alertmanagr支持高可用的集群部署，负责管理、整合和分发警报到不通目的地，并能做到告警收敛 数据模型 # Prometheus使用一个多维时间序列数据模型，结合了时间序列名称和称为标签(label)的键/值对，这些标签提供了维度。每个时间序列由时间序列名称和标签的组合唯一标识。 \u0026lt;time series name\u0026gt; {\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;,...}\nmachine_cpu_cores{beta_kubernetes_io_arch=\u0026ldquo;amd64\u0026rdquo;,beta_kubernetes_io_os=\u0026ldquo;linux\u0026rdquo;,instance=\u0026ldquo;192.168.1.51\u0026rdquo;,job=\u0026ldquo;kubernetes-cadvisor\u0026rdquo;,kubernetes_io_arch=\u0026ldquo;amd64\u0026rdquo;,kubernetes_io_hostname=\u0026ldquo;192.168.1.51\u0026rdquo;,kubernetes_io_os=\u0026ldquo;linux\u0026rdquo;} 2 machine_cpu_cores收集的是主机的cpu核数，taget label标识了是job kubernetes-cadvisor抓取的，instance 192.168.1.51的指标，还包括此node的其它labels；获取到的value值为2\n集群拓扑图 # 节点部署规划 # IP k8srole service 192.168.1.50 master node-exporter/alertmanager 192.168.1.51 node node-exporter/alertmanager/grafana/prometheus-0/prometheus-federate-0/kube-state-metrics 192.168.1.52 node node-exporter/alertmanager/prometheus-1/prometheus-federate-1 部署架构图 # 在多k8s集群模式下，每个集群部署prometheus server用于收集该集群相关指标,借助prometheus联邦模式，实现监控数据的统一收集展现及告警通知 集群内prometheus server部署 # statefulset方式部署,并使用本地持久卷 Local Volume\n创建prometheus server数据目录 # $ groupadd -g 65534 nfsnobody $ useradd -g 65534 -u 65534 -s /sbin/nologin nfsnobody $ chown nfsnobody. /data/prometheus/ 创建storageclass,指定卷挂载为拓扑感知模式，即pod调度后再绑定卷 # $cat storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: prometheus-lpv provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer 创建两个local volume，利用nodeAffinity绑定到指定节点 # # cat pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: prometheus-lpv-0 spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: prometheus-lpv local: path: /data/prometheus nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.51 --- apiVersion: v1 kind: PersistentVolume metadata: name: prometheus-lpv-1 spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: prometheus-lpv local: path: /data/prometheus nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - 192.168.1.52 初始化rabc # $cat rbac-setup.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: - extensions resources: - ingresses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: kube-system ######　创建configmap(prometheus.yml)\n$cat prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: kube-system data: prometheus.yml: | global: scrape_interval: 30s evaluation_interval: 30s external_labels: cluster: \u0026#34;01\u0026#34; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: - prometheus-0.prometheus:9090 - prometheus-1.prometheus:9090 - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: \u0026#39;kubernetes-cadvisor\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor metric_relabel_configs: - action: replace source_labels: [id] regex: \u0026#39;^/machine\\.slice/machine-rkt\\\\x2d([^\\\\]+)\\\\.+/([^/]+)\\.service$\u0026#39; target_label: rkt_container_name replacement: \u0026#39;${2}-${1}\u0026#39; - action: replace source_labels: [id] regex: \u0026#39;^/system\\.slice/(.+)\\.service$\u0026#39; target_label: systemd_service_name replacement: \u0026#39;${1}\u0026#39; - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: \u0026#39;kubernetes-service-endpoints\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - source_labels: [__address__] action: replace target_label: instance regex: (.+):(.+) replacement: $1 statefulset创建后，pod的命名方式为\u0026lt;statefulset name\u0026gt;-0、\u0026lt;statefulset name\u0026gt;-1… 对应生成的dns域名为，podname.stateflusetname.namespace.svc.cluster.local\njobprometheus用来获取prometheus本身的监控指标，本例中prometheus server为statefulset方式部署，指定targets为prometheus-0.prometheus:9090、prometheus-1.prometheus:9090 external_labels用于外部系统标签，在使用remote_write或者federate时，可以为该集群指标添加标签\n创建headless service # $cat service-statefulset.yaml apiVersion: v1 kind: Service metadata: name: prometheus namespace: kube-system spec: ports: - name: prometheus port: 9090 targetPort: 9090 selector: k8s-app: prometheus clusterIP: None 创建prometheus statefulset # $cat prometheus-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: prometheus namespace: kube-system labels: k8s-app: prometheus kubernetes.io/cluster-service: \u0026#34;true\u0026#34; spec: serviceName: \u0026#34;prometheus\u0026#34; podManagementPolicy: \u0026#34;Parallel\u0026#34; replicas: 2 selector: matchLabels: k8s-app: prometheus template: metadata: labels: k8s-app: prometheus annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: k8s-app operator: In values: - prometheus topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; priorityClassName: system-cluster-critical hostNetwork: true dnsPolicy: ClusterFirstWithHostNet containers: - name: prometheus-server-configmap-reload image: \u0026#34;jimmidyson/configmap-reload:v0.1\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; args: - --volume-dir=/etc/config - --webhook-url=http://localhost:9090/-/reload volumeMounts: - name: config-volume mountPath: /etc/config readOnly: true resources: limits: cpu: 10m memory: 10Mi requests: cpu: 10m memory: 10Mi - image: prom/prometheus:v2.11.0 imagePullPolicy: IfNotPresent name: prometheus command: - \u0026#34;/bin/prometheus\u0026#34; args: - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; - \u0026#34;--storage.tsdb.path=/prometheus\u0026#34; - \u0026#34;--storage.tsdb.retention=24h\u0026#34; - \u0026#34;--web.console.libraries=/etc/prometheus/console_libraries\u0026#34; - \u0026#34;--web.console.templates=/etc/prometheus/consoles\u0026#34; - \u0026#34;--web.enable-lifecycle\u0026#34; ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: \u0026#34;/prometheus\u0026#34; name: prometheus-data - mountPath: \u0026#34;/etc/prometheus\u0026#34; name: config-volume readinessProbe: httpGet: path: /-/ready port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 livenessProbe: httpGet: path: /-/healthy port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 resources: requests: cpu: 100m memory: 100Mi limits: cpu: 1000m memory: 2500Mi serviceAccountName: prometheus volumes: - name: config-volume configMap: name: prometheus-config volumeClaimTemplates: - metadata: name: prometheus-data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] storageClassName: \u0026#34;prometheus-lpv\u0026#34; resources: requests: storage: 20Gi volumeClaimTemplates指定storageclass,在pod创建时会生成PVC，为pod分配prometheus-lpv的pv。只要PVC不被删除，pod均会被调度到已绑定的pv节点上。\n$ kubectl get pv prometheus-lpv-0 50Gi RWO Retain Bound kube-system/prometheus-data-prometheus-1 prometheus-lpv 13d prometheus-lpv-1 50Gi RWO Retain Bound kube-system/prometheus-data-prometheus-0 prometheus-lpv 13d $ kubectl get pvc -n kube-system prometheus-data-prometheus-0 Bound prometheus-lpv-1 50Gi RWO prometheus-lpv 13d prometheus-data-prometheus-1 Bound prometheus-lpv-0 50Gi RWO prometheus-lpv 13d 访问prometheus web UI # prometheus.yml中指定的job，相应target已经抓取到；可通过graph查询metrics\n​ 按照目前配置，prometheus、apiserver、容器指标数据都能获取到，但是主机及k8s相关资源指标数据还没有，还需要部署node-exporter及kube-state-metrics\n","date":"6 November 2019","permalink":"/2019/11/k8s-prometheus-1/","section":"博客","summary":"简介 # Prometheus是一个开源的监控系统，通过抓取或拉取应用程序中暴露的时间序列数据来工作。时间序列数据通常由应用程序本身通过客户端库或者成为exporter的代理来作为HTTP端点暴露。目前已经存在很多exporter和客户端库，支持多种编程语言、框架和开源应用程序。\n官方架构图 # Prometheus通过配置target，来抓取对应主机、进程、服务或应用程序的指标。一组具有相同角色的target被称为job。比如，定义kubernetes-nodes的job，来抓取集群所有主机的相关指标 服务发现（target） 静态资源列表 基于文件的发现，可使用自动化配置管理工具自动更新 自动发现，支持基于AWS/Consul/dns/kubernetes/marathon等的服务发现 Prometheus将收集时间序列数据存储在本地，也可以配置remote_write存储到其它时间序列数据库，比如OpenTSDB、InfluxDB、M3DB等 可视化: Prometheus内置了web UI，也可以和功能强大的开源仪表板Grafana集成，可自定义dashboard满足各种定制化需求 聚合和告警: Prometheus可以查询和聚合时间序列数据，并创建规则来记录常用的查询和聚合。还可以定义警报规则，满足条件时触发警报，并推送到Alertmanager服务。Alertmanagr支持高可用的集群部署，负责管理、整合和分发警报到不通目的地，并能做到告警收敛 数据模型 # Prometheus使用一个多维时间序列数据模型，结合了时间序列名称和称为标签(label)的键/值对，这些标签提供了维度。每个时间序列由时间序列名称和标签的组合唯一标识。 \u0026lt;time series name\u0026gt; {\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;,...}\nmachine_cpu_cores{beta_kubernetes_io_arch=\u0026ldquo;amd64\u0026rdquo;,beta_kubernetes_io_os=\u0026ldquo;linux\u0026rdquo;,instance=\u0026ldquo;192.168.1.51\u0026rdquo;,job=\u0026ldquo;kubernetes-cadvisor\u0026rdquo;,kubernetes_io_arch=\u0026ldquo;amd64\u0026rdquo;,kubernetes_io_hostname=\u0026ldquo;192.168.1.51\u0026rdquo;,kubernetes_io_os=\u0026ldquo;linux\u0026rdquo;} 2 machine_cpu_cores收集的是主机的cpu核数，taget label标识了是job kubernetes-cadvisor抓取的，instance 192.","title":"高可用prometheus监控集群搭建(一)"},{"content":"","date":"23 August 2019","permalink":"/tags/flannel/","section":"Tags","summary":"","title":"flannel"},{"content":"","date":"23 August 2019","permalink":"/series/k8s1.14.6%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","section":"Series","summary":"","title":"k8s1.14.6集群部署"},{"content":"1.Daemonset方式 # $ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml $ cat kube-flannel.yml apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026#34;/etc/cni/net.d\u0026#34; - pathPrefix: \u0026#34;/etc/kube-flannel\u0026#34; - pathPrefix: \u0026#34;/run/flannel\u0026#34; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [\u0026#39;NET_ADMIN\u0026#39;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unsed in CaaSP rule: \u0026#39;RunAsAny\u0026#39; --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: [\u0026#39;extensions\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: [\u0026#39;psp.flannel.unprivileged\u0026#39;] - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: | { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } net-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: kube-flannel-ds-amd64 namespace: kube-system labels: tier: node app: flannel spec: template: metadata: labels: tier: node app: flannel spec: hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.11.0-amd64 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 2. 静态Pod方式 # 2.1 master节点上创建flannel 相关的CRD\na) flannel pod 的PodSecurityPolicy，定义资源访问及配置限制 b) 访问apiserver的ServiceAccount, ClusterRole, ClusterRoleBinding\n$ kubectl apply -f kube-flannel-CRD.yaml $ cat kube-flannel-CRD.yaml apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026#34;/etc/cni/net.d\u0026#34; - pathPrefix: \u0026#34;/etc/kube-flannel\u0026#34; - pathPrefix: \u0026#34;/run/flannel\u0026#34; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [\u0026#39;NET_ADMIN\u0026#39;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unsed in CaaSP rule: \u0026#39;RunAsAny\u0026#39; --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel rules: - apiGroups: [\u0026#39;extensions\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: [\u0026#39;psp.flannel.unprivileged\u0026#39;] - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system 2.2 master节点上创建flannel访问apiserver的kubeconfig\nServiceAccount flannel声明后，自动创建secret对象，可获取secret中的token，用于flannel pod 访问apiserver $ cat create-flannel-kubeconfig.sh SECRET=$(kubectl get sa flannel -n kube-system -o jsonpath=\u0026#39;{.secrets[].name}\u0026#39;) TOKEN=$(kubectl get secret $SECRET -n kube-system -o jsonpath=\u0026#39;{.data.token}\u0026#39;|base64 -d) KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=flannel.kubeconfig kubectl config set-credentials flannel@kubernetes \\ --token=$TOKEN \\ --kubeconfig=flannel.kubeconfig kubectl config set-context flannel@kubernetes \\ --cluster=kubernetes \\ --user=flannel@kubernetes \\ --kubeconfig=flannel.kubeconfig kubectl config use-context flannel@kubernetes --kubeconfig=flannel.kubeconfig $ sh create-flannel-kubeconfig.sh 将生成的flannel.kubeconfig文件分发到node节点192.168.18.160的/root/k8s-1.14.6/conf/flannel目录下 2.3 计算节点192.168.18.160上创建flannel cni插件的配置文件\n$ cd /root/k8s-1.14.6/conf/flannel $ cat \u0026lt;\u0026lt;EOF \u0026gt; net-conf.json { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } EOF $ cp /etc/cni/net.d/10-flannel.conflist cni-conf.json 2.4 计算节点192.168.18.160上创建kube-flannel的静态pod文件kube-flannel-pod.yaml\napiVersion: v1 kind: Pod metadata: labels: tier: node app: flannel name: kube-flannel namespace: kube-system spec: hostNetwork: true restartPolicy: Always containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --kubeconfig-file=/etc/kube-flannel/flannel.kubeconfig resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: cni mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /root/k8s-1.14.6/conf/flannel 2.5 查看kube-flannel是否正常启动\n$ docker ps bbc6831c81f9 ff281650a721 \u0026#34;/opt/bin/flanneld -…\u0026#34; 16 seconds ago Up 15 seconds k8s_kube-flannel_kube-flannel-192.168.18.160_kube-system_b61385e9f07935caba0db5f9c2eb1a9e_1 f66c5be28bb1 k8s.gcr.io/pause:3.1 \u0026#34;/pause\u0026#34; 16 seconds ago Up 15 seconds k8s_POD_kube-flannel-192.168.18.160_kube-system_b61385e9f07935caba0db5f9c2eb1a9e_1 $ docker logs bbc6831c81f9 I1021 07:41:51.567961 1 main.go:514] Determining IP address of default interface I1021 07:41:51.568823 1 main.go:527] Using interface with name eth0 and address 29.20.18.160 I1021 07:41:51.568846 1 main.go:544] Defaulting external address to interface address (29.20.18.160) I1021 07:41:51.671121 1 kube.go:126] Waiting 10m0s for node controller to sync I1021 07:41:51.671194 1 kube.go:309] Starting kube subnet manager I1021 07:41:52.671328 1 kube.go:133] Node controller sync successful I1021 07:41:52.671376 1 main.go:244] Created subnet manager: Kubernetes Subnet Manager - 29.20.18.160 I1021 07:41:52.671382 1 main.go:247] Installing signal handlers I1021 07:41:52.671450 1 main.go:386] Found network config - Backend type: vxlan I1021 07:41:52.671505 1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I1021 07:41:52.673135 1 main.go:317] Wrote subnet file to /run/flannel/subnet.env I1021 07:41:52.673149 1 main.go:321] Running backend. I1021 07:41:52.673157 1 main.go:339] Waiting for all goroutines to exit I1021 07:41:52.673186 1 vxlan_network.go:60] watching for new subnet leases $ ifconfig flannel.1 flannel.1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1400 inet 10.244.4.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::74b1:a2ff:fefd:1e3d prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether 76:b1:a2:fd:1e:3d txqueuelen 0 (Ethernet) RX packets 397 bytes 50538 (49.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 403 bytes 55506 (54.2 KiB) TX errors 0 dropped 10 overruns 0 carrier 0 collisions 0 ","date":"23 August 2019","permalink":"/2019/08/k8s1.14.6-flannel/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e1.Daemonset方式 \n    \u003cdiv id=\"1daemonset方式\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1daemonset%e6%96%b9%e5%bc%8f\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e","title":"k8s1.14.6集群搭建之kube-flannel部署"},{"content":" kube-proxy组件的部署，可以选择使用Daemonset的方式或者StaticPod的方式\nDaemonset方式保证在加入集群的节点上都运行kube-proxy Pod\nStaticPod是由kubectl进行管理的运行在该Node上的Pod，不能通过apiserver进行管理\n1. Daemonset方式 # 在master节点上创建kube-proxy的CRD\n$ cd /root/k8s-1.14.6/manifests $ kubeclt apply -f kube-proxy.yaml $ cat kube-proxy.yaml kind: ConfigMap apiVersion: v1 metadata: labels: app: kube-proxy name: kube-proxy namespace: kube-system data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \u0026#34;\u0026#34; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /var/lib/kube-proxy/kubeconfig.conf qps: 5 clusterCIDR: 10.244.0.0/16 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; strictARP: false syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;\u0026#34; nodePortAddresses: null oomScoreAdj: -999 portRange: \u0026#34;\u0026#34; resourceContainer: /kube-proxy udpIdleTimeout: 250ms winkernel: enableDSR: false networkName: \u0026#34;\u0026#34; sourceVip: \u0026#34;\u0026#34; kubeconfig.conf: |- apiVersion: v1 kind: Config clusters: - cluster: certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt server: https://192.168.18.142:6443 name: default contexts: - context: cluster: default namespace: default user: default name: default current-context: default users: - name: default user: tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kube-proxy:node-proxier roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-proxier subjects: - kind: ServiceAccount name: kube-proxy namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-proxy namespace: kube-system --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: k8s-app: kube-proxy name: kube-proxy namespace: kube-system spec: template: metadata: labels: k8s-app: kube-proxy spec: containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=$(NODE_NAME) env: - name: NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: k8s.gcr.io/kube-proxy:v1.14.6 name: kube-proxy securityContext: privileged: true volumeMounts: - mountPath: /var/lib/kube-proxy name: kube-proxy - mountPath: /run/xtables.lock name: xtables-lock - mountPath: /lib/modules name: lib-modules readOnly: true hostNetwork: true priorityClassName: system-node-critical serviceAccount: kube-proxy serviceAccountName: kube-proxy tolerations: - key: CriticalAddonsOnly operator: Exists - operator: Exists volumes: - configMap: defaultMode: 420 name: kube-proxy name: kube-proxy - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock - hostPath: path: /lib/modules name: lib-modules updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate 2. 静态Pod的方式 # 2.1 master节点上签发kube-proxy的认证证书\n$ cd /root/k8s-1.14.6/ssl $ cat \u0026lt;\u0026lt;EOF \u0026gt; apiserver-kube-proxy-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;system:kube-proxy\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-kube-proxy-csr.json |cfssljson -bare apiserver-kube-proxy CN指定该证书的User为system:kube-proxy，kube-apiserver 预定义的 ClusterRoleBinding system:node-proxier 将User system:kube-proxy 与 ClusterRole stem:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限\n$ kubectl get clusterrolebinding system:node-proxier -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-07-09T05:58:48Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:node-proxier resourceVersion: \u0026#34;98\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Anode-proxier uid: e3e61976-d2c6-11e9-9f22-fa163e67ff45 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-proxier subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: system:kube-proxy 2.2 生成kube-proxy访问apiserver的kubeconfig\n$ cat create-kube-proxy-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials default \\ --client-certificate=/root/k8s-1.14.6/ssl/apiserver-kube-proxy.pem \\ --client-key=/root/k8s-1.14.6/ssl/apiserver-kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=default \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig $ sh create-kube-proxy-kubeconfig.sh 将生成的kube-proxy.kubeconfig文件分发到node节点192.168.18.160的/root/k8s-1.14.6/conf/kube-proxy目录下\n2.3 node节点上创建kube-prox的y配置文件config.conf\n$ cd /root/k8s-1.14.6/conf/kube-proxy $ ls config.conf kube-proxy.kubeconfig $ cat config.conf apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \u0026#34;\u0026#34; burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /var/lib/kube-proxy/kube-proxy.kubeconfig qps: 5 clusterCIDR: 10.244.0.0/16 configSyncPeriod: 15m0s conntrack: max: null maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \u0026#34;\u0026#34; iptables: masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; strictARP: false syncPeriod: 30s kind: KubeProxyConfiguration metricsBindAddress: 127.0.0.1:10249 mode: \u0026#34;\u0026#34; nodePortAddresses: null oomScoreAdj: -999 portRange: \u0026#34;\u0026#34; resourceContainer: /kube-proxy udpIdleTimeout: 250ms winkernel: enableDSR: false networkName: \u0026#34;\u0026#34; sourceVip: \u0026#34;\u0026#34; 2.4 计算节点上创建kube-proxy的静态pod文件kube-proxy-pod.yaml\n$ cd /root/k8s-1.14.6/manifests apiVersion: v1 kind: Pod metadata: labels: tier: node k8s-app: kube-proxy name: kube-proxy namespace: kube-system spec: containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=$(NODE_NAME) env: - name: NODE_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName image: k8s.gcr.io/kube-proxy:v1.14.6 name: kube-proxy securityContext: privileged: true volumeMounts: - mountPath: /var/lib/kube-proxy name: kube-proxy - mountPath: /run/xtables.lock name: xtables-lock - mountPath: /lib/modules name: lib-modules readOnly: true hostNetwork: true restartPolicy: Always priorityClassName: system-node-critical volumes: - hostPath: path: /root/k8s-1.14.6/conf/kube-proxy name: kube-proxy name: kube-proxy - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock - hostPath: path: /lib/modules name: lib-modules 2.5 查看kube-proxy是否正常启动\n$ docker ps 88f7d447327e ed8adf767eeb \u0026#34;/usr/local/bin/kube…\u0026#34; 3 minutes ago Up 3 minutes k8s_kube-proxy_kube-proxy-192.168.18.160_kube-system_6a4b4c4a6b6e6cb8c0c89cb8edd2d00c_1 2f0be7471cb1 k8s.gcr.io/pause:3.1 \u0026#34;/pause\u0026#34; 3 minutes ago Up 3 minutes k8s_POD_kube-proxy-192.168.18.160_kube-system_6a4b4c4a6b6e6cb8c0c89cb8edd2d00c_1 $ docker logs 88f7d447327e W1021 06:17:23.534438 1 proxier.go:498] Failed to load kernel module ip_vs with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules W1021 06:17:23.535741 1 proxier.go:498] Failed to load kernel module ip_vs_rr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules W1021 06:17:23.536945 1 proxier.go:498] Failed to load kernel module ip_vs_wrr with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules W1021 06:17:23.538102 1 proxier.go:498] Failed to load kernel module ip_vs_sh with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules W1021 06:17:23.543687 1 server_others.go:267] Flag proxy-mode=\u0026#34;\u0026#34; unknown, assuming iptables proxy I1021 06:17:23.553870 1 server_others.go:146] Using iptables Proxier. I1021 06:17:23.554171 1 server.go:562] Version: v1.14.6 I1021 06:17:23.565879 1 conntrack.go:52] Setting nf_conntrack_max to 131072 I1021 06:17:23.566114 1 config.go:202] Starting service config controller I1021 06:17:23.566145 1 controller_utils.go:1027] Waiting for caches to sync for service config controller I1021 06:17:23.566146 1 config.go:102] Starting endpoints config controller I1021 06:17:23.566178 1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller I1021 06:17:23.666289 1 controller_utils.go:1034] Caches are synced for service config controller I1021 06:17:23.666289 1 controller_utils.go:1034] Caches are synced for endpoints config controller ","date":"23 August 2019","permalink":"/2019/08/k8s1.14.6-kube-proxy/","section":"博客","summary":"kube-proxy组件的部署，可以选择使用Daemonset的方式或者StaticPod的方式\nDaemonset方式保证在加入集群的节点上都运行kube-proxy Pod\nStaticPod是由kubectl进行管理的运行在该Node上的Pod，不能通过apiserver进行管理\n1. Daemonset方式 # 在master节点上创建kube-proxy的CRD\n$ cd /root/k8s-1.14.6/manifests $ kubeclt apply -f kube-proxy.yaml $ cat kube-proxy.yaml kind: ConfigMap apiVersion: v1 metadata: labels: app: kube-proxy name: kube-proxy namespace: kube-system data: config.","title":"k8s1.14.6集群搭建之kube-proxy部署"},{"content":"前言 # 在部署k8s node节点时，kubelet要与apiserver交互，在双方没有互信的情况下，如何通过master的认证鉴权？kubelet bootstrap就解决了这个问题，本文使用Bootstrap Token Authentication的认证方式，使得kubelet通过apiserver的认证、获取apiserver的CA证书，并成功注册到集群中\n1. 创建kubelet bootstrap的kubeconfig # 1.1 master节点192.168.18.142上执行create-bootstrap-kubeconfig.sh脚本\n先创建bootstrap-token，再生成bootstrap.kubeconfig，再创建clusterrolebinding\n$ cat create-bootstrap-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; TOKEN_ID=$(openssl rand -hex 3) TOKEN_SECRET=$(openssl rand -hex 8) BOOTSTRAP_TOKEN=\u0026#34;${TOKEN_ID}.${TOKEN_SECRET}\u0026#34; AUTH_EXTRA_GROUPS=\u0026#34;system:bootstrappers:default-node-token\u0026#34; cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: bootstrap-token-${TOKEN_ID} namespace: kube-system type: bootstrap.kubernetes.io/token stringData: description: \u0026#34;The bootstrap token for k8s.\u0026#34; token-id: ${TOKEN_ID} token-secret: ${TOKEN_SECRET} expiration: 2029-07-16T00:00:00Z usage-bootstrap-authentication: \u0026#34;true\u0026#34; usage-bootstrap-signing: \u0026#34;true\u0026#34; auth-extra-groups: ${AUTH_EXTRA_GROUPS} EOF kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-credentials tls-bootstrap-token-user \\ --token ${BOOTSTRAP_TOKEN} \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-context tls-bootstrap-token-user@kubernetes \\ --cluster kubernetes \\ --user tls-bootstrap-token-user \\ --kubeconfig=bootstrap.kubeconfig kubectl config use-context tls-bootstrap-token-user@kubernetes --kubeconfig=bootstrap.kubeconfig #将自定义的auth-extra-groups绑定角色system:node-bootstrapper kubectl create clusterrolebinding kubelet-bootstrap \\ --clusterrole system:node-bootstrapper \\ --group ${AUTH_EXTRA_GROUPS} #将自定义的auth-extra-groups绑定角色,实现自动签署证书请求 kubectl create clusterrolebinding node-autoapprove-bootstrap \\ --clusterrole system:certificates.k8s.io:certificatesigningrequests:nodeclient \\ --group ${AUTH_EXTRA_GROUPS} #将system:node绑定角色,实现自动刷新node节点过期证书 kubectl create clusterrolebinding node-autoapprove-certificate-rotation \\ --clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \\ --group system:node $ sh create-bootstrap-kubeconfig.sh token的name必须是 bootstrap-token-\u0026lt;token-id\u0026gt; 的格式 token的type必须是 bootstrap.kubernetes.io/token token的token-id和token-secret分别是6位和16位数字和字母的组合 auth-extra-groups 定义了token代表的用户所属的额外的group，而默认group名为system:bootstrappers 这种类型token代表的用户名为 system:bootstrap:\u0026lt;token-id\u0026gt; expiration字段定义token的过期时间 1.2 将生成的bootstrap.kubeconfig分发到计算节点192.168.18.160上\n$ scp bootstrap.kubeconfig 192.168.18.160:/root/k8s-1.14.6/kubelet/ 2. 计算节点配置并启动kubelet # $ swapoff -a $ sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab $ cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system 2.1 创建kubelet.service\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; /usr/lib/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/ [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/root/k8s-1.14.6/kubelet/bootstrap.kubeconfig --kubeconfig=/root/k8s-1.14.6/kubelet/kubelet.conf\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/root/k8s-1.14.6/kubelet/config.yaml\u0026#34; EnvironmentFile=-/etc/sysconfig/kubelet ExecStart=/root/k8s-1.14.6/bin/kubelet \\$KUBELET_KUBECONFIG_ARGS \\$KUBELET_CONFIG_ARGS \\$KUBELET_EXTRA_ARGS Restart=always StartLimitInterval=0 RestartSec=10 [Install]\tWantedBy=multi-user.target EOF /root/k8s-1.14.6/kubelet/kubelet.conf是kubelet.service启动后生成的kubeconfig文件，用于访问apiserver\n2.2 创建kubelet配置文件config.yaml、/etc/sysconfig/kubelet\n$　cat config.yaml address: 0.0.0.0 apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /root/k8s-1.14.6/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd cgroupsPerQOS: true clusterDNS: - 172.16.0.10 clusterDomain: cluster.local configMapAndSecretChangeDetectionStrategy: Watch containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuCFSQuotaPeriod: 100ms cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kind: KubeletConfiguration kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeLeaseDurationSeconds: 40 nodeStatusReportFrequency: 1m0s nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 port: 10250 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /root/k8s-1.14.6/manifests streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s $ cat /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=\u0026#34;--hostname-override=192.168.18.160 --cert-dir=/root/k8s-1.14.6/kubelet/ssl --pod-infra-container-image=k8s.gcr.io/pause:3.1 --network-plugin=cni\u0026#34; 2.3 配置docker的cgroupdriver为systemd\n$ cat /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;0.0.0.0/0\u0026#34;], \u0026#34;hosts\u0026#34;: [\u0026#34;unix:///var/run/docker.sock\u0026#34;, \u0026#34;tcp://0.0.0.0:20008\u0026#34;], \u0026#34;graph\u0026#34;: \u0026#34;/data/docker\u0026#34;, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ], \u0026#34;userland-proxy\u0026#34;:false } 2.4 安装cni网络插件\nkubelet参数中\n--network-plugin=cni启用cni网络插件 --cni-conf-dir指定networkconfig目录，默认路径是:/etc/cni/net.d --cni-bin-dir指定插件可执行文件目录，默认路径是:/opt/cni/bin $ wget https://github.com/containernetworking/plugins/releases/download/v0.7.5/cni-plugins-amd64-v0.7.5.tgz $ mkdir -pv /opt/cni/bin $ tar xf cni-plugins-amd64-v0.7.5.tgz -C /opt/cni/bin $ ls -l /opt/cni/bin bridge dhcp flannel host-device host-local ipvlan loopback macvlan portmap ptp sample tuning vlan 2.5 创建cni配置文件\n本文的k8s集群使用flannel网络插件，修改配置如下:\n$ mkdir /etc/cni/net.d/ -pv $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/cni/net.d/10-flannel.conflist { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } EOF 2.6 启动kubelet.service\n$ systemctl daemon-reload $ systemctl start kubelet.service ","date":"23 August 2019","permalink":"/2019/08/k8s1.14.6-node/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e前言 \n    \u003cdiv id=\"前言\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e5%89%8d%e8%a8%80\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在部署k8s node节点时，kubelet要与apiserver交互，在双方没有互信的情况下，如何通过master的认证鉴权？kubelet bootstrap就解决了这个问题，本文使用Bootstrap Token Authentication的认证方式，使得kubelet通过apiserver的认证、获取apiserver的CA证书，并成功注册到集群中\u003c/p\u003e\n\u003c/blockquote\u003e","title":"k8s1.14.6集群搭建之node节点部署"},{"content":"","date":"22 August 2019","permalink":"/tags/etcd/","section":"Tags","summary":"","title":"etcd"},{"content":"1. 创建自签名CA及相关证书 # 1.1 创建自签名CA(apiserver访问相关)\n$ cd /root/k8s-1.14.6/ssl/ $ cat \u0026lt;\u0026lt;EOF \u0026gt; ca-config.json { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ] } } } } EOF $ cat \u0026lt;\u0026lt;EOF \u0026gt; ca-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ gencert -initca ca-csr.json|cfssljson -bare ca 1.2 签名证书apiserver.pem、apiserver-kubelet-client.pem\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; apiserver-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;kube-apiserver\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;172.16.0.1\u0026#34;, --\u0026gt; 为service-cluster-ip-range参数值 \u0026#34;127.0.0.1\u0026#34;, \u0026#34;192.168.18.142\u0026#34;, --\u0026gt; master集群所有节点ip \u0026#34;kubernetes\u0026#34;, \u0026#34;kubernetes.default\u0026#34;, \u0026#34;kubernetes.default.svc\u0026#34;, \u0026#34;kubernetes.default.svc.cluster\u0026#34;, \u0026#34;kubernetes.default.svc.cluster.local\u0026#34; --\u0026gt; clusterDomain ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json |cfssljson -bare apiserver $ cat \u0026lt;\u0026lt;EOF \u0026gt; apiserver-kubelet-client-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;kube-apiserver-kubelet-client\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-kubelet-client-csr.json |cfssljson -bare apiserver-kubelet-client 1.3 创建front-proxy自签名CA，开启k8s api aggregation要用\n$ cd /root/k8s-1.14.6/ssl/front-proxy $ cat \u0026lt;\u0026lt;EOF \u0026gt; ca-config.json { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ] } } } } EOF $ cat \u0026lt;\u0026lt;EOF \u0026gt; ca-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;front-proxy-ca\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ cfssl gencert -initca ca-csr.json|cfssljson -bare ca 1.4 签名front-proxy相关证书\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; front-proxy-client-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;front-proxy-client\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json |cfssljson -bare front-proxy-client 1.5 创建用于server-account认证的公钥与私钥\n$ openssl genrsa -out sa.key 2048 $ openssl rsa -in sa.key -out sa.pub -pubout 2. 部署apiserver # $ swapoff -a $ sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab $ cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl --system 2.1 二进制方式启动，由systemd托管\n$ kube-apiserver \\ --logtostderr=false \\ --v=2 \\ --log-file=/root/k8s-1.14.6/logs/kube-apiserver.log \\ --advertise-address=192.168.18.142 \\ --allow-privileged=true \\ --authorization-mode=Node,RBAC \\ --client-ca-file=/root/k8s-1.14.6/ssl/ca.pem \\ --enable-admission-plugins=NodeRestriction \\ --enable-bootstrap-token-auth=true \\ --etcd-cafile=/root/k8s-1.14.6/ssl/etcd/ca.pem \\ --etcd-certfile=/root/k8s-1.14.6/ssl/etcd/apiserver-etcd-client.pem \\ --etcd-keyfile=/root/k8s-1.14.6/ssl/etcd/apiserver-etcd-client-key.pem \\ --etcd-servers=https://192.168.18.142:2379 \\ --insecure-port=0 \\ --kubelet-client-certificate=/root/k8s-1.14.6/ssl/apiserver-kubelet-client.pem \\ --kubelet-client-key=/root/k8s-1.14.6/ssl/apiserver-kubelet-client-key.pem \\ --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\ --proxy-client-cert-file=/root/k8s-1.14.6/ssl/front-proxy/front-proxy-client.pem \\ --proxy-client-key-file=/root/k8s-1.14.6/ssl/front-proxy/front-proxy-client-key.pem \\ --requestheader-allowed-names=front-proxy-client \\ --requestheader-client-ca-file=/root/k8s-1.14.6/ssl/front-proxy/ca.pem \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --secure-port=6443 \\ --service-account-key-file=/root/k8s-1.14.6/ssl/sa.pub \\ --service-cluster-ip-range=172.16.0.0/16 \\ --tls-cert-file=/root/k8s-1.14.6/ssl/apiserver.pem \\ --tls-private-key-file=/root/k8s-1.14.6/ssl/apiserver-key.pem 2.2 Staticpod方式\n2.2.1 Master节点部署kubelet.service\n$ swapoff –a --\u0026gt; 还要注释/etc/fstab中的swap挂载 $ cat \u0026lt;\u0026lt;EOF \u0026gt; /usr/lib/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/ [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/root/k8s-1.14.6/kubelet/bootstrap.kubeconfig --kubeconfig=/root/k8s-1.14.6/kubelet/kubelet.conf\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/root/k8s-1.14.6/kubelet/config.yaml\u0026#34; EnvironmentFile=-/etc/sysconfig/kubelet ExecStart=/root/k8s-1.14.6/bin/kubelet \\$KUBELET_KUBECONFIG_ARGS \\$KUBELET_CONFIG_ARGS \\$KUBELET_EXTRA_ARGS Restart=always StartLimitInterval=0 RestartSec=10 [Install]\tWantedBy=multi-user.target EOF 2.2.2 生成kubelet的kubeconfig文件kubelet.conf\n$ cat create-kubelet-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; --\u0026gt; 高可用master集群的HA或者F5地址 HOSTNAME=\u0026#34;192.168.18.142\u0026#34; --\u0026gt; 对应master节点IP kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kubelet.conf kubectl config set-credentials system:node:${HOSTNAME} \\ --client-certificate=/root/k8s-1.14.6/ssl/apiserver-kubelet-client.pem \\ --client-key=/root/k8s-1.14.6/ssl/apiserver-kubelet-client-key.pem \\ --embed-certs=true \\ --kubeconfig=kubelet.conf kubectl config set-context system:node:${HOSTNAME}@kubernetes \\ --cluster=kubernetes \\ --user=system:node:${HOSTNAME} \\ --kubeconfig=kubelet.conf kubectl config use-context system:node:${HOSTNAME}@kubernetes --kubeconfig=kubelet.conf $ sh create-kubelet-kubeconfig.sh 2.2.3 创建kubelet配置文件config.yaml与/etc/sysconfig/kubelet\n$ cat config.yaml address: 0.0.0.0 apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /root/k8s-1.14.6/ssl/ca.pem authorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s cgroupDriver: systemd --\u0026gt; 默认为cgroups cgroupsPerQOS: true clusterDNS: - 172.16.0.10 --\u0026gt; k8s集群DNS服务的clusterIP clusterDomain: cluster.local --\u0026gt; k8s集群DNS服务的domain configMapAndSecretChangeDetectionStrategy: Watch containerLogMaxFiles: 5 containerLogMaxSize: 10Mi contentType: application/vnd.kubernetes.protobuf cpuCFSQuota: true cpuCFSQuotaPeriod: 100ms cpuManagerPolicy: none cpuManagerReconcilePeriod: 10s enableControllerAttachDetach: true enableDebuggingHandlers: true enforceNodeAllocatable: - pods eventBurst: 10 eventRecordQPS: 5 evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5% evictionPressureTransitionPeriod: 5m0s failSwapOn: true fileCheckFrequency: 20s hairpinMode: promiscuous-bridge healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 20s imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 imageMinimumGCAge: 2m0s iptablesDropBit: 15 iptablesMasqueradeBit: 14 kind: KubeletConfiguration kubeAPIBurst: 10 kubeAPIQPS: 5 makeIPTablesUtilChains: true maxOpenFiles: 1000000 maxPods: 110 nodeLeaseDurationSeconds: 40 nodeStatusReportFrequency: 1m0s nodeStatusUpdateFrequency: 10s oomScoreAdj: -999 podPidsLimit: -1 port: 10250 registryBurst: 10 registryPullQPS: 5 resolvConf: /etc/resolv.conf rotateCertificates: true runtimeRequestTimeout: 2m0s serializeImagePulls: true staticPodPath: /root/k8s-1.14.6/manifests --\u0026gt; kubetlet读取静态Pod文件的路径 streamingConnectionIdleTimeout: 4h0m0s syncFrequency: 1m0s volumeStatsAggPeriod: 1m0s $ cat /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=\u0026#34;--hostname-override=192.168.18.142 --cert-dir=/root/k8s-1.14.6/kubelet/ssl --pod-infra-container-image=k8s.gcr.io/pause:3.1 --network-plugin=cni\u0026#34; 2.2.4 修改docker的cgroupdriver为systemd\n$ cat /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;0.0.0.0/0\u0026#34;], \u0026#34;hosts\u0026#34;: [\u0026#34;unix:///var/run/docker.sock\u0026#34;, \u0026#34;tcp://0.0.0.0:20008\u0026#34;], \u0026#34;graph\u0026#34;: \u0026#34;/data/docker\u0026#34;, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ], \u0026#34;tlsverify\u0026#34;: true, \u0026#34;tlscacert\u0026#34;: \u0026#34;/etc/docker/ca.pem\u0026#34;, \u0026#34;tlscert\u0026#34;: \u0026#34;/etc/docker/server-cert.pem\u0026#34;, \u0026#34;tlskey\u0026#34;: \u0026#34;/etc/docker/server-key.pem\u0026#34;, \u0026#34;userland-proxy\u0026#34;:false } 2.2.5 创建cni配置文件\n$ mkdir /etc/cni/net.d/ -pv $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/cni/net.d/10-flannel.conflist { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } EOF 2.2.6 创建静态pod文件kube-apiserver-pod.yaml\n$ cd /root/k8s-1.14.6/manifests $ cat kube-apiserver-pod.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --logtostderr=false - --log-file=/var/log/kube-apiserver.log - --advertise-address=192.168.18.142 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/etc/kubernetes/pki/ca.pem - --enable-admission-plugins=NodeRestrictionc - --enable-bootstrap-token-auth=true - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem - --etcd-certfile=/etc/kubernetes/pki/etcd/apiserver-etcd-client.pem - --etcd-keyfile=/etc/kubernetes/pki/etcd/apiserver-etcd-client-key.pem - --etcd-servers=https://192.168.18.142:2379 --\u0026gt; etcd集群逗号分隔 - --insecure-port=0 - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.pem - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client-key.pem - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy/front-proxy-client.pem - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy/front-proxy-client-key.pem - --requestheader-allowed-names=front-proxy-client - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy/ca.pem - --requestheader-extra-headers-prefix=X-Remote-Extra- - --requestheader-group-headers=X-Remote-Group - --requestheader-username-headers=X-Remote-User - --secure-port=6443 - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-cluster-ip-range=172.16.0.0/16 - --tls-cert-file=/etc/kubernetes/pki/apiserver.pem - --tls-private-key-file=/etc/kubernetes/pki/apiserver-key.pem image: k8s.gcr.io/kube-apiserver:v1.14.6 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 192.168.18.142 path: /healthz port: 6443 scheme: HTTPS initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-apiserver resources: requests: cpu: 250m volumeMounts: - mountPath: /etc/ssl/certs name: ca-certs readOnly: true - mountPath: /etc/pki name: etc-pki readOnly: true - mountPath: /etc/kubernetes/pki name: k8s-certs readOnly: true - mountPath: /var/log name: logs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/ssl/certs type: DirectoryOrCreate name: ca-certs - hostPath: path: /etc/pki type: DirectoryOrCreate name: etc-pki - hostPath: path: /root/k8s-1.14.6/ssl name: k8s-certs - hostPath: path: /root/k8s-1.14.6/logs name: logs status: {} 2.2.7 启动kubelet.service\n$ systemctl start kubelet.service 3. 配置kubectl访问apiserver # $ cd /root/k8s-1.14.6/conf $ cat create-admin-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=admin.conf kubectl config set-credentials kubernetes-admin \\ --client-certificate=/root/k8s-1.14.6/ssl/apiserver-kubelet-client.pem \\ --client-key=/root/k8s-1.14.6/ssl/apiserver-kubelet-client-key.pem \\ --embed-certs=true \\ --kubeconfig=admin.conf kubectl config set-context kubernetes-admin@kubernetes \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=admin.conf kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=admin.conf $ sh create-admin-kubeconfig.sh $ mv admin.conf /root/.kube/config $ kubectl get cs ","date":"22 August 2019","permalink":"/2019/08/k8s1.14.6-apiserver/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e1. 创建自签名CA及相关证书 \n    \u003cdiv id=\"1-创建自签名ca及相关证书\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1-%e5%88%9b%e5%bb%ba%e8%87%aa%e7%ad%be%e5%90%8dca%e5%8f%8a%e7%9b%b8%e5%85%b3%e8%af%81%e4%b9%a6\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e1.1 创建自签名CA(apiserver访问相关)\u003c/p\u003e","title":"k8s1.14.6集群搭建之apiserver部署"},{"content":"1. 创建kubeconfig # 创建controller-manager访问apiserver的kubeconfig\n$ cd /root/k8s-1.14.6/conf $ cat create-controller-manager-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=controller-manager.conf kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=/root/k8s-1.14.6/ssl/apiserver-kubelet-client.pem \\ --client-key=/root/k8s-1.14.6/ssl/apiserver-kubelet-client-key.pem \\ --embed-certs=true \\ --kubeconfig=controller-manager.conf kubectl config set-context system:kube-controller-manager@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=controller-manager.conf kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.conf $ sh create-controller-manager-kubeconfig.sh 2. 部署controller-manager # 2.1 二进制方式启动\nkube-controller-manager \\ --logtostderr=false \\ --v=2 \\ --log-file=/root/k8s-1.14.6/logs/kube-controller.log \\ --allocate-node-cidrs=true \\ --authentication-kubeconfig=/root/k8s-1.14.6/conf/controller-manager.conf \\ --authorization-kubeconfig=/root/k8s-1.14.6/conf/controller-manager.conf \\ --bind-address=127.0.0.1 \\ --client-ca-file=/root/k8s-1.14.6/ssl/ca.pem \\ --cluster-cidr=10.244.0.0/16 \\ --\u0026gt; cni插件flannel的ip区间 --cluster-signing-cert-file=/root/k8s-1.14.6/ssl/ca.pem \\ --cluster-signing-key-file=/root/k8s-1.14.6/ssl/ca-key.pem \\ --controllers=*,bootstrapsigner,tokencleaner \\ --kubeconfig=/root/k8s-1.14.6/conf/controller-manager.conf \\ --leader-elect=true \\ --node-cidr-mask-size=24 \\ --requestheader-client-ca-file=/root/k8s-1.14.6/ssl/front-proxy/ca.pem \\ --root-ca-file=/root/k8s-1.14.6/ssl/ca.pem \\ --service-account-private-key-file=/root/k8s-1.14.6/ssl/sa.key \\ --use-service-account-credentials=true 默认监听10252、10257端口\n2.2 StaticPod方式部署\n创建静态pod文件kube-controller-manage-pod.yaml\n$ cd /root/k8s-1.14.6/manifests apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-controller-manager tier: control-plane name: kube-controller-manager namespace: kube-system spec: containers: - command: - kube-controller-manager - --logtostderr=false - --v=2 - --log-file=/var/log/kube-controller.log - --allocate-node-cidrs=true - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf - --bind-address=127.0.0.1 - --client-ca-file=/etc/kubernetes/pki/ca.pem - --cluster-cidr=10.244.0.0/16 --\u0026gt; cni插件flannel的ip区间 - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem - --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem - --controllers=*,bootstrapsigner,tokencleaner - --kubeconfig=/etc/kubernetes/controller-manager.conf - --leader-elect=true - --node-cidr-mask-size=24 - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy/ca.pem - --root-ca-file=/etc/kubernetes/pki/ca.pem - --service-account-private-key-file=/etc/kubernetes/pki/sa.key - --use-service-account-credentials=true image: k8s.gcr.io/kube-controller-manager:v1.14.6 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10252 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-controller-manager resources: requests: cpu: 200m volumeMounts: - mountPath: /etc/ssl/certs name: ca-certs readOnly: true - mountPath: /etc/pki name: etc-pki readOnly: true - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec name: flexvolume-dir - mountPath: /etc/kubernetes/pki name: k8s-certs readOnly: true - mountPath: /etc/kubernetes/controller-manager.conf name: kubeconfig readOnly: true - mountPath: /var/log name: logs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/ssl/certs type: DirectoryOrCreate name: ca-certs - hostPath: path: /etc/pki type: DirectoryOrCreate name: etc-pki - hostPath: path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec type: DirectoryOrCreate name: flexvolume-dir - hostPath: path: /root/k8s-1.14.6/ssl type: DirectoryOrCreate name: k8s-certs - hostPath: path: /root/k8s-1.14.6/conf/controller-manager.conf type: FileOrCreate name: kubeconfig - hostPath: path: /root/k8s-1.14.6/logs name: logs status: {} ","date":"22 August 2019","permalink":"/2019/08/k8s1.14.6-controller-manager/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e1. 创建kubeconfig \n    \u003cdiv id=\"1-创建kubeconfig\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1-%e5%88%9b%e5%bb%bakubeconfig\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e创建controller-manager访问apiserver的kubeconfig\u003c/p\u003e","title":"k8s1.14.6集群搭建之controller-manager部署"},{"content":"1. 创建自签名CA及相关证书 # 1.1 创建自签名CA\ncd /root/k8s-1.14.6/ssl/etcd cat \u0026lt;\u0026lt;EOF \u0026gt; ca-config.json { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;etcd\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;87600h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34; ] } } } } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; ca-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;etcd-ca\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34; } ] } EOF cfssl gencert -initca ca-csr.json|cfssljson -bare ca ls /root/k8s-1.14.6/ssl/etcd ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem 1.2 签名证书\n使用1.1步骤创建的CA(ca.pem)签名相关证书server.pem、peer.pem、apiserver-etcd-client.pem\ncat \u0026lt;\u0026lt;EOF \u0026gt; server-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;etcd-ca\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;192.168.18.142\u0026#34; --\u0026gt; hosts: etcd集群中所有节点 ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34; } ] } EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json|cfssljson -bare server cat \u0026lt;\u0026lt;EOF \u0026gt; peer-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;etcd-ca\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;192.168.18.142\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34; } ] } EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd peer-csr.json|cfssljson -bare peer cat \u0026lt;\u0026lt;EOF \u0026gt; apiserver-etcd-client-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;kube-apiserver-etcd-client\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;\u0026#34; } ] EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd apiserver-etcd-client-csr.json |cfssljson -bare apiserver-etcd-client 2. 部署ETCD # 以单节点为例，容器化部署etcd\ndocker run \\ --detach \\ --name k8s1.14.6-etcd-cluster \\ --restart always \\ --volume /root/k8s-1.14.6/ssl/etcd/:/etcd/ssl \\ --volume /root/k8s-1.14.6/etcd/:/etcd/data \\ --log-driver json-file \\ --log-opt max-file=5 \\ --log-opt max-size=50m \\ --network host \\ quay.io/coreos/etcd:v3.3.10 \\ /usr/local/bin/etcd \\ --advertise-client-urls=https://192.168.18.142:2379 \\ --cert-file=/etcd/ssl/server.pem \\ --client-cert-auth=true \\ --data-dir=/etcd/data \\ --initial-advertise-peer-urls=https://192.168.18.142:2380 \\ --initial-cluster=etcd-node1=https://192.168.18.142:2380 \\ --\u0026gt; 多点集群逗号分隔 --key-file=/etcd/ssl/server-key.pem \\ --listen-client-urls=https://192.168.18.142:2379 \\ --listen-peer-urls=https://192.168.18.142:2380 \\ --name=etcd-node1 \\ --\u0026gt; 当前节点name --peer-cert-file=/etcd/ssl/peer.pem \\ --peer-client-cert-auth=true \\ --peer-key-file=/etcd/ssl/peer-key.pem \\ --peer-trusted-ca-file=/etcd/ssl/ca.pem \\ --snapshot-count=10000 \\ --trusted-ca-file=/etcd/ssl/ca.pem 2.1 etcd集群服务验证\n$ docker exec -it k8s1.14.6-etcd-cluster sh etcdctl --ca-file=/etcd/ssl/ca.pem \\ --cert-file=/etcd/ssl/server.pem \\ --key-file=/etcd/ssl/server-key.pem \\ --endpoints=\u0026#34;https://192.168.18.142:2379\u0026#34; cluster-health member 3980bdec1233ede9 is healthy: got healthy result from https://192.168.18.142:2379 cluster is healthy ","date":"22 August 2019","permalink":"/2019/08/k8s1.14.6-etcd/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e1. 创建自签名CA及相关证书 \n    \u003cdiv id=\"1-创建自签名ca及相关证书\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1-%e5%88%9b%e5%bb%ba%e8%87%aa%e7%ad%be%e5%90%8dca%e5%8f%8a%e7%9b%b8%e5%85%b3%e8%af%81%e4%b9%a6\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e1.1 创建自签名CA\u003c/p\u003e","title":"k8s1.14.6集群搭建之ETCD集群部署"},{"content":"1. 创建kubeconfig # 创建scheduler访问的apiserver的kubeconfig\n$ cd /root/k8s-1.14.6/conf $ cat create-scheduler-kubeconfig.sh KUBE_APISERVER=\u0026#34;https://192.168.18.142:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/root/k8s-1.14.6/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=scheduler.conf kubectl config set-credentials system:kube-scheduler \\ --client-certificate=/root/k8s-1.14.6/ssl/apiserver-kubelet-client.pem \\ --client-key=/root/k8s-1.14.6/ssl/apiserver-kubelet-client-key.pem \\ --embed-certs=true \\ --kubeconfig=scheduler.conf kubectl config set-context system:kube-scheduler@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=scheduler.conf kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=scheduler.conf $ sh create-scheduler-kubeconfig.sh 2. 部署kube-scheduler # 2.1 二进制方式启动\nkube-scheduler \\ --logtostderr=false \\ --v=2 \\ --log-file=/root/k8s-1.14.6/logs/kube-scheduler.log \\ --bind-address=127.0.0.1 \\ --kubeconfig=/root/k8s-1.14.6/conf/scheduler.conf \\ --leader-elect=true 默认监听10251、10259端口\n2.2 StaticPod方式部署\n创建静态pod文件kube-scheduler-pod.yaml\n$ cd /root/k8s-1.14.6/manifests apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --logtostderr=false - --v=2 - --log-file=/var/log/kube-scheduler.log - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true image: k8s.gcr.io/kube-scheduler:v1.14.6 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10251 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-scheduler resources: requests: cpu: 100m volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true - mountPath: /var/log name: logs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /root/k8s-1.14.6/conf/scheduler.conf type: FileOrCreate name: kubeconfig - hostPath: path: /root/k8s-1.14.6/logs name: logs status: {} 3. 查看集群状态 # $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} ","date":"22 August 2019","permalink":"/2019/08/k8s1.14.6-scheduler/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e1. 创建kubeconfig \n    \u003cdiv id=\"1-创建kubeconfig\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1-%e5%88%9b%e5%bb%bakubeconfig\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cp\u003e创建scheduler访问的apiserver的kubeconfig\u003c/p\u003e","title":"k8s1.14.6集群搭建之scheduler部署"},{"content":"","date":"24 March 2019","permalink":"/tags/nginx/","section":"Tags","summary":"","title":"nginx"},{"content":"前言 # 当nginx代理的后端服务器有301、302跳转时，需要注意指定proxy_set_header Host请求头部，会直接影响nginx反馈给客户端的URL请求，要结合实际需求调整proxy_set_header Host和proxy_redirect将被代理服务器设置在response header中的Location做转换。\n搭建测试环境 # 1.测试架构规划\n主机192.168.196.134 nginx虚拟主机www.test.com；监听端口31001；转发文根 /th；后端server 192.168.196.135:31009 nginx虚拟主机192.168.196.134；监听端口31000；转发文根 *.jsp；后端server 192.168.196.136:31005 主机192.168.196.135 nginx虚拟主机 192.168.196.135；监听端口31009；转发文根 /th/ft；后端server 192.168.196.134:31000 主机192.168.196.136 tomcat server; 监听端口31005 192.168.196.134:31001/th \u0026mdash;\u0026gt; 192.168.196.135:31009/th/ft \u0026ndash;\u0026gt; 192.168.196.134:31000 *.jsp \u0026ndash;\u0026gt; 192.168.196.136:31005\n2.192.168.196.136上部署war包\nsh-4.2$ cat test.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=ISO-8859-1\u0026#34; pageEncoding=\u0026#34;ISO-8859-1\u0026#34;%\u0026gt; \u0026lt;% out.println(\u0026#34;Protocol: \u0026#34; + request.getProtocol()); out.println(\u0026#34;Server Name: \u0026#34; + request.getServerName()); out.println(\u0026#34;Server Port: \u0026#34; + request.getServerPort()); out.println(\u0026#34;Remote Addr: \u0026#34; + request.getRemoteAddr()); out.println(\u0026#34;Remote Host: \u0026#34; + request.getRemoteHost()); out.println(\u0026#34;Request URL: \u0026#34; + request.getRequestURL()); out.println(\u0026#34;Host: \u0026#34; + request.getHeader(\u0026#34;Host\u0026#34;)); out.println(\u0026#34;Connection : \u0026#34; + request.getHeader(\u0026#34;Connection\u0026#34;)); %\u0026gt; $jar cvf th.war th/ft/test.jsp 3.192.168.196.134上配置 nginx\nupstream backendsrv { server 192.168.196.136:31005; } server { listen 31001; server_name www.test.com; location /th { root html; index index.html index.htm; proxy_pass http://192.168.196.135:31009; #proxy_redirect ~^http://www.test.com:31009(.*) http://$host:$server_port$1; proxy_set_header Host $http_host; } } server { listen 31000; server_name www.firsthost.com 192.168.196.134; location /th/ft { root html; index index.html index.htm; } location ~* .jsp$ { proxy_pass http://backendsrv; proxy_set_header Host $http_host; } } 4.192.168.196.135上配置nginx\nupstream backendnginx { server 192.168.196.134:31000; } server { listen 31009; server_name 192.168.196.135; location /th/ft { root html; index index.html index.htm; proxy_pass http://backendnginx; #proxy_redirect ~^http://www.test.com:31000(.*) http://$host:$server_port$1; proxy_set_header Host $http_host; } location /th { root html; index index.html index.htm; } } 5.按照当前配置在客户端访问:\n$curl -L www.test.com:31001/th/ft/test.jsp Protocol: HTTP/1.0 Server Name: www.test.com Server Port: 31001 Remote Addr: 192.168.196.134 Remote Host: 192.168.196.134 Request URL: http://www.test.com:31001/th/ft/test.jsp Host: www.test.com:31001 Connection : close 由于每个转发文根下都设置了Host请求头(proxy_set_header Host $http_host;)，最终tomcat应用服务器从请求中获取的Host信息为面向客户端的第一级代理nginx的虚拟主机server_name及监听端口\n$curl -I www.test.com:31001/th HTTP/1.1 301 Moved Permanently Server: nginx Date: Thu, 21 Mar 2019 02:50:47 GMT Content-Type: text/html Content-Length: 178 Connection: keep-alive Location: http://www.test.com:31009/th/ $curl -I www.test.com:31001/th/ft HTTP/1.1 301 Moved Permanently Server: nginx Date: Thu, 21 Mar 2019 02:50:51 GMT Content-Type: text/html Content-Length: 178 Connection: keep-alive Location: http://www.test.com:31000/th/ft/ 301跳转时，返回给客户端重新发送请求的Location中URL有问题。 后端server的响应头中的location字段信息为: 1.host值被设置为第一个转发Location所在虚拟主机的server_name www.test.com\n这里要注意的时，如果nginx转发Location中未设置Host请求头，则host值为proxy_pass后指定的地址\n2.端口被设置为每个转发Location中proxy_pass/upstream定义的端口\n此时要用proxy_redirect将被代理服务器设置在response header中的Location做转换。官方解释：\nproxy_redirect 语法：proxy_redirect [ default|off|redirect replacement ] 默认值：proxy_redirect default 使用字段：http, server, location 如果需要修改从被代理服务器传来的应答头中的\u0026quot;Location\u0026quot;和\u0026quot;Refresh\u0026quot;字段，可以用这个指令设置。\n6.根据上面的访问结果，对应跳转地址：/th \u0026ndash;\u0026gt; /th/和/th/ft \u0026ndash;\u0026gt; /th/ft/\n在相应的转发Location下，分别添加:\nproxy_redirect ~^http://www.test.com:31009(.*) http://$host:$server_port$1; proxy_redirect ~^http://www.test.com:31000(.*) http://$host:$server_port$1; $host和$server_port 分别为转发文根Location所在虚拟主机的server_name和listen port\n7.reload Nginx后，从客户端访问\n$curl -I www.test.com:31001/th HTTP/1.1 301 Moved Permanently Server: nginx Date: Thu, 21 Mar 2019 03:15:10 GMT Content-Type: text/html Content-Length: 178 Connection: keep-alive Location: http://www.test.com:31001/th/ $curl -I www.test.com:31001/th/ft HTTP/1.1 301 Moved Permanently Server: nginx Date: Thu, 21 Mar 2019 03:18:22 GMT Content-Type: text/html Content-Length: 178 Connection: keep-alive Location: http://www.test.com:31001/th/ft 301跳转时，返回给客户端重新发送请求的Location中URL已正确显示\n最佳实践 # 依据转发Location中Host请求头是否被指定，来修改proxy_redirect的值:\n如果Host请求头未指定过，则后端server返回的Location为nginx中proxy_pass后的servername/IP与proxy_pass后的端口或upstream中转发端口的拼接\n如果Host请求头在当前转发Location指定\nHost在nginx收到的请求中未指定过，则后端server返回的Location为nginx当前转发Location的server_name与proxy_pass后的端口或upstream中转发端口的拼接 Host在nginx收到的请求中已指定，则后端server返回的Location为nginx接收的请求中Host与proxy_pass后的端口或upstream中转发端口的拼接 ","date":"24 March 2019","permalink":"/2019/03/nginx-proxy_redirect/","section":"博客","summary":"前言 # 当nginx代理的后端服务器有301、302跳转时，需要注意指定proxy_set_header Host请求头部，会直接影响nginx反馈给客户端的URL请求，要结合实际需求调整proxy_set_header Host和proxy_redirect将被代理服务器设置在response header中的Location做转换。\n搭建测试环境 # 1.测试架构规划\n主机192.168.196.134 nginx虚拟主机www.test.com；监听端口31001；转发文根 /th；后端server 192.168.196.135:31009 nginx虚拟主机192.168.196.134；监听端口31000；转发文根 *.jsp；后端server 192.168.196.136:31005 主机192.168.196.135 nginx虚拟主机 192.168.196.135；监听端口31009；转发文根 /th/ft；后端server 192.168.196.134:31000 主机192.168.196.136 tomcat server; 监听端口31005 192.168.196.134:31001/th \u0026mdash;\u0026gt; 192.168.196.135:31009/th/ft \u0026ndash;\u0026gt; 192.168.196.134:31000 *.","title":"Nginx的proxy_redirect配置"},{"content":"前言 # nginx反向代理后端应用服务器时，遇到web应用服务器需要获取客户端真实IP，该如何正确配置nginx?本文分别在一级反向代理及两级反向代理情况下，测试请求头X-Real-IP与X-Forwarded-For如何配置\n一级代理测试验证 # 1.测试架构: nginx反向代理 + tomcat web服务\nip server server port 192.168.196.135 nginx 31005 192.168.196.136 tomcat 31001 192.168.196.1 客户端 - 2.部署tomcat测试war包\n$cat test.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=ISO-8859-1\u0026#34; pageEncoding=\u0026#34;ISO-8859-1\u0026#34;%\u0026gt; \u0026lt;% out.println(\u0026#34;Protocol: \u0026#34; + request.getProtocol()); out.println(\u0026#34;Server Name: \u0026#34; + request.getServerName()); out.println(\u0026#34;Server Port: \u0026#34; + request.getServerPort()); out.println(\u0026#34;Remote Addr: \u0026#34; + request.getRemoteAddr()); out.println(\u0026#34;Remote Host: \u0026#34; + request.getRemoteHost()); out.println(\u0026#34;Request URL: \u0026#34; + request.getRequestURL()); out.println(\u0026#34;Host: \u0026#34; + request.getHeader(\u0026#34;Host\u0026#34;)); out.println(\u0026#34;X-Real-IP: \u0026#34; + request.getHeader(\u0026#34;X-Real-IP\u0026#34;)); out.println(\u0026#34;X-Forwarded-For: \u0026#34; + request.getHeader(\u0026#34;X-Forwarded-For\u0026#34;)); out.println(\u0026#34;Connection : \u0026#34; + request.getHeader(\u0026#34;Connection\u0026#34;)); %\u0026gt; $jar cvf th.war th/test.jsp 3.修改nginx.conf\nupstream backendsrv { server 192.168.196.136:31005; } server { listen 31001; server_name www.test.com; location /th { root html; index index.html index.htm; proxy_pass http://backendsrv; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } nginx重设请求头X-Real-IP为$remote_addr、 X-Forwarded-For为$proxy_add_x_forwarded_for\n$proxy_add_x_forwarded_for:\n$remote_addr变量值添加在客户端“X-Forwarded-For”请求头的后面，并以逗号分隔。 如果客户端请求未携带“X-Forwarded-For”请求头，$proxy_add_x_forwarded_for 变量值将与$remote_addr变量相同\n4.使用curl构造X-Real-IP、X-Forwarded-For请求头，在客户端发起请求\n$curl -H \u0026#34;X-Forwarded-For: 1.1.1.1\u0026#34; -H \u0026#34;X-Real-IP: 2.2.2.2\u0026#34; 192.168.196.135:31001/th/test.jsp Protocol: HTTP/1.0 Server Name: 192.168.196.135 Server Port: 31001 Remote Addr: 192.168.196.135 Remote Host: 192.168.196.135 Request URL: http://192.168.196.135:31001/th/test.jsp Host: 192.168.196.135:31001 X-Real-IP: 192.168.196.1 X-Forwarded-For: 1.1.1.1, 192.168.196.1 Connection : close 根据请求结果看出：\nnginx清理了伪造的X-Real-IP请求头并重写为clientIP- nginx在X-Forwarded-For后追加clientIP 后端web应用可通过X-Real-IP或者X-Forwarded-For的最后一个IP获取clientIP\n5.本例中，nginx为一级代理，为避免客户端伪造X-Forwarded-For请求头，可添加 proxy_set_header X-Forwarded-For $remote_addr\nupstream backendsrv { server 192.168.196.136:31005; } server { listen 31001; server_name www.test.com; location /th { root html; index index.html index.htm; proxy_pass http://backendsrv; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } $curl -H \u0026#34;X-Forwarded-For: 1.1.1.1\u0026#34; 192.168.196.135:31001/th/test.jsp Protocol: HTTP/1.0 Server Name: 192.168.196.135 Server Port: 31001 Remote Addr: 192.168.196.135 Remote Host: 192.168.196.135 Request URL: http://192.168.196.135:31001/th/test.jsp Host: 192.168.196.135:31001 X-Real-IP: 192.168.196.1 X-Forwarded-For: 192.168.196.1 Connection : close 二级代理测试验证 # 1.测试架构: nginx1反向代理 +nginx2反向代理+ tomcat web服务\nip server server port 192.168.196.133 nginx1 31009 192.168.196.135 nginx2 31001 192.168.196.136 tomcat 31005 192.168.196.1 client - 2.修改nginx.conf\n一级反向代理nginx1 upstream backendnginx { server 192.168.196.135:31001; } server { listen 31009; server_name 192.168.196.133; location /th { root html; index index.html index.htm; proxy_pass http://backendnginx; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 二级反向代理nginx2 upstream backendsrv { server 192.168.196.136:31005; } server { listen 31001; server_name www.test.com; location /th/ft { root html; index index.html index.htm; proxy_pass http://backendsrv; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 3.使用curl构造X-Forwarded-For请求头，在客户端发起请求\n$curl -H \u0026#34;X-Forwarded-For: unknown, 1.1.1.1\u0026#34; 192.168.196.133:31009/th/ft/test.jsp Protocol: HTTP/1.0 Server Name: 192.168.196.133 Server Port: 31009 Remote Addr: 192.168.196.135 Remote Host: 192.168.196.135 Request URL: http://192.168.196.133:31009/th/ft/test.jsp Host: 192.168.196.133:31009 X-Real-IP: 192.168.196.1 X-Forwarded-For: unknown, 1.1.1.1, 192.168.196.1, 192.168.196.133 Connection : close nginx依次将$remote_addr追加到X-Forwarded-For\nclientIP为倒数第n个IP, n为proxy的次数\n4.面向客户端的第一级nginx中，添加proxy_set_header X-Forwarded-For $remote_addr;\nupstream backendnginx { server 192.168.196.135:31001; } server { listen 31009; server_name 192.168.196.133; location /th { root html; index index.html index.htm; proxy_pass http://backendnginx; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } $curl -H \u0026#34;X-Forwarded-For: unknown, 1.1.1.1\u0026#34; 192.168.196.133:31009/th/ft/test.jsp Protocol: HTTP/1.0 Server Name: 192.168.196.133 Server Port: 31009 Remote Addr: 192.168.196.135 Remote Host: 192.168.196.135 Request URL: http://192.168.196.133:31009/th/ft/test.jsp Host: 192.168.196.133:31009 X-Real-IP: 192.168.196.1 X-Forwarded-For: 192.168.196.1, unknown, 1.1.1.1, 192.168.196.1, 192.168.196.133 Connection : close X-Forwarded-For的第一个IP为clientIP\n最佳实践 # 第一级反向代理nginx: proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n中间的反向代理nginx只需配置: proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n","date":"19 March 2019","permalink":"/2019/03/nginx-real-ip/","section":"博客","summary":"前言 # nginx反向代理后端应用服务器时，遇到web应用服务器需要获取客户端真实IP，该如何正确配置nginx?本文分别在一级反向代理及两级反向代理情况下，测试请求头X-Real-IP与X-Forwarded-For如何配置\n一级代理测试验证 # 1.测试架构: nginx反向代理 + tomcat web服务\nip server server port 192.168.196.135 nginx 31005 192.168.196.136 tomcat 31001 192.168.196.1 客户端 - 2.部署tomcat测试war包\n$cat test.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=ISO-8859-1\u0026#34; pageEncoding=\u0026#34;ISO-8859-1\u0026#34;%\u0026gt; \u0026lt;% out.","title":"nginx请求头X-Real-IP与X-Forwarded-For"},{"content":"前言 # nginx作为反向代理时，当满足location匹配条件后，向后端动态应用服务器发送请求，如果后端服务器会根据此次请求头做一些操作（比如通过Host或者server_name请求头的值进行校验等），就会出现问题。本文就用实例来分析一下nginx转发的请求头信息\n环境准备 # IP SERVER 10.168.11.12 Nginx1.12 10.168.11.13 Tomcat7.0 一、准备tomcat war包\n创建test.jsp文件来获取各请求头信息 \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=ISO-8859-1\u0026#34; pageEncoding=\u0026#34;ISO-8859-1\u0026#34;%\u0026gt; \u0026lt;% out.println(\u0026#34;Protocol: \u0026#34; + request.getProtocol()); out.println(\u0026#34;Server Name: \u0026#34; + request.getServerName()); out.println(\u0026#34;Server Port: \u0026#34; + request.getServerPort()); out.println(\u0026#34;Remote Addr: \u0026#34; + request.getRemoteAddr()); out.println(\u0026#34;Remote Host: \u0026#34; + request.getRemoteHost()); out.println(\u0026#34;Request URL: \u0026#34; + request.getRequestURL()); out.println(\u0026#34;Host: \u0026#34; + request.getHeader(\u0026#34;Host\u0026#34;)); out.println(\u0026#34;Connection : \u0026#34; + request.getHeader(\u0026#34;Connection\u0026#34;)); %\u0026gt; 打包th.war并部署 jar cvf th.war th/test.jsp 10.168.11.13服务器上安装Tomcat7.0，并监听30000端口。这里不再描述tomcat安装部署及发布包的步骤。\n访问http://10.168.11.13:30000/th/test.jsp tomcat从请求中获取的信息如下：\nProtocol: HTTP/1.1 \u0026lt;-- http协议版本 Server Name: 10.168.11.13 Server Port: 30000 Remote Addr: 10.168.196.134 \u0026lt;-- client IP Remote Host: 10.168.196.134 \u0026lt;-- client IP Request URL: http://10.168.11.13:30000/th/test.jsp Host: 10.168.11.13:30000 Connection : keep-alive 二、配置Nginx\nnginx.conf中定义server、upstream upstream backendsrv { server 10.168.11.13:30000; } server { listen 10000; server_name www.test.com; location /th { root html; index index.html index.htm; proxy_pass http://backendsrv; } } 启动nginx并访问http://10.168.11.12:10000/th/test.jsp Protocol: HTTP/1.0 Server Name: backendsrv Server Port: 80 Remote Addr: 10.168.11.12 Remote Host: 10.168.11.12 Request URL: http://backendsrv/th/test.jsp Host: backendsrv Connection : close Nginx代理发送http请求默认为HTTP 1.0，可通过定义proxy_http_version 1.1来修改，一般是在使用keepalive连接时才使用1.1版本 在使用proxy_set_header的默认值时，Nginx发往后端请求中的Host及server_name为代理的upstream中指定的域名 结果分析 # 1.以下是官方doc中的描述 Syntax:\tproxy_set_header field value; Default: ​ proxy_set_header Host $proxy_host; ​ proxy_set_header Connection close; Context: http, server, location\n允许重新定义或者添加发往后端服务器的请求头。value可以包含文本、变量或者它们的组合。 当且仅当当前配置级别中没有定义proxy_set_header指令时，会从上面的级别继承配置。 默认情况下，只有两个请求头会被重新定义： ​\tproxy_set_header Host $proxy_host; ​\tproxy_set_header Connection close;\n所以当客户端访问未携带Host请求头部，并使用proxy_set_header默认值时，请求头部Host值为backendsrv，Connection值为close，而端口则为默认80\n2.$host 和 $http_host的区别\n在server上下文中分别指定请求头Host值为$host和$http_host，结果对比如下:\nproxy_set_header Host $host proxy_set_header Host $http_host Protocol HTTP/1.0 HTTP/1.0 Server Name www.test.com www.test.com Server Port 80 10000 Remote Addr 10.168.11.12 10.168.11.12 Remote Host 10.168.11.12 10.168.11.12 Request URL http://www.test.com/th/test.jsp http://www.test.com:10000/th/test.jsp Host www.test.com www.test.com:10000 Connection close close 可以看出，由于客户端访问未携带Host请求头部，无法继承，则被修改为server上下文中定义的虚拟主机名www.test.com；并且$http_host会修改默认端口80为server上下文中监听的端口10000。所以当后端应用服务器需要根据请求头部Host信息做进一步处理时，要根据需求在nginx中正确配置Host值。\n最佳实践 # 当后端服务器有301、302跳转时，需要注意指定proxy_set_header Host请求头部，会直接影响nginx反馈给客户端的URL请求，要结合实际需求调整proxy_set_header Host和proxy_redirect将被代理服务器设置在response header中的Location做转换。\n","date":"4 March 2019","permalink":"/2019/03/nginx-proxy_set_header/","section":"博客","summary":"前言 # nginx作为反向代理时，当满足location匹配条件后，向后端动态应用服务器发送请求，如果后端服务器会根据此次请求头做一些操作（比如通过Host或者server_name请求头的值进行校验等），就会出现问题。本文就用实例来分析一下nginx转发的请求头信息\n环境准备 # IP SERVER 10.168.11.12 Nginx1.12 10.168.11.13 Tomcat7.0 一、准备tomcat war包\n创建test.jsp文件来获取各请求头信息 \u0026lt;%@ page language=\u0026#34;java\u0026#34; contentType=\u0026#34;text/html; charset=ISO-8859-1\u0026#34; pageEncoding=\u0026#34;ISO-8859-1\u0026#34;%\u0026gt; \u0026lt;% out.println(\u0026#34;Protocol: \u0026#34; + request.getProtocol()); out.println(\u0026#34;Server Name: \u0026#34; + request.getServerName()); out.","title":"Nginx请求头proxy_set_header"},{"content":"","date":"9 December 2018","permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"前言 # 随着公司容器管理平台部署的系统越来越多，所纳管集群主机也越来越多，面对大规模的文件和镜像分发，由于单一文件服务器、镜像仓库实例会受到部署主机的网络带宽限制，导致分发耗时长、成功率低的问题。利用P2P技术搭建文件和镜像的分发系统，文件和镜像可在集群主机之间共享传输，可有效解决上述问题。\nDragonfly简介 # 阿里开源，托管在github上 https://github.com/dragonflyoss/Dragonfly\n核心组件： ​ 1) cluster manger: 超级节点（supernode）集群 ​\t- 管理peer节点注册 ​\t- 负责CDN，从源文件服务器下载文件并生成数据块存储 ​\t- 调度peer节点在P2P网络中选择合适的下载源 ​ 2) dfget proxy： ​\tdfget是p2p客户端，部署在peer节点上，负责数据块的下载和共享。在镜像分发系统中也叫dfdaemon\nDragonfly镜像分发的工作模式: ​\t1) dfdaemon 拦截peer节点docker pull的HTTP请求，从镜像仓库服务器获取到镜像manifest文件后，使用dfget向supernode集群请求下载镜像的各layer文件 ​\t2) supernode从镜像仓库6下载镜像layer文件并生成数据块 ​\t3) peer节点通过dfget下载数据块，下载完成后supernode调度后续peer节点以P2P方式进行数据块下载 ​\t4) 所有layer文件通过dfget下载完成后，docker进程根据manifests文件整合为本地镜像，并按照镜像规范保存各镜像layer\nDragonfly镜像分发系统的搭建 # 一、节点规划\nIP roles services 192.168.196.134 supernode,registry,webserver docker,supernode,docker-distribution,nginx 192.168.196.2 supernode docker,supernode 192.168.196.3 peer docker 二、下载源码 ​\t最近一次release的版本0.2.0，在节点192.168.196.134上执行编译。\n$ mkdir /data/dragonfly $ cd /data/dragonfly $ wget https://github.com/dragonflyoss/Dragonfly/archive/v0.2.0.tar.gz -O Dragonfly-0.2.0.tar.gz $ tar xf Dragonfly-0.2.0.tar.gz 三、编译安装supernode\n​\t3.1 编译环境准备\n​\tsupernode.jar包编译依赖maven (yum install maven)，镜像定制依赖docker服务\n​\t3.2 修改默认注册端口8001为48001及默认下载服务端口8002为48002\n$cd /data/dragonfly/Dragonfly-0.2.0/src/supernode/src/main/java/com/alibaba/dragonfly/supernode/common $ sed -i \u0026#39;s/8001/48001/\u0026#39; Constants.java $ cd /data/dragonfly/Dragonfly-0.2.0/src/supernode/src/main/docker/source $ cat nginx.conf ...... server { listen 48001; location / { root /home/admin/supernode/repo; } } server { listen 48002; location /peer { proxy_pass http://127.0.0.1:8080; } ...... ​\t3.3 修改Dockerfile及配置文件\n​\t本例中主机docker版本为1.13不支持多阶段定制，需要修改Dockerfile文件\n​\t新建config.properties配置文件， 参考官方文档 https://d7y.io/user_guide/supernode_configuration/\n$ cd /data/dragonfly/Dragonfly-0.2.0/src/supernode/src/main/docker $ cat Dockerfile #私有centos7.2镜像(已配置阿里yum源) FROM 192.168.196.134:5000/library/centos7.2:01 COPY sources /tmp/sources RUN yum install -y epel-release \u0026amp;\u0026amp; \\ yum install -y -q telnet nginx java-1.8.0-openjdk.x86_64 \u0026amp;\u0026amp; \\ cp /tmp/sources/start.sh /root/start.sh \u0026amp;\u0026amp; \\ cp /tmp/sources/nginx.conf /etc/nginx/nginx.conf \u0026amp;\u0026amp; \\ cp /tmp/sources/config.properties /root/config.properties \u0026amp;\u0026amp;\\ mkdir -p /home/admin/supernode/bin \u0026amp;\u0026amp; \\ cp /tmp/sources/start.sh /home/admin/supernode/bin/ \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum /tmp/* \u0026amp;\u0026amp; \\ history -c ADD supernode.jar supernode.jar EXPOSE 48001 48002 ENTRYPOINT [\u0026#34;/root/start.sh\u0026#34;] $ cat source/config.properties supernode.cluster[0].ip = \u0026#39;192.168.196.134\u0026#39; supernode.cluster[0].registerPort = 48001 supernode.cluster[1].ip = \u0026#39;192.168.196.2\u0026#39; supernode.cluster[1].registerPort = 48001 $ cat source/start.sh #!/usr/bin/env bash nginx java -Dspring.config.location=/root/config.properties -Djava.security.egd=file:/dev/./urandom -jar supernode.jar ​\t3.4 编译supernode\n​\t参考官方文档 https://d7y.io/user_guide/install_server/\n$ cd /data/dragonfly/Dragonfly-0.2.0 $ ./build/build.sh supernode $ docker images supernode REPOSITORY TAG IMAGE ID CREATED SIZE supernode 0.2.0 aac72150e4a0 2 minutes ago 493 MB ​\t3.5 启动supernode服务\n$ docker run -d -p 48001:48001 -p 48002:48002 supernode:0.2.0 394df40d13eded44f78e943b49bbd233bf19258f777c0c42586ae969db39f142 $ docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bafcf15d15ae supernode:0.2.0 \u0026#34;/root/start.sh\u0026#34; 30 seconds ago Up 26 seconds 0.0.0.0:48001-48002-\u0026gt;48001-48002/tcp stupefied_meninsky $ telnet localhost 48001 Trying ::1... Connected to localhost. Escape character is \u0026#39;^]\u0026#39;. ^] telnet\u0026gt; quit Connection closed. $ telnet localhost 48002 Trying ::1... Connected to localhost. Escape character is \u0026#39;^]\u0026#39;. ^] telnet\u0026gt; quit Connection closed. # 将supernode镜像推到私有仓库 $ docker tag supernode:0.2.0 192.168.196.134:5000/library/supernode:0.2.0 $ docker push 192.168.196.134:5000 # 在192.168.196.2上pull镜像并启动supernode实例 $ docker pull 192.168.196.134:5000/library/supernode:0.2.0 $ docker run -d --net host 192.168.196.134:5000/library/supernode:0.2.0 1c9bef1f4c14f4427291ecff793bef6665a617c71e10cb13c481d56763430b2d $ ss -ntl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 128 *:48001 *:* LISTEN 0 128 *:48002 *:* LISTEN 0 100 :::8080 :::* LISTEN 0 128 :::22 :::* 四、编译安装df-client\n​\t4.1 编译环境准备\n​\t依赖go环境，需要安装golint\n$ yum install -y make go #默认GOPATH $ cd /root/go/src #GFW下的安装方法 $ git clone https://github.com/golang/tools.git tools $ git clone https://github.com/golang/lint.git $ mkdir -p golang.org/x $ cp github.com/golang/lint/ golang.org/x/ -a $ go install /root/go/src/golang.org/x/lint/golint $ cp /root/go/bin/golint /usr/local/sbin/ ​\t4.2 修改dfget请求supernode下载端口8002为48002\n$ cd/data/dragonfly/Dragonfly-0.2.0/src/getter $ sed -i \u0026#39;s/8002/48002/\u0026#39; env.py $ sed -i \u0026#39;s/8002/48002/\u0026#39; component/httputil.py #core/server.py中有关peer节点数据块存储时间的代码，默认180s #98 expire_time = 180 #135 alive_qu.get(True, 5 * 60.0) ​\t4.3 编译df-client\n​\t参考文档 https://d7y.io/user_guide/install_client/\n$ cd /data/dragonfly/Dragonfly-0.2.0/build/client $ ./configure --prefix=/data/dragonfly $ make \u0026amp;\u0026amp; make install # 将编译好的df-client，打包传到192.168.196.3节点上 $ cd /data/dragonfly $ tar cf df-client.tar df-client/ $ scp df-client.tar 192.168.196.3:/data/ ​\t4.4 在peer节点192.168.196.3安装df-client\n​\t1) 新建/etc/dragonfly.conf配置文件，指定要注册的supernode集群\n$ cat /etc/dragonfly.conf [node] address=192.168.196.2,192.168.196.134 ​\t2) 修改docker daemon.json文件\n# 配置registry-mirrors $ cat /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;0.0.0.0/0\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://127.0.0.1:65001\u0026#34;] } $ systemctl restart docker.service ​\t3) 启动dfdaemon\n$ export PATH=/data/df-client:$PATH $ nohup dfdaemon --registry 192.168.196.134:5000 \u0026amp; $ cat nohup.out launch dfdaemon on port:65001 $ ss -ntl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 128 :::22 :::* LISTEN 0 128 :::65001 :::* $ cd /root/.small-dragonfly/logs $ cat dfdaemon.log time=\u0026#34;2018-09-12 21:10:00\u0026#34; level=info msg=init... time=\u0026#34;2018-09-12 21:10:00\u0026#34; level=info msg=\u0026#34;dfget version:0.2.0\\n\u0026#34; time=\u0026#34;2018-09-12 21:10:00\u0026#34; level=info msg=\u0026#34;init finish\u0026#34; time=\u0026#34;2018-09-12 21:10:00\u0026#34; level=info msg=\u0026#34;start dfdaemon param:\u0026amp;{DfPath:/data/df-client/dfget DFRepo:/root/.small-dragonfly/dfdaemon/data/ RateLimit:20M CallSystem:com_ops_dragonfly URLFilter:Signature\u0026amp;Expires\u0026amp;OSSAccessKeyId Notbs:true MaxProcs:4 Version:false Verbose:false Help:false HostIP:127.0.0.1 Port:65001 Registry:192.168.196.134:5000 DownRule: CertFile: KeyFile:}\u0026#34; 五、镜像分发测试\n​\t私有镜像仓库镜像：\n​\t192.168.196.134:5000/library/supernode:0.2.0 ​\t192.168.196.134:5000/testnasp/nginx:0.1\n​\t在peer节点上 pull镜像：\n$ docker pull testnasp/nginx:0.1 Trying to pull repository docker.io/testnasp/nginx ... 0.1: Pulling from docker.io/testnasp/nginx ed070c36b93c: Pull complete 07e00549345e: Pull complete c68fedf2fd94: Pull complete Digest: sha256:2ff1929d16fb3df64564eeb91686bb07e585ed365ab3601e1fed164ff3dc4bc4 Status: Downloaded newer image for docker.io/testnasp/nginx:0.1 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/testnasp/nginx 0.1 cd5239a0906a 6 months ago 109 MB # pull镜像时可以查看dfget日志 $ cd /root/.small-dragonfly/logs $ tailf dfdaemon.log time=\u0026#34;2018-09-12 21:45:00\u0026#34; level=info msg=\u0026#34;start download url:http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:07e00549345efcbf93e50fc43440a05315c3ed049c28b7e7b709c1515b6550e0 to f5da0b91-b417-4f90-8a66-7ab99c5f581b in repo\u0026#34; time=\u0026#34;2018-09-12 21:45:00\u0026#34; level=info msg=\u0026#34;start download url:http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:ed070c36b93c24fc135fd9541fc687a8fb0664192a6c8e3397644e365944221a to 0afb237f-c0d1-4ee4-a8d0-ec43cfe69109 in repo\u0026#34; time=\u0026#34;2018-09-12 21:45:00\u0026#34; level=info msg=\u0026#34;dfget url:http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:07e00549345efcbf93e50fc43440a05315c3ed049c28b7e7b709c1515b6550e0 [SUCCESS] cost:20s\u0026#34; time=\u0026#34;2018-09-12 21:45:00\u0026#34; level=info msg=\u0026#34;dfget url:http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:ed070c36b93c24fc135fd9541fc687a8fb0664192a6c8e3397644e365944221a [SUCCESS] cost:22s\u0026#34; $ cat dfclient.log [2018-09-12 21:45:45,160] INFO sign:1719-1544622329.923 lineno:51 : cmd params:Namespace(callsystem=\u0026#39;com_ops_dragonfly\u0026#39;, console=False, dfdaemon=True, filter=\u0026#39;Signature\u0026amp;Expires\u0026amp;OSSAccessKeyId\u0026#39;, header=[\u0026#39;User-Agent:docker/1.13.1 go/go1.8.3 kernel/3.10.0-693.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/1.13.1 \\\\(linux\\\\))\u0026#39;, \u0026#39;Accept-Encoding:identity\u0026#39;, \u0026#39;X-Forwarded-For:127.0.0.1\u0026#39;], identifier=None, locallimit=\u0026#39;20M\u0026#39;, md5=None, node=None, notbs=True, output=\u0026#39;/root/.small-dragonfly/dfdaemon/data/f5da0b91-b417-4f90-8a66-7ab99c5f581b\u0026#39;, pattern=None, showbar=False, timeout=None, totallimit=\u0026#39;20M\u0026#39;, url=\u0026#39;http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:07e00549345efcbf93e50fc43440a05315c3ed049c28b7e7b709c1515b6550e0\u0026#39;, version=False),python version:sys.version_info(major=2, minor=7, micro=5, releaselevel=\u0026#39;final\u0026#39;, serial=0) ...中间省略... [2018-09-12 21:45:50,965] INFO sign:1720-1544622330.044 lineno:109 : move src:/root/.small-dragonfly/data/0afb237f-c0d1-4ee4-a8d0-ec43cfe69109-1720-1544622330.044 to dst:/root/.small-dragonfly/dfdaemon/data/0afb237f-c0d1-4ee4-a8d0-ec43cfe69109 result:True cost:0.057 [2018-09-12 21:45:50,965] INFO sign:1720-1544622330.044 lineno:195 : download successfully from dragonfly [2018-09-12 21:45:51,003] INFO sign:1720-1544622330.044 lineno:60 : local http result:success for path:/client/ and cost:0.002 [2018-09-12 21:45:51,003] INFO sign:1720-1544622330.044 lineno:94 : |7773d958cc784318c36095a405f4aa918d0c7ca45f7cad12c3e08c68fbca68fb|http://192.168.196.134:5000/v2/testnasp/nginx/blobs/sha256:ed070c36b93c24fc135fd9541fc687a8fb0664192a6c8e3397644e365944221a|22496029|22496059|192.168.196.134|com_ops_dragonfly|20.957| [2018-09-12 21:45:51,003] INFO sign:1720-1544622330.044 lineno:111 : download SUCCESS cost:20.959s length:22496029 reason:0 Tips：\n要分发的镜像命名空间是library时，要注意peer节点pull下的镜像名字\n$ docker pull library/supernode:0.2.0 Trying to pull repository docker.io/library/supernode ... 0.2.0: Pulling from docker.io/library/supernode 89ac44988659: Already exists 58f1be4f44ca: Already exists 8de7fe9b2274: Pull complete adfad6e78041: Pull complete 802d3ffa4c82: Pull complete Digest: sha256:4b49d1655f518be7fa5b450df887a54519a57a462ca289ea21ab3d9e54fc846c Status: Downloaded newer image for docker.io/supernode:0.2.0 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/supernode 0.2.0 aac72150e4a0 5 days ago 493 MB dfdaemon也可以容器化部署，使用host模式运行即可。还有要注意,目前一个dfdaemon只能指定一个仓库地址。\n","date":"9 December 2018","permalink":"/2018/12/docker-dragonfly/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e前言 \n    \u003cdiv id=\"前言\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e5%89%8d%e8%a8%80\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003e随着公司容器管理平台部署的系统越来越多，所纳管集群主机也越来越多，面对大规模的文件和镜像分发，由于单一文件服务器、镜像仓库实例会受到部署主机的网络带宽限制，导致分发耗时长、成功率低的问题。利用P2P技术搭建文件和镜像的分发系统，文件和镜像可在集群主机之间共享传输，可有效解决上述问题。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"P2P文件与镜像分发开源解决方案Dragonfly安装部署"},{"content":" k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。\n集群节点规划及架构 # 节点 IP service master 192.168.196.134 etcd,flannel,docker,kubernetes-master,docker-distribution,nginx node1 192.168.196.135 flannel,docker,kubernetes-node node2 192.168.196.136 flannel,docker,kubernetes-node 对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群\n服务安装 # etcd # 所有关于集群状态的配置信息都以key/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods 本例只是etcd单机部署，在master节点上yum安装etcd并修改etcd.conf，启动etcd\n$ grep ^[^#] /etc/etcd/etcd.conf ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://192.168.196.134:2379\u0026#34; ETCD_NAME=etcd1 ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;http://192.168.196.134:2380\u0026#34; ETCD_ADVERTISE_CLIENT_URLS=\u0026#34;http://192.168.196.134:2379\u0026#34; ETCD_INITIAL_CLUSTER=\u0026#34;etcd1=http://192.168.196.134:2380\u0026#34; $ ss -ntl|egrep \u0026#34;2379|2380\u0026#34; LISTEN 0 128 192.168.196.134:2379 *:* LISTEN 0 128 127.0.0.1:2380 *:* #etcdctl简单命令 $ etcdctl --endpoints http://192.168.196.134:2379 mkdir /testdir $ etcdctl --endpoints http://192.168.196.134:2379 ls /testdir $ etcdctl --endpoints http://192.168.196.134:2379 ls / /testdir $ etcdctl --endpoints http://192.168.196.134:2379 mk /testdir/testkey testvalue testvalue $ etcdctl --endpoints http://192.168.196.134:2379 get /testdir/testkey testvalue $ etcdctl --endpoints http://192.168.196.134:2379 rm /testdir/testkey PrevNode.Value: testvalue $ etcdctl --endpoints http://192.168.196.134:2379 rmdir /testdir flannel # overlay网络解决方案，实现pod到pod之间的网络通信，并为每个pod提供唯一的IP地址\n各节点yum安装flannel，修改/etc/sysconfig/flanneld $ grep ^[^#] /etc/sysconfig/flanneld FLANNEL_ETCD_ENDPOINTS=\u0026#34;http://192.168.196.134:2379\u0026#34; FLANNEL_ETCD_PREFIX=\u0026#34;/atomic.io/network\u0026#34; \u0026lt;--定义在etcd中存储的目录 FLANNEL_OPTIONS=\u0026#34;-iface=ens34 -ip-masq=true\u0026#34; 在etcd中手动创建自定义flannel网段的地址池10.55.0/16 $ etcdctl --endpoints http://192.168.196.134:2379 mkdir /atomic.io $ etcdctl --endpoints http://192.168.196.134:2379 mkdir /atomic.io/network $ etcdctl --endpoints http://192.168.196.134:2379 mk /atomic.io/network/config \u0026#39;{\u0026#34;Network\u0026#34;: \u0026#34;10.55.0.0/16\u0026#34;}\u0026#39; {\u0026#34;Network\u0026#34;: \u0026#34;10.55.0.0/16\u0026#34;} 各节点启动flanneld服务，可查看flannel0桥随机分配的地址 # master节点 $ ifconfig flannel0 | grep -A1 flannel0 flannel0: flags=4305\u0026lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u0026gt; mtu 1472 inet 10.55.68.0 netmask 255.255.0.0 destination 10.55.68.0 # flannel配置信息存在subnet.env文件 $ cat /var/run/flannel/subnet.env FLANNEL_NETWORK=10.55.0.0/16 FLANNEL_SUBNET=10.55.68.1/24 FLANNEL_MTU=1472 FLANNEL_IPMASQ=true # node1节点 $ ifconfig flannel0 | grep -A1 flannel0 flannel0: flags=4305\u0026lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u0026gt; mtu 1472 inet 10.55.91.0 netmask 255.255.0.0 destination 10.55.91.0 # node2节点 $ ifconfig flannel0 | grep -A1 flannel flannel0: flags=4305\u0026lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u0026gt; mtu 1472 inet 10.55.2.0 netmask 255.255.0.0 destination 10.55.2.0 registry # ​\t为集群搭建私有镜像仓库，可以从dockerHub上拉取镜像启动，也可以yum安装docker-distribution和nginx进行搭建，本文使用后一种方法\n在master节点上，yum安装docker-distribution并修改config.yml,启动服务 $ cat /etc/docker-distribution/registry/config.yml version: 0.1 log: fields: service: registry storage: cache: layerinfo: inmemory filesystem: rootdirectory: /var/lib/registry \u0026lt;--存储路径 http: addr: 127.0.0.1:5000 \u0026lt;--监听地址和端口 在master节点，yum安装nginx，新建一个代理到后端registry的nginx配置文件registry.conf,启动服务 $ cat /etc/nginx/conf.d/registry.conf server { listen 8088; server_name registry; client_max_body_size 0; location / { proxy_pass http://127.0.0.1:5000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_buffering off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } # 8080和5000端口已监听 $ netstat -ntl Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 192.168.196.134:2379 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:2380 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8088 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:5000 0.0.0.0:* LISTEN tcp6 0 0 :::80 :::* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp6 0 0 ::1:25 :::* LISTEN 修改各节点/etc/hosts文件，配置registry解析，以master节点为例 $ cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.196.134 registry 192.168.196.135 node1 192.168.196.136 node2 # 验证镜像仓库服务是否正常 $ curl -Lv registry:8088/v2 * About to connect() to registry port 8088 (#0) * Trying 192.168.196.134... * Connected to registry (192.168.196.134) port 8088 (#0) \u0026gt; GET /v2 HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: registry:8088 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 301 Moved Permanently \u0026lt; Server: nginx/1.12.2 \u0026lt; Date: Tue, 03 Jul 2018 14:45:49 GMT \u0026lt; Content-Type: text/html; charset=utf-8 \u0026lt; Content-Length: 39 \u0026lt; Connection: keep-alive \u0026lt; Docker-Distribution-Api-Version: registry/2.0 \u0026lt; Location: /v2/ \u0026lt; * Ignoring the response-body * Connection #0 to host registry left intact * Issue another request to this URL: \u0026#39;HTTP://registry:8088/v2/\u0026#39; * Found bundle for host registry: 0x78fee0 * Re-using existing connection! (#0) with host registry * Connected to registry (192.168.196.134) port 8088 (#0) \u0026gt; GET /v2/ HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: registry:8088 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: nginx/1.12.2 \u0026lt; Date: Tue, 03 Jul 2018 14:45:49 GMT \u0026lt; Content-Type: application/json; charset=utf-8 \u0026lt; Content-Length: 2 \u0026lt; Connection: keep-alive \u0026lt; Docker-Distribution-Api-Version: registry/2.0 \u0026lt; * Connection #0 to host registry left intact {} docker # 1.master节点yum安装 docker-engine-1.13.1-1\n修改/etc/docker/daemon.json $ cat /etc/docker/daemon.json { \u0026#34;storage-driver\u0026#34;: \u0026#34;devicemapper\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;dm.thinpooldev=/dev/mapper/docker--vg-thinpool\u0026#34;, \u0026#34;dm.use_deferred_removal=true\u0026#34;, \u0026#34;dm.use_deferred_deletion=true\u0026#34; ], \u0026#34;graph\u0026#34;: \u0026#34;/docker\u0026#34;, \u0026#34;insecure-registries\u0026#34;: [\u0026#34;registry:8088\u0026#34;] \u0026lt;--配置镜像仓库地址 } 修改/usr/lib/systemd/system/docker.service 加入$DOCKER_NETWORK_OPTIONS启动参数，否则docker无法根据flannel服务随机分配的网段配置docker0网桥 [service]下添加ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT；否则，docker启动后FORWARD链的默认策略为DROP $vi /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT #flannel服务启动后会自动生成/var/run/flannel/docker文件，可用于指定docker的相关网络参数 $ cat /var/run/flannel/docker DOCKER_OPT_BIP=\u0026#34;--bip=10.55.68.1/24\u0026#34; DOCKER_OPT_IPMASQ=\u0026#34;--ip-masq=false\u0026#34; DOCKER_OPT_MTU=\u0026#34;--mtu=1472\u0026#34; DOCKER_NETWORK_OPTIONS=\u0026#34; --bip=10.55.68.1/24 --ip-masq=false --mtu=1472\u0026#34; 启动docker $ systemctl daemon-reload $ systemctl start docker $ ifconfig|grep -A1 \u0026#34;docker0\\|flannel0\u0026#34; docker0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 inet 10.55.68.1 netmask 255.255.255.0 broadcast 0.0.0.0 -- flannel0: flags=4305\u0026lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u0026gt; mtu 1472 inet 10.55.68.0 netmask 255.255.0.0 destination 10.55.68.0 $ docker info | grep -A2 Insecure Insecure Registries: registry:8088 127.0.0.0/8 2.node1节点和node2节点yum安装docker-1.13.1\n添加镜像仓库地址，修改配置文件/etc/sysconfig/docker $ grep ^[^#] /etc/sysconfig/docker OPTIONS=\u0026#39;--selinux-enabled --log-driver=journald --signature-verification=false -g /docker\u0026#39; if [ -z \u0026#34;${DOCKER_CERT_PATH}\u0026#34; ]; then DOCKER_CERT_PATH=/etc/docker fi ADD_REGISTRY=\u0026#34;--add-registry registry\u0026#34; INSECURE_REGISTRY=\u0026#34;--insecure-registry registry:8088\u0026#34; 修改/usr/lib/systemd/system/docker.service\n[service]下添加ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT；否则，docker启动后FORWARD链的默认策略为DROP\n启动docker服务\n$ systemctl daemon-reload $ systemctl start docker $ ifconfig |grep -A1 \u0026#34;docker0\\|flannel\u0026#34; docker0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 inet 10.55.2.1 netmask 255.255.255.0 broadcast 0.0.0.0 -- flannel0: flags=4305\u0026lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST\u0026gt; mtu 1472 inet 10.55.2.0 netmask 255.255.0.0 destination 10.55.2.0 $ docker info|grep -A2 Insecure Insecure Registries: registry:8088 127.0.0.0/8 kubernetes-master # master节点yum安装kubernetes-master，修改/etc/kubernetes/下的配置文件 $ls /etc/kubernetes/ apiserver config controller-manager scheduler #config中的配置为apiserver、controller-manager及scheduler的通用配置项 $ grep ^[^#] /etc/kubernetes/config KUBE_LOGTOSTDERR=\u0026#34;--logtostderr=true\u0026#34; KUBE_LOG_LEVEL=\u0026#34;--v=0\u0026#34; KUBE_ALLOW_PRIV=\u0026#34;--allow-privileged=false\u0026#34; KUBE_MASTER=\u0026#34;--master=http://192.168.196.134:8080\u0026#34; \u0026lt;--指定apiserver地址及端口 $ grep ^[^#] /etc/kubernetes/apiserver KUBE_API_ADDRESS=\u0026#34;--insecure-bind-address=0.0.0.0\u0026#34; KUBE_ETCD_SERVERS=\u0026#34;--etcd-servers=http://192.168.196.134:2379\u0026#34; \u0026lt;--etcd服务地址 KUBE_SERVICE_ADDRESSES=\u0026#34;--service-cluster-ip-range=10.254.0.0/16\u0026#34; \u0026lt;--k8s集群的service网段 KUBE_ADMISSION_CONTROL=\u0026#34;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota\u0026#34; KUBE_API_ARGS=\u0026#34;\u0026#34; master节点上启动kube-apiserver、kube-scheduler、kube-controller-manager服务\n2.1 Kubernetes API Server: 通过kube-apiserver进程提供服务；用户通过Rest操作或kubectl cli与API Server交互；它用于所有与API对象（Pod、RC、Service等）相关的操作(如增、删、改、查及watch等)，是集群内各功能模块之间数据交互和通信的中心枢纽。 2.2 Controller Manager: 集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（serviceAccount）、资源定额（ResourceQuota）等的管理，提供自愈功能，确保集群始终处于预期的工作状态 2.3 Scheduler: 负责Pod的调度，将待调度的Pod(API新创建的Pod、Controller Manager为补足副本而创建的Pod等)按照特定的调度算法和调度策略绑定到集群中某个node上，并将绑定信息写入etcd\n$ systemctl start kube-apiserver.service kube-scheduler.service kube-controller-manager.service # 使用命令行工具测试 $ kubectl -s 192.168.196.134:8080 --version Kubernetes v1.5.2 kubernetes-node # node1/node2节点yum安装kubernetes-node，修改/etc/kubernetes/下的配置文件 #config中的配置为kubelet及kube-proxy的通用配置项 $ grep ^[^#] /etc/kubernetes/config KUBE_LOGTOSTDERR=\u0026#34;--logtostderr=true\u0026#34; KUBE_LOG_LEVEL=\u0026#34;--v=0\u0026#34; KUBE_ALLOW_PRIV=\u0026#34;--allow-privileged=false\u0026#34; KUBE_MASTER=\u0026#34;--master=lhttp://192.168.196.134:8080\u0026#34; $ grep ^[^#] /etc/kubernetes/kubelet KUBELET_ADDRESS=\u0026#34;--address=0.0.0.0\u0026#34; KUBELET_HOSTNAME=\u0026#34;--hostname-override=node1\u0026#34; \u0026lt;--注意node1/node2的区别 KUBELET_API_SERVER=\u0026#34;--api-servers=http://192.168.196.134:8080\u0026#34; KUBELET_POD_INFRA_CONTAINER=\u0026#34;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest\u0026#34; \u0026lt;--随pod一起启动的容器的镜像 KUBELET_ARGS=\u0026#34;\u0026#34; 在node节点上启动kubelet、kube-proxy\n2.1 kubelet: 处理Master节点下发到本节点的任务，管理Pod及Pod中的容器（通过API Server监听Scheduler产生的Pod绑定event）；每个kubelet进程会在API Server上注册节点自身信息，定期向Master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。 2.2 kube-proxy: kube-proxy服务进程，可以看做是service的透明代理兼负载均衡，其核心功能是将到某个service的访问请求转发到后端的多个Pod实例上，事实上是通过iptables的NAT转换来实现的。Service是对一组Pod的抽象，创建时可被分配一个虚拟的Cluster IP。\n$ systemctl start kubelet.service kube-proxy.service # 使用命令行工具测试 $ kubectl -s 192.168.196.134:8080 get node NAME STATUS AGE node1 Ready 2m node2 Ready 2m 部署示例 # 1.查看当前版本的kubernetes服务支持的API类型及版本\n$ curl 192.168.196.134:8080 { \u0026#34;paths\u0026#34;: [ \u0026#34;/api\u0026#34;, \u0026#34;/api/v1\u0026#34;, \u0026#34;/apis\u0026#34;, \u0026#34;/apis/apps\u0026#34;, \u0026#34;/apis/apps/v1beta1\u0026#34;, \u0026#34;/apis/authentication.k8s.io\u0026#34;, \u0026#34;/apis/authentication.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/authorization.k8s.io\u0026#34;, \u0026#34;/apis/authorization.k8s.io/v1beta1\u0026#34;, \u0026#34;/apis/autoscaling\u0026#34;, \u0026#34;/apis/autoscaling/v1\u0026#34;, \u0026#34;/apis/batch\u0026#34;, \u0026#34;/apis/batch/v1\u0026#34;, \u0026#34;/apis/batch/v2alpha1\u0026#34;, \u0026#34;/apis/certificates.k8s.io\u0026#34;, \u0026#34;/apis/certificates.k8s.io/v1alpha1\u0026#34;, \u0026#34;/apis/extensions\u0026#34;, \u0026#34;/apis/extensions/v1beta1\u0026#34;, \u0026#34;/apis/policy\u0026#34;, \u0026#34;/apis/policy/v1beta1\u0026#34;, \u0026#34;/apis/rbac.authorization.k8s.io\u0026#34;, \u0026#34;/apis/rbac.authorization.k8s.io/v1alpha1\u0026#34;, \u0026#34;/apis/storage.k8s.io\u0026#34;, \u0026#34;/apis/storage.k8s.io/v1beta1\u0026#34;, \u0026#34;/healthz\u0026#34;, \u0026#34;/healthz/ping\u0026#34;, \u0026#34;/healthz/poststarthook/bootstrap-controller\u0026#34;, \u0026#34;/healthz/poststarthook/extensions/third-party-resources\u0026#34;, \u0026#34;/healthz/poststarthook/rbac/bootstrap-roles\u0026#34;, \u0026#34;/logs\u0026#34;, \u0026#34;/metrics\u0026#34;, \u0026#34;/swaggerapi/\u0026#34;, \u0026#34;/ui/\u0026#34;, \u0026#34;/version\u0026#34; ] } 2.部署一个nginx服务的deployment示例\n$ cat deployment-nginx.yml apiVersion: extensions/v1beta1 \u0026lt;--指定APIversion kind: Deployment \u0026lt;--指定资源类型为deployment metadata: name: deployment-nginx \u0026lt;--deployment的名称 spec: replicas: 2 \u0026lt;--期望pod的副本数 revisionHistoryLimit: 10 template: metadata: labels: \u0026lt;--pod标签 app: nginx spec: containers: - name: nginx image: registry:8088/nginx:latest ports: - containerPort: 80 $ kubectl -s 192.168.196.134:8080 create -f deployment-nginx.yml 查看deployment信息\nget查询某个resource的详细信息，describe查询某个resource相关的状态信息\n$ kubectl -s 192.168.196.134:8080 get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment-nginx 2 2 2 2 7d $ kubectl -s 192.168.196.134:8080 describe deployment Name: deployment-nginx Namespace: default CreationTimestamp: Fri, 27 Jul 2018 13:37:02 +0800 Labels: app=nginx Selector: app=nginx Replicas: 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: deployment-nginx-2481570099 (2/2 replicas created) No events. $ kubectl -s 192.168.196.134:8080 get deployment -o yaml \u0026lt;--支持导出为json和yaml格式 apiVersion: v1 items: - apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;1\u0026#34; creationTimestamp: 2018-07-27T05:37:02Z generation: 1 labels: app: nginx name: deployment-nginx namespace: default resourceVersion: \u0026#34;88336\u0026#34; selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/deployment-nginx uid: 16ed87a7-915f-11e8-a083-000c29815d48 spec: replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: registry:8088/nginx:latest imagePullPolicy: Always name: nginx ports: - containerPort: 80 protocol: TCP resources: {} terminationMessagePath: /dev/termination-log dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 status: availableReplicas: 2 conditions: - lastTransitionTime: 2018-07-30T05:36:05Z lastUpdateTime: 2018-07-30T05:36:05Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026#34;True\u0026#34; type: Available observedGeneration: 1 replicas: 2 updatedReplicas: 2 kind: List metadata: {} resourceVersion: \u0026#34;\u0026#34; selfLink: \u0026#34;\u0026#34; 查看pod的信息 # pod所部署的节点及分配的IP $ kubectl -s 192.168.196.134:8080 get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE deployment-nginx-2481570099-cfkhn 1/1 Running 4 7d 10.55.2.2 node2 deployment-nginx-2481570099-q6xv8 1/1 Running 3 7d 10.55.91.2 node1 #除了通过get/describe查询pod的全部信息，还可以通过template抓取指定key的值 $ kubectl -s 192.168.196.134:8080 get pod -o template \\ deployment-nginx-2481570099-cfkhn --template={{.status.podIP}} 10.55.2.2 验证flannel网络下pod的连通性 #登录到node1上 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 17485262b0a6 registry:8088/nginx:latest \u0026#34;nginx -g \u0026#39;daemon ...\u0026#34; 5 hours ago Up 5 hours k8s_nginx.4f504e9_deployment-nginx-2481570099-q6xv8_default_16f355db-915f-11e8-a083-000c29815d48_e2f3c24d dc1833e50415 registry:8088/pod-infrastructure:v3.4 \u0026#34;/pod\u0026#34; 5 hours ago Up 5 hours k8s_POD.ee70020d_deployment-nginx-2481570099-q6xv8_default_16f355db-915f-11e8-a083-000c29815d48_7c54ed33 #访问本节点上的pod服务 $ curl 10.55.91.2:80 Welcome to nginx! # 访问node2节点上的pod服务 $ curl 10.55.2.2:80 Welcome to nginx! #登录到node2上 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c087c12e8afd registry:8088/nginx:latest \u0026#34;nginx -g \u0026#39;daemon off\u0026#34; 5 hours ago Up 5 hours k8s_nginx.4f504e9_deployment-nginx-2481570099-cfkhn_default_16f36465-915f-11e8-a083-000c29815d48_80f894f4 195f5d3de8c1 registry:8088/pod-infrastructure:v3.4 \u0026#34;/pod\u0026#34; 5 hours ago Up 5 hours k8s_POD.ee70020d_deployment-nginx-2481570099-cfkhn_default_16f36465-915f-11e8-a083-000c29815d48_e4b4e2e1 #访问本节点上的pod服务 $ curl 10.55.2.2:80 Welcome to nginx! # 访问node2节点上的pod服务 $ curl 10.55.91.2:80 Welcome to nginx! #同样的master节点也配置flanne网络，可访问pod的应用 3.部署service示例\n#为部署的pod创建service $ cat service-nginx.yml kind: Service apiVersion: v1 metadata: name: service-nginx spec: ports: - port: 80 \u0026lt;-- service的访问端口 targetPort: 80 \u0026lt;-- nginx容器服务端口 selector: app: nginx \u0026lt;-- 以pod标签识别要代理的后端应用 type: NodePort \u0026lt;-- 负载均衡为轮询 #NodePort模式为 绑定到pod所在Node节点的网卡，对外提供服务，端口为NodePort(不指定时随机分配) $ kubectl -s 192.168.196.134:8080 create -f service-nginx.yml #查看service-nginx的详细信息 $ kubectl -s 192.168.196.134:8080 get service -o wide service-nginx NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-nginx 10.254.104.158 \u0026lt;nodes\u0026gt; 80:30284/TCP 7d app=nginx $ kubectl -s 192.168.196.134:8080 get service -o yaml service-nginx apiVersion: v1 kind: Service metadata: creationTimestamp: 2018-07-27T07:01:04Z name: service-nginx namespace: default resourceVersion: \u0026#34;17086\u0026#34; selfLink: /api/v1/namespaces/default/services/service-nginx uid: d437faec-916a-11e8-a083-000c29815d48 spec: clusterIP: 10.254.104.158 \u0026lt;-- 为service分配的随机clusterIP ports: - nodePort: 30284 \u0026lt;--绑定到node节点物理网卡的随机端口 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None \u0026lt;-- 1） type: NodePort status: loadBalancer: {} 1)默认负载策略为轮询，可指定sessionAffinity的值为clientIP实现session绑定 $ kubectl -s 192.168.196.134:8080 describe service service-nginx Name: service-nginx Namespace: default Labels: \u0026lt;none\u0026gt; Selector: app=nginx Type: NodePort IP: 10.254.104.158 Port: \u0026lt;unset\u0026gt; 80/TCP NodePort: \u0026lt;unset\u0026gt; 30284/TCP Endpoints: 10.55.2.2:80,10.55.91.2:80 Session Affinity: None No events. #在任意node节点上访问service-nginx绑定的ClusterIP:Port $ curl 10.254.104.158:80 Welcome to nginx! #通过监测两个node的nginx访问日志可以看出是转发策略为轮询 #在任意节点上访问service-naginx绑定在主机网卡上的NodePort [root@master ~]# curl 192.168.196.136:30284 Welcome to nginx! [root@master ~]# curl 192.168.196.135:30284 Welcome to nginx! [root@node1 ~]# curl 192.168.196.135:30284 Welcome to nginx! [root@node1 ~]# curl 192.168.196.136:30284 Welcome to nginx! [root@node2 ~]# curl 192.168.196.136:30284 Welcome to nginx! [root@node2 ~]# curl 192.168.196.135:30284 Welcome to nginx! 4.部署kubernetes dashboard ​\tgithub上项目地址https://github.com/kubernetes/dashboard\n由于安装的k8s服务版本为v1.5.2，这里选择dashboard release版本为v1.5.0的镜像，dockerhub上搜索并pull到本地( https://hub.docker.com/r/ist0ne/kubernetes-dashboard-amd64/ )，打标签推到私有仓库 获取github上dashboard的部署文件 $ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.5.0/src/deploy/kubernetes-dashboard.yaml 修改kubernetes-dashboard.yaml文件中\nimage: registry:8088/kubernetes-dashboard-amd64:v1.5.0 \u0026ndash;apiserver-host=http://192.168.196.134:8080 部署dashboard\n$ kubectl -s 192.168.196.134:8080 create -f kubernetes-dashboard.yaml deployment \u0026#34;kubernetes-dashboard\u0026#34; created service \u0026#34;kubernetes-dashboard\u0026#34; created 这里要注意，kubernetes-dashboard部署的namespace为kube-system而不是default，需在命令行指定 -n kube-system或者\u0026ndash;namespace=kube-system才能获取到对应的资源信息\n浏览器访问192.168.196.134:8080/ui ","date":"9 August 2018","permalink":"/2018/08/k8s-deploy/","section":"博客","summary":"k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。\n集群节点规划及架构 # 节点 IP service master 192.168.196.134 etcd,flannel,docker,kubernetes-master,docker-distribution,nginx node1 192.168.196.135 flannel,docker,kubernetes-node node2 192.168.196.136 flannel,docker,kubernetes-node 对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群\n服务安装 # etcd # 所有关于集群状态的配置信息都以key/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods 本例只是etcd单机部署，在master节点上yum安装etcd并修改etcd.conf，启动etcd\n$ grep ^[^#] /etc/etcd/etcd.conf ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://192.168.196.134:2379\u0026#34; ETCD_NAME=etcd1 ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;http://192.","title":"Kubernetes集群部署"},{"content":"","date":"8 August 2018","permalink":"/tags/ca/","section":"Tags","summary":"","title":"CA"},{"content":"背景 # 通常，我们在配置docker daemon的监听方式为本地的socket和开放TCP端口，docker服务允许远程访问，方便对docker集群的主机执行管理命令及调用等。如果不使用TLS加密docker，任意客户端都可以对docker主机进行远程操作，有很大的安全隐患。而配置TLS后，只有信任的客户端才可以远程访问docker服务。\nTLS认证配置 # 自签名的证书\u0026mdash;使用openssl创建私有CA并签发公钥 #生成根证书RSA私钥,123456位私钥文件的密码 $ openssl genrsa -aes256 -passout pass:123456 -out cakey.pem 2048 Generating RSA private key, 2048 bit long modulus ........+++ ......................+++ e is 65537 (0x10001) #生成自签名证书 $ openssl req -new -x509 -days 365 -key cakey.pem -passin pass:123456 -sha256 -out ca.pem -subj \u0026#34;/C=CN/ST=./L=./O=./CN=192.168.196.136\u0026#34; #生成server私钥 $ openssl genrsa -out server.key 2048 Generating RSA private key, 2048 bit long modulus .....................................................................+++ ...............+++ e is 65537 (0x10001) #指定允许通过IP地址访问的IP列表，使用逗号隔开 $ echo subjectAltName=IP:192.168.196.136 \u0026gt; extfile.cnf $ echo extendedKeyUsage=serverAuth \u0026gt;\u0026gt; extfile.cnf #生成server证书申请文件 $ openssl req -new -sha256 -key server.key -out server.csr -subj \u0026#34;/CN=server\u0026#34; #CA签署证书，并颁发server证书 $ openssl x509 -passin pass:123456 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey cakey.pem -CAcreateserial -out server.crt -extfile extfile.cnf Signature ok subject=/CN=192.168.196.136 Getting CA Private Key #生成client私钥 $ openssl genrsa -out client.key 2048 Generating RSA private key, 2048 bit long modulus .................................................+++ ...................................................+++ e is 65537 (0x10001) #生成client证书申请文件 $ openssl req -new -sha256 -key client.key -out client.csr -subj \u0026#34;/CN=client\u0026#34; #CA签署证书，并颁发client证书 $ openssl x509 -passin pass:123456 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey cakey.pem -CAcreateserial -out client.crt -extfile extfile.cnf Signature ok subject=/CN=client Getting CA Private Key 配置docker服务开启TLS认证 #修改daemon.json文件，配置docker服务开启TLS认证 $ cat daemon.json { \u0026#34;hosts\u0026#34;: [\u0026#34;unix:///var/run/docker.sock\u0026#34;, \u0026#34;tcp://0.0.0.0:3666\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;registry:8088\u0026#34;], \u0026#34;tlsverify\u0026#34;:true, \u0026#34;tlscacert\u0026#34;:\u0026#34;/etc/docker/ca.pem\u0026#34;, \u0026#34;tlscert\u0026#34;:\u0026#34;/etc/docker/server.crt\u0026#34;, \u0026#34;tlskey\u0026#34;:\u0026#34;/etc/docker/server.key\u0026#34; } $ systemctl restart docker #在本机执行验证命令 $ docker --tlsverify --tlscacert ca.pem --tlscert client.crt --tlskey=client.key -H tcp://192.168.196.136:3666 version Client: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-61.git85d7426.el7.centos.x86_64 Go version: go1.8.3 Git commit: 85d7426/1.12.6 Built: Tue Oct 24 15:40:21 2017 OS/Arch: linux/amd64 Server: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-61.git85d7426.el7.centos.x86_64 Go version: go1.8.3 Git commit: 85d7426/1.12.6 Built: Tue Oct 24 15:40:21 2017 OS/Arch: linux/amd64 分发key文件到客户端，验证\n只有有秘钥文件的客户端才能连接到此docker服务器上\n$ scp ca.pem client.crt client.key 192.168.196.135:/root/ root@192.168.196.135\u0026#39;s password: ca.pem 100% 1245 1.2KB/s 00:00 client.crt 100% 1086 1.1KB/s 00:00 client.key 100% 1675 1.6KB/s 00:00 #登录到135主机 $ pwd /root $ docker --tlsverify --tlscacert ca.pem --tlscert client.crt --tlskey=client.key -H tcp://192.168.196.136:3666 version Client: Version: 1.13.1 API version: 1.24 (downgraded from 1.26) Package version: docker-1.12.6-61.git85d7426.el7.centos.x86_64 Go version: go1.9.4 Git commit: dded712/1.13.1 Built: Tue Jul 17 18:34:48 2018 OS/Arch: linux/amd64 Server: Version: 1.12.6 API version: 1.24 (minimum version ) Package version: docker-1.12.6-61.git85d7426.el7.centos.x86_64 Go version: go1.8.3 Git commit: 85d7426/1.12.6 Built: Tue Oct 24 15:40:21 2017 OS/Arch: linux/amd64 Experimental: false TLS认证模式 # 服务端认证模式 选项 说明 tlsverify, tlscacert, tlscert, tlskey 向客户端发送服务端证书, 校验客户端证书是否由指定的CA(自签名根证书)签发 tls, tlscert, tlskey 向客户端发送服务端证书, 不校验客户端证书是否由指定的CA(自签名根证书)签发 客户端认证模式 选项 说明 tls 校验服务端证书是否由 公共的CA机构签发 tlsverify, tlscacert 校验服务端证书是否由指定的CA(自签名根证书)签发 tls, tlscert, tlskey 使用客户端证书进行身份验证, 但不校验服务端证书是否由指定的CA(自签名根证书)签发 tlsverify, tlscacert, tlscert, tlskey 使用客户端证书进行身份验证且校验服务端证书是否由指定的CA(自签名根证书)签发 ​\t引用上例，我们将ca.pem、server.crt、server.key复制到docker集群的192.168.196.135主机的/etc/docker/目录下，并修改daemon.json开启docker服务TLS认证。此时客户端认证模式选择第四种时，会提示server证书无效，因为在签发证书时指定了允许通过IP地址访问的IP为192.168.196.135。\n#登录到135主机 $ cd $ docker --tlsverify --tlscacert ca.pem --tlscert client.crt --tlskey=client.key -H tcp://192.168.196.135:3666 version Client: Version: 1.13.1 API version: 1.26 Package version: error during connect: Get https://192.168.196.135:3666/v1.26/version: x509: certificate is valid for 192.168.196.136, not 192.168.196.135 $ docker --tls --tlscacert ca.pem --tlscert client.crt --tlskey=client.key -H tcp://192.168.196.135:3666 version Client: Version: 1.13.1 API version: 1.26 Package version: docker-1.13.1-68.gitdded712.el7.centos.x86_64 Go version: go1.9.4 Git commit: dded712/1.13.1 Built: Tue Jul 17 18:34:48 2018 OS/Arch: linux/amd64 Server: Version: 1.13.1 API version: 1.26 (minimum version 1.12) Package version: docker-1.13.1-68.gitdded712.el7.centos.x86_64 Go version: go1.9.4 Git commit: dded712/1.13.1 Built: Tue Jul 17 18:34:48 2018 OS/Arch: linux/amd64 Experimental: false ​\t如果客户端认证模式选择第四种，可以在签发server证书时，将现有docker集群的IP添加到extfile.cnf，然后拷贝到集群中的主机上，分别配置docker服务开启认证即可，而对于后续新添加节点，则需要更新extfile.cnf并重新签发server证书。\n$ echo subjectAltName=IP:192.168.196.136,IP:192.168.196.135 \u0026gt; extfile.cnf $ openssl x509 -passin pass:123456 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey cakey.pem -CAcreateserial -out server.crt -extfile extfile.cnf ​\t如果客户端认证模式选择不校验服务端证书，则无需指定IP，直接拷贝签发的server证书到所有节点即可\n","date":"8 August 2018","permalink":"/2018/08/docker-tls/","section":"博客","summary":"\u003ch4 class=\"relative group\"\u003e背景 \n    \u003cdiv id=\"背景\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e8%83%8c%e6%99%af\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h4\u003e\n\u003cblockquote\u003e\n\u003cp\u003e通常，我们在配置docker daemon的监听方式为本地的socket和开放TCP端口，docker服务允许远程访问，方便对docker集群的主机执行管理命令及调用等。如果不使用TLS加密docker，任意客户端都可以对docker主机进行远程操作，有很大的安全隐患。而配置TLS后，只有信任的客户端才可以远程访问docker服务。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Docker配置TLS安全认证"},{"content":"","date":"8 August 2018","permalink":"/tags/openssl/","section":"Tags","summary":"","title":"openssl"},{"content":"背景 # 公司容器管理平台需要周期性对weblogic镜像进行打补丁升级操作，由于docker的分层管理机制，weblogic镜像体积会越来越大，影响后续的维护部署。我就写了一个批量export镜像并推送至镜像仓库的脚本，来精简镜像大小\n脚本运行示例 # 脚本执行步骤 # 准备镜像列表文件\n$ cat images_list.txt \u0026lt;-- 需要export的镜像 registry:8088/nginx:latest registry:8088/kubernetes-dashboard-amd64:v1.5.0 #export之前镜像仓库中的镜像tag $ curl registry:8088/v2/nginx/tags/list {\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;latest\u0026#34;]} $ curl registry:8088/v2/kubernetes-dashboard-amd64/tags/list {\u0026#34;name\u0026#34;:\u0026#34;kubernetes-dashboard-amd64\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;v1.5.0\u0026#34;]} 运行脚本\n$ sh export-images.sh -h ------------------------------------------------------------------------- Usage: export_images.sh -f IMAGES_FILE -t IMAGE_TAG [-p THREAD_NUM] [-h] Description: IMAGES_FILE: the list of images need to be exported IMAGE_TAG: the target tag of exported images THREAD_NUM: the target thread numbers ------------------------------------------------------------------------- $ sh export-images.sh -f images_list.txt -t test -p 2 开始执行任务 registry:8088/kubernetes-dashboard-amd64:test pushed registry:8088/nginx:test pushed 任务执行结束 #export之后镜像仓库中的镜像tag，增加了test $ curl registry:8088/v2/nginx/tags/list {\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;latest\u0026#34;,\u0026#34;test\u0026#34;]} $ curl registry:8088/v2/kubernetes-dashboard-amd64/tags/list {\u0026#34;name\u0026#34;:\u0026#34;kubernetes-dashboard-amd64\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;v1.5.0\u0026#34;,\u0026#34;test\u0026#34;]} 查看脚本运行日志\n$ cat export_images.log Trying to pull repository registry:8088/kubernetes-dashboard-amd64 ... v1.5.0: Pulling from registry:8088/kubernetes-dashboard-amd64 Digest: sha256:ddd3454819c089517b434a3ea42abf3c184fce9bf45704abf22513082d900eba Status: Image is up to date for registry:8088/kubernetes-dashboard-amd64:v1.5.0 .....中间省略..... Sending build context to Docker daemon 2.048 kB Step 1/5 : FROM registry:8088/kubernetes-dashboard-amd64:tmp ---\u0026gt; 17b601e3c483 .....中间省略..... Successfully built 8712115d0f0b Login Succeeded The push refers to a repository [registry:8088/nginx] .....中间省略..... registry:8088/kubernetes-dashboard-amd64:test pushed Untagged: registry:8088/kubernetes-dashboard-amd64:tmp Untagged: registry:8088/kubernetes-dashboard-amd64:test Untagged: registry:8088/kubernetes-dashboard-amd64@sha256:5c601d866b5a6b135fc6654417c4f8726661ce6903f00fc96ffe8127cdc272a0 .....中间省略..... Deleted: sha256:cc940a53e84603ed3886909d8150f456fc93d25abf85d5ea07099d92e33640d9 .....后面省略..... shell脚本 # $ cat export-images.sh #!/bin/bash #Author:Feixiang Fu #set -x #指定镜像仓库的验证用户及密码 user_name=fufeixiang passwd=123456 tty=$(tty) usage(){ echo \u0026#34;--------------------------------------------------------------\u0026#34; echo \u0026#34;Usage:\u0026#34; echo -e \u0026#34;\\texport_images.sh -f IMAGES_FILE -t IMAGE_TAG [-p THREAD_NUM] [-h]\u0026#34; echo \u0026#34;Description:\u0026#34; echo -e \u0026#34;\\tIMAGES_FILE: the list of images need to be exported\u0026#34; echo -e \u0026#34;\\tIMAGE_TAG: the target tag of exported images\u0026#34; echo -e \u0026#34;\\tTHREAD_NUM: the target thread numbers\u0026#34; echo \u0026#34;--------------------------------------------------------------\u0026#34; exit -1 } function work(){ i=$RANDOM docker pull $org_image #随机容器名称 docker run -d --name container$i $org_image sh -c \u0026#34;tail -f /etc/hosts\u0026#34; docker export container$i \u0026gt; container${i}.tar docker import container${i}.tar $image_name:tmp docker rm -f container$i rm -rf container${i}.tar mkdir build-$i cat \u0026lt;\u0026lt;EOF \u0026gt; build-$i/Dockerfile FROM $image_name:tmp WORKDIR /app/bin ENTRYPOINT [\u0026#34;/usr/bin/dumb-init\u0026#34;,\u0026#34;--\u0026#34;] USER dcos CMD [\u0026#34;sh\u0026#34;, \u0026#34;/app/bin/startServer.sh\u0026#34;] EOF #export镜像之后，entrypoint/cmd等会丢失，需要重新定制 docker build -t $image_name:$tag build-$i/ docker login -u $user_name -p $passwd $image_addr if [ \u0026#34;$?\u0026#34; -eq 0 ];then docker push $image_name:$tag \u0026amp;\u0026amp; echo -e \u0026#34;\\e[32m${image_name}:${tag} pushed\\e[0m\u0026#34; |tee $tty else echo -e \u0026#34;\\e[31m账户${user_name}密码不匹配,${image_name}:${tag} push failed\\e[0m\u0026#34; |tee $tty fi #清理import的镜像及build的新镜像 docker rmi $image_name:tmp $image_name:$tag } [ -z \u0026#34;$*\u0026#34; ] \u0026amp;\u0026amp; usage \u0026amp;\u0026amp; exit 1 while getopts \u0026#39;f:t:p:h\u0026#39; opt;do case $opt in f) IMAGES_FILE=\u0026#34;$OPTARG\u0026#34;;; t) tag=\u0026#34;$OPTARG\u0026#34;;; p) thread=\u0026#34;$OPTARG\u0026#34;;; h) usage;; ?) usage;; esac done echo \u0026#34;开始执行任务\u0026#34; #支持多进程模式 mkfifo tmp.fifo exec 6\u0026lt;\u0026gt; tmp.fifo thread=${thread:-1} for ((j=0;j\u0026lt;$thread;j++)); do echo done \u0026gt;\u0026amp;6 while read line || [ -n \u0026#34;$line\u0026#34; ];do org_image=${line%% *} image_name=${line%:*} #user_name=`echo $line|cut -d\u0026#34; \u0026#34; -f2` #passwd=${line##* } image_addr=${line%%/*} read -u6 { work \u0026gt;\u0026gt; export_images.log echo \u0026gt;\u0026amp;6 } \u0026amp; done \u0026lt; $IMAGES_FILE wait exec 6\u0026gt;\u0026amp;- rm -rf tmp.fifo build-*/ echo \u0026#34;任务执行结束\u0026#34; ","date":"7 August 2018","permalink":"/2018/08/docker-images-export/","section":"博客","summary":"背景 # 公司容器管理平台需要周期性对weblogic镜像进行打补丁升级操作，由于docker的分层管理机制，weblogic镜像体积会越来越大，影响后续的维护部署。我就写了一个批量export镜像并推送至镜像仓库的脚本，来精简镜像大小\n脚本运行示例 # 脚本执行步骤 # 准备镜像列表文件\n$ cat images_list.txt \u0026lt;-- 需要export的镜像 registry:8088/nginx:latest registry:8088/kubernetes-dashboard-amd64:v1.5.0 #export之前镜像仓库中的镜像tag $ curl registry:8088/v2/nginx/tags/list {\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;latest\u0026#34;]} $ curl registry:8088/v2/kubernetes-dashboard-amd64/tags/list {\u0026#34;name\u0026#34;:\u0026#34;kubernetes-dashboard-amd64\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;v1.5.0\u0026#34;]} 运行脚本\n$ sh export-images.sh -h ------------------------------------------------------------------------- Usage: export_images.","title":"批量export Docker镜像脚本"},{"content":"","date":"23 July 2018","permalink":"/tags/go/","section":"Tags","summary":"","title":"go"},{"content":"1.json解析到Struct类型\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type Book struct { BookName string `json:\u0026#34;bookname\u0026#34;` AuthorName string `json:\u0026#34;authorname\u0026#34;` AuthorAge string `json:\u0026#34;authorage\u0026#34;` } func main() { jsonbuf := ` { \u0026#34;bookname\u0026#34;:\u0026#34;booker\u0026#34;, \u0026#34;authorname\u0026#34;: \u0026#34;Tom\u0026#34;, \u0026#34;authorage\u0026#34;: \u0026#34;28\u0026#34; }` var book Book err := json.Unmarshal([]byte(jsonbuf), \u0026amp;book) if err != nil { fmt.Println(err) return } fmt.Printf(\u0026#34;book = %+v\\n\u0026#34;, book) // book = {BookName:booker AuthorName:Tom AuthorAge:28} } 2.json解析到自定义类型\ntype Unmarshaler interface { UnmarshalJSON([]byte) error } encoding/json包中定义了Unmarshaler接口，默认UnmarshalJSON方法是对要解析的json数据不做任何处理。所以要解析json数据到自定义类型，此类型需实现Unmarshaler接口。\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) type Book struct { BookName string `json:\u0026#34;bookname\u0026#34;` Author Author `json:\u0026#34;author\u0026#34;` } type Author struct { Name string Age string } // Author结构体实现json.Unmarshaler接口，解析json数据时执行其UnmarshalJSON方法 func (a *Author) UnmarshalJSON(data []byte) (err error) { str := string(bytes.Trim(data, `\u0026#34; `)) // 删除要解析的data中的\u0026#34;和空格 split := strings.Split(str, \u0026#34;:\u0026#34;) if len(split) != 2 { return err } a.Name = split[0] a.Age = split[1] return nil } func main() { jsonbuf := ` { \u0026#34;bookname\u0026#34;:\u0026#34;booker\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Tom:28\u0026#34; }` var book Book err := json.Unmarshal([]byte(jsonbuf), \u0026amp;book) if err != nil { fmt.Println(err) return } fmt.Printf(\u0026#34;book = %+v\\n\u0026#34;, book) // book = {BookName:booker Author:{Name:Tom Age:28}} } ","date":"23 July 2018","permalink":"/2018/07/golang-json/","section":"博客","summary":"1.json解析到Struct类型\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) type Book struct { BookName string `json:\u0026#34;bookname\u0026#34;` AuthorName string `json:\u0026#34;authorname\u0026#34;` AuthorAge string `json:\u0026#34;authorage\u0026#34;` } func main() { jsonbuf := ` { \u0026#34;bookname\u0026#34;:\u0026#34;booker\u0026#34;, \u0026#34;authorname\u0026#34;: \u0026#34;Tom\u0026#34;, \u0026#34;authorage\u0026#34;: \u0026#34;28\u0026#34; }` var book Book err := json.","title":"Golang自定义json解析类型的实现"},{"content":"","date":"8 May 2018","permalink":"/tags/direct-lvm/","section":"Tags","summary":"","title":"direct-lvm"},{"content":" devicemapper是Linux发行版RHEL下Docker Engine的默认存储驱动，它有两种配置模式: loop-lvm和direct-lvm\nloop-lvm是默认模式，基于loop设备文件创建thin pool来存储docker镜像及容器数据，官方不推荐在生产环境中使用此模式 direct-lvm是直接使用块设备创建thin pool，在中等负载和高密度环境下会有更好的性能优势，也是官方推荐在生产环境中使用的模式 基础环境准备 # 操作系统: CentOS7.2\nDocker Engine: docker-engine-1.13.1-1.el7.centos.x86_64\n硬盘: /dev/sda,/dev/sdb\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 200G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 190G 0 part ├─centos-root 253:0 0 90G 0 lvm / └─centos-data 253:1 0 100G 0 lvm /data sdb 8:16 0 100G 0 disk sr0 11:0 1 603M 0 rom 划分逻辑卷 # 基于/dev/sdb制作逻辑卷组appvg, 并划分逻辑卷appvg-docker，用于挂载docker graph目录(/docker) $ pvcreate /dev/sdb Physical volume \u0026#34;/dev/sdb\u0026#34; successfully created $ vgcreate appvg /dev/sdb Volume group \u0026#34;appvg\u0026#34; successfully created #规划20G用于逻辑卷/dev/appvg/docker $ lvcreate -n docker -L 20G appvg Logical volume \u0026#34;docker\u0026#34; created. $ lvs appvg LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert docker appvg -wi-a----- 20.00g $ mkfs.xfs /dev/appvg/docker meta-data=/dev/appvg/docker isize=256 agcount=4, agsize=1310720 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 data = bsize=4096 blocks=5242880, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=0 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 $ mkdir /docker $ mount /dev/appvg/docker /docker #开机自动挂载 $ tail -n1 /etc/fstab /dev/mapper/appvg-docker /docker xfs defaults 0 0 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 200G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 190G 0 part ├─centos-root 253:0 0 90G 0 lvm / └─centos-data 253:1 0 100G 0 lvm /data sdb 8:16 0 100G 0 disk └─appvg-docker 253:2 0 20G 0 lvm /docker sr0 11:0 1 603M 0 rom 基于逻辑卷组appvg, 创建thinpool和thinpoolmeta $ lvcreate --wipesignatures y -n thinpool -L 40G appvg Logical volume \u0026#34;thinpool\u0026#34; created. $ lvcreate --wipesignatures y -n thinpoolmeta -L 800M appvg Logical volume \u0026#34;thinpoolmeta\u0026#34; created. #将逻辑卷thinpool和thinpoolmeta转换为thin pool $ lvconvert -y --zero n -c512K --thinpool appvg/thinpool --poolmetadata appvg/thinpoolmeta WARNING: Converting logical volume appvg/thinpool and appvg/thinpoolmeta to pool\u0026#39;s data and metadata volumes. THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.) Converted appvg/thinpool to thin pool. #创建thinpool自动扩容配置文件 $vi /etc/lvm/profile/appvg-thinpool.profile activation { thin_pool_autoextend_threshold=60 thin_pool_autoextend_percent=20 } #应用thinpool配置文件 $ lvchange --metadataprofile appvg-thinpool appvg/thinpool Logical volume \u0026#34;thinpool\u0026#34; changed. #查看thinpool处于监控状态 $ lvs -o+seg_monitor appvg LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert Monitor docker appvg -wi-ao---- 20.00g thinpool appvg twi-a-t--- 40.00g 0.00 0.01 monitored 启动docker服务 # 创建docker配置文件/etc/docker/daemon.json $ cat /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;0.0.0.0/0\u0026#34;], \u0026#34;hosts\u0026#34;: [\u0026#34;unix:///var/run/docker.sock\u0026#34;, \u0026#34;tcp://0.0.0.0:3666\u0026#34;], \u0026#34;graph\u0026#34;: \u0026#34;/docker\u0026#34;, \u0026#34;storage-driver\u0026#34;: \u0026#34;devicemapper\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;dm.thinpooldev=/dev/mapper/appvg-thinpool\u0026#34;, \u0026#34;dm.use_deferred_deletion=true\u0026#34;, \u0026#34;dm.use_deferred_removal=true\u0026#34; ] } 启动docker服务并执行docker info $ systemctl start docker $ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 1.13.1 Storage Driver: devicemapper Pool Name: appvg-thinpool --\u0026gt; 显示为创建的逻辑卷thinpool Pool Blocksize: 524.3 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: Metadata file: Data Space Used: 20.45 MB Data Space Total: 42.95 GB Data Space Available: 42.93 GB Metadata Space Used: 110.6 kB Metadata Space Total: 838.9 MB Metadata Space Available: 838.8 MB Thin Pool Minimum Free Space: 4.295 GB Udev Sync Supported: true Deferred Removal Enabled: true Deferred Deletion Enabled: true Deferred Deleted Device Count: 0 ...中间省略... Docker Root Dir: /docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled Experimental: false Insecure Registries: 0.0.0.0/0 127.0.0.0/8 Live Restore Enabled: false ","date":"8 May 2018","permalink":"/2018/05/docker-devicemapper/","section":"博客","summary":"devicemapper是Linux发行版RHEL下Docker Engine的默认存储驱动，它有两种配置模式: loop-lvm和direct-lvm\nloop-lvm是默认模式，基于loop设备文件创建thin pool来存储docker镜像及容器数据，官方不推荐在生产环境中使用此模式 direct-lvm是直接使用块设备创建thin pool，在中等负载和高密度环境下会有更好的性能优势，也是官方推荐在生产环境中使用的模式 基础环境准备 # 操作系统: CentOS7.2\nDocker Engine: docker-engine-1.13.1-1.el7.centos.x86_64\n硬盘: /dev/sda,/dev/sdb\n$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 200G 0 disk ├─sda1 8:1 0 500M 0 part /boot └─sda2 8:2 0 190G 0 part ├─centos-root 253:0 0 90G 0 lvm / └─centos-data 253:1 0 100G 0 lvm /data sdb 8:16 0 100G 0 disk sr0 11:0 1 603M 0 rom 划分逻辑卷 # 基于/dev/sdb制作逻辑卷组appvg, 并划分逻辑卷appvg-docker，用于挂载docker graph目录(/docker) $ pvcreate /dev/sdb Physical volume \u0026#34;/dev/sdb\u0026#34; successfully created $ vgcreate appvg /dev/sdb Volume group \u0026#34;appvg\u0026#34; successfully created #规划20G用于逻辑卷/dev/appvg/docker $ lvcreate -n docker -L 20G appvg Logical volume \u0026#34;docker\u0026#34; created.","title":"Docker存储驱动direct-lvm的配置"},{"content":"","date":"25 February 2018","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"方法一 # 1. os.system(\u0026lsquo;cmd\u0026rsquo;)方法\n使用system方法可以返回运行cmd命令的状态返回值，同时会在终端输出运行结果，但无法将执行的结果保存起来\nsystem方法比较适用于执行单个命令、通常没有输出结果的情况\npython交互模式下 \u0026gt;\u0026gt;\u0026gt; os.system(\u0026#39;tar cvf /data/1.tar /data/docker\u0026#39;) tar: Removing leading `/\u0026#39; from member names /data/docker/ /data/docker/auth/ /data/docker/auth/htpasswd /data/docker/a.sh 0 \u0026lt;-- 命令执行状态返回值 python脚本中 $ cat sh-system.py #!/usr/bin/python36 import os,sys if len(sys.argv) \u0026lt; 3 : print (\u0026#39;2 arguments is needed\u0026#39;) sys.exit(1) os.system(\u0026#39;./test.sh %s %s\u0026#39;%(sys.argv[1],sys.argv[2])) \u0026lt;--自动打印cmd在linux上执行的信息，无需使用print #os.system(\u0026#39;./test.sh \u0026#39;+sys.argv[1]+\u0026#39; \u0026#39;+sys.argv[2]) $ cat test.sh #!/usr/bin/bash echo -e \u0026#34;This is a shell script. \\nNAME:$1\\tAUTHOR:$2\u0026#34; 运行python脚本，终端结果如下\n[root@master python_learning]# ./sh-system.py test tom This is a shell script. NAME:test AUTHOR:tom 方法二 # 2. commands.getstatusoutput(\u0026lsquo;cmd1;cmd2;\u0026hellip;\u0026rsquo;)方法\ncommands.getstatusoutput方法可以取得命令的输出（包括标准和错误输出）和执行状态位\npython3已经使用subprocess取代\npython2交互式模式下 [root@master python_learning]# python Python 2.7.5 (default, Aug 4 2017, 00:39:18) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import os,commands \u0026gt;\u0026gt;\u0026gt; status,result=commands.getstatusoutput(\u0026#39;ls\u0026#39;) \u0026gt;\u0026gt;\u0026gt; status 0 \u0026gt;\u0026gt;\u0026gt; result \u0026#39;getIP.py\\nsh-commands.py\\nsh-popen.py\\nsh-subprocess.py\\nsh-system.py\\ntest.sh\\nzipdir\\nzip.py\u0026#39; \u0026gt;\u0026gt;\u0026gt; commands.getoutput(\u0026#39;ls\u0026#39;) \u0026lt;--只返回命令执行结果 \u0026#39;getIP.py\\nsh-commands.py\\nsh-popen.py\\nsh-subprocess.py\\nsh-system.py\\ntest.sh\\nzipdir\\nzip.py\u0026#39; python脚本中 [root@master python_learning]# cat ./sh-commands.py #!/usr/bin/python import os,commands status,result=commands.getstatusoutput(\u0026#39;ls;cat /data/1.txt\u0026#39;) print (status) print (result) 运行python脚本，终端结果如下\n[root@master python_learning]# ./sh-commands.py 256 \u0026lt;--cmd1;cmd2;...中有一个cmd执行失败,状态返回码就不为0 sh-commands.py sh-system.py test.sh cat: /data/1.txt: No such file or directory 方法三 # 3.os.popen(\u0026lsquo;cmd\u0026rsquo;)方法\npopen(command [, mode=\u0026lsquo;r\u0026rsquo; [, bufsize]])\n-\u0026gt; pipe Open a pipe to/from a command returning a file object. 返回一个类文件对象，调用该对象的read()或readlines()方法可以读取输出内容\ncommand \u0026ndash; 使用的命令\nmode \u0026ndash; 模式权限可以是 \u0026lsquo;r\u0026rsquo;(默认) 或 \u0026lsquo;w\u0026rsquo;\nbufsize \u0026ndash; 指明了文件需要的缓冲大小：0意味着无缓冲；1意味着行缓冲；其它正值表示使用参数大小的缓冲（大概值，以字节为单位）。负的bufsize意味着使用系统的默认值，一般来说，对于tty设备，它是行缓冲；对于其它文件，它是全缓冲。如果没有改参数，使用系统的默认值\npython脚本中 [root@master python_learning]# vim sh-popen.py #!/usr/bin/env python36 #-*- coding:utf-8 -*- import os,sys if len(sys.argv) \u0026lt; 3 : print (\u0026#39;2 arguments is needed\u0026#39;) sys.exit(1) cmd = \u0026#39;./test.sh %s %s\u0026#39;%(sys.argv[1],sys.argv[2]) print (os.popen(cmd).readlines()) \u0026lt;--返回的是类文件对象，需要调用print打印出来 print (os.popen(cmd).read(),end=\u0026#39;\u0026#39;) 运行python脚本，终端结果如下\n[root@master python_learning]# ./sh-popen.py test tom [\u0026#39;This is a shell script. \\n\u0026#39;, \u0026#39;NAME:test\\tAUTHOR:tom\\n\u0026#39;] This is a shell script. NAME:test AUTHOR:tom 方法四 # 4.subprocess模块\nsubprocess被用来替换一些老的模块和函数，如：os.system、os.spawn*、os.popen*、popen2.、commands.。所以强烈推荐使用subprocess模块。\nsubprocess模块中只定义了一个类: Popen。可以使用Popen来创建进程，并与进程进行复杂的交互。它的构造函数如下：\n*class subprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=False, shell=False, cwd=None, env=None, universal_newlines=False, startupinfo=None, creationflags=0,restore_signals=True, start_new_session=False, pass_fds=(), , encoding=None, errors=None)\n各参数请参考 Python Documentation中的定义\n常用的几个函数subprocess.call()、subprocess.check_call()、subprocess.check_output()、subprocess.Popen()。实际上，上面的几个函数都是基于Popen()的封装(wrapper)。这些封装的目的在于让我们容易使用子进程。\nsubprocess.call() subprocess.call(args,* , stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None)\nRun the command described by args. Wait for command to complete, then return the returncode attribute\n运行命令，父进程等待子进程完成(阻塞)，然后返回returncode(状态码)\n[root@master python_learning]# python36 Python 3.6.3 (default, Jan 4 2018, 16:40:53) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; subprocess.call([\u0026#39;ls\u0026#39;,\u0026#39;-l\u0026#39;,\u0026#39;/data\u0026#39;]) \u0026lt;--args序列化的表达方式 total 20 -rw-r--r--. 1 root root 10240 Feb 8 11:06 1.tar -rw-r--r--. 1 root root 24 Feb 22 14:56 1.txt drwxr-xr-x. 2 root root 79 Dec 25 09:56 build_images drwxr-xr-x. 2 root root 6 Jan 15 16:05 container drwxr-xr-x. 4 root root 30 Jan 10 15:47 dcos drwxr-xr-x. 3 root root 28 Feb 8 11:06 docker drwxr-xr-x. 3 root root 4096 Feb 22 15:31 python_learning drwxr-xr-x. 5 root root 36 Dec 11 11:07 zookeeper 0 \u0026gt;\u0026gt;\u0026gt; subprocess.call(\u0026#39;ls -l /data\u0026#39;,shell=True) \u0026lt;--args字符串表达，指定shell=True total 20 -rw-r--r--. 1 root root 10240 Feb 8 11:06 1.tar -rw-r--r--. 1 root root 24 Feb 22 14:56 1.txt drwxr-xr-x. 2 root root 79 Dec 25 09:56 build_images drwxr-xr-x. 2 root root 6 Jan 15 16:05 container drwxr-xr-x. 4 root root 30 Jan 10 15:47 dcos drwxr-xr-x. 3 root root 28 Feb 8 11:06 docker drwxr-xr-x. 3 root root 4096 Feb 22 15:31 python_learning drwxr-xr-x. 5 root root 36 Dec 11 11:07 zookeeper 0 \u0026gt;\u0026gt;\u0026gt; subprocess.call(\u0026#39;ls -l /data/ ; ls notexistfile\u0026#39;,shell=True) total 20 -rw-r--r--. 1 root root 10240 Feb 8 11:06 1.tar -rw-r--r--. 1 root root 24 Feb 22 14:56 1.txt drwxr-xr-x. 2 root root 79 Dec 25 09:56 build_images drwxr-xr-x. 2 root root 6 Jan 15 16:05 container drwxr-xr-x. 4 root root 30 Jan 10 15:47 dcos drwxr-xr-x. 3 root root 28 Feb 8 11:06 docker drwxr-xr-x. 3 root root 4096 Feb 22 15:31 python_learning drwxr-xr-x. 5 root root 36 Dec 11 11:07 zookeeper ls: cannot access notexistfile: No such file or directory 2 Tips:\n使用shlex.split()方法序列化args\n\u0026gt;\u0026gt;\u0026gt; import shlex,subprocess \u0026gt;\u0026gt;\u0026gt; cmd=input() /bin/echo -e -n \u0026#34;this a test \u0026#39;$CASE\u0026#39;\u0026#34; \u0026gt;\u0026gt;\u0026gt; args=shlex.split(cmd) \u0026gt;\u0026gt;\u0026gt; print (args) [\u0026#39;/bin/echo\u0026#39;, \u0026#39;-e\u0026#39;, \u0026#39;-n\u0026#39;, \u0026#34;this a test \u0026#39;$CASE\u0026#39;\u0026#34;] \u0026gt;\u0026gt;\u0026gt; p = subprocess.Popen(args) \u0026gt;\u0026gt;\u0026gt; this a test \u0026#39;$CASE\u0026#39; subprocess.check_call() subprocess.check_call(args, *,stdin=None, stdout=None, stderr=None, shell=False, cwd=None, timeout=None)\nRun command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute.\n运行命令，父进程等待子进程完成(阻塞)，如果子进程返回的returncode不为0的话，将抛出CalledProcessError异常。在异常对象中，包括进程的returncode信息。\n\u0026gt;\u0026gt;\u0026gt; subprocess.check_call(\u0026#39;ls -l /data/;ls notexistfile\u0026#39;,shell=True) total 20 -rw-r--r--. 1 root root 10240 Feb 8 11:06 1.tar -rw-r--r--. 1 root root 24 Feb 22 14:56 1.txt drwxr-xr-x. 2 root root 79 Dec 25 09:56 build_images drwxr-xr-x. 2 root root 6 Jan 15 16:05 container drwxr-xr-x. 4 root root 30 Jan 10 15:47 dcos drwxr-xr-x. 3 root root 28 Feb 8 11:06 docker drwxr-xr-x. 3 root root 4096 Feb 22 15:31 python_learning drwxr-xr-x. 5 root root 36 Dec 11 11:07 zookeeper ls: cannot access notexistfile: No such file or directory Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/usr/lib64/python3.6/subprocess.py\u0026#34;, line 291, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command \u0026#39;ls -l /data/;ls notexistfile\u0026#39; returned non-zero exit status 2. subprocess.check_output() subprocess.check_output(args, *, stdin=None, stderr=None, shell=False, cwd=None, encoding=None, errors=None, universal_newlines=False, timeout=None)\nRun command with arguments and return its output.\nIf the return code was non-zero it raises a CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute and any output in the output attribute.\n运行命令，父进程等待子进程完成(阻塞)，并返回子进程的标准输出，如果子进程返回的returncode不为0的话，将抛出CalledProcessError异常。在异常对象中，包括进程的returncode信息和标准输出的信息\n\u0026gt;\u0026gt;\u0026gt; subprocess.check_output(\u0026#39;ls -l\u0026#39;,shell=True) b\u0026#39;total 32\\n-rw-r--r--. 1 root root 164 Feb 22 19:28 \\\\\\n-rwxr-xr-x. 1 root root 229 Feb 22 10:24 getIP.py\\n-rwxr-xr-x. 1 root root 253 Feb 8 13:37 sh-commands.py\\n-rwxr-xr-x. 1 root root 369 Feb 22 15:31 sh-popen.py\\n-rwxr-xr-x. 1 root root 572 Feb 8 20:44 sh-subprocess.py\\n-rwxr-xr-x. 1 root root 407 Feb 22 14:46 sh-system.py\\n-rwxr-xr-x. 1 root root 72 Feb 8 13:24 test.sh\\ndrwxr-xr-x. 4 root root 42 Feb 8 18:29 zipdir\\n-rwxr-xr-x. 1 root root 1046 Feb 8 19:00 zip.py\\n\u0026#39; \u0026gt;\u0026gt;\u0026gt; subprocess.check_output(\u0026#39;ls notexistfile\u0026#39;,shell=True) ls: cannot access notexistfile: No such file or directory Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/usr/lib64/python3.6/subprocess.py\u0026#34;, line 336, in check_output **kwargs).stdout File \u0026#34;/usr/lib64/python3.6/subprocess.py\u0026#34;, line 418, in run output=stdout, stderr=stderr) subprocess.CalledProcessError: Command \u0026#39;ls notexistfile\u0026#39; returned non-zero exit status 2. 使用try和except语句处理subprocess.CalledProcessError异常\n#!/usr/bin/env python36 import subprocess code_flag=0 try: p2=subprocess.check_output(\u0026#39;ls notexistfile\u0026#39;,shell=True) print(p2) except subprocess.CalledProcessError: #print (\u0026#39;returncode is not 0\u0026#39;) code_flag=2 if code_flag != 0: print (\u0026#39;execute cmd failed\u0026#39;) 如果要获取标准错误，需定义stderr参数 stderr=subprocess.STDOUT\n\u0026gt;\u0026gt;\u0026gt; subprocess.check_output( ... \u0026#34;ls non_existent_file; exit 0\u0026#34;, ... stderr=subprocess.STDOUT, ... shell=True) \u0026#39;ls: non_existent_file: No such file or directory\\n\u0026#39; subprocess.Popen() 经常的使用方法为subporcess.Popen, 我们可以在Popen()建立子进程的时候改变标准输入、标准输出和标准错误，并可以利用subprocess.PIPE将多个子进程的输入和输出连接在一起，构成管道(pipe):\nimport subprocess child1 = subprocess.Popen([\u0026#34;ls\u0026#34;,\u0026#34;-l\u0026#34;], stdout=subprocess.PIPE) child2 = subprocess.Popen([\u0026#34;wc\u0026#34;], stdin=child1.stdout,stdout=subprocess.PIPE) out = child2.communicate() print(out) subprocess.PIPE\n在创建Popen对象时，subprocess.PIPE可以初始化stdin, stdout或stderr参数。表示与子进程通信的标准流，它实际上为文本流提供一个缓存区，缓存区有一定的容量限制，当缓存区满了之后，子进程就会停止写入数据，程序就会卡住\nPopen对象创建后，主程序不会自动等待子进程完成。我们必须调用对象的wait()方法，父进程才会等待 (也就是阻塞block)。这里要注意的是如果进程输出文本数据超过buffersize（默认64k）,调用wait()方法会使程序锁死，推荐使用communicate()方法（communicate()是Popen对象的一个方法，该方法也会阻塞父进程，直到子进程完成 ），这个方法会把输出放在内存，而不是管道里，所以这时候上限就和内存大小有关了，一般不会有问题。而且如果要获得程序返回值，可以在调用 Popen.communicate() 之后取 Popen.returncode 的值。但如果超过内存，那么要考虑比如文件 stdout=open(\u0026ldquo;process.out\u0026rdquo;, \u0026ldquo;w\u0026rdquo;) 的方式来解决，不能再使用管道了\nPopen对象的方法：\nPopen.poll() 用于检查子进程是否已经结束。设置并返回returncode属性。\nPopen.wait() 等待子进程结束。设置并返回returncode属性。\nPopen.communicate(input=None) 与子进程进行交互。向stdin发送数据，或从stdout和stderr中读取数据。可选参数input指定发送到子进程的参数。 Communicate()返回一个元组：(stdoutdata, stderrdata)。注意：如果希望通过进程的stdin向其发送数据，在创建Popen对象的时候，参数stdin必须被设置为PIPE。同样，如 果希望从stdout和stderr获取数据，必须将stdout和stderr设置为PIPE。\nPopen.send_signal(signal) 向子进程发送信号。\nPopen.terminate() 停止(stop)子进程。在windows平台下，该方法将调用Windows API TerminateProcess（）来结束子进程。\nPopen.kill() 杀死子进程。\nPopen.stdin 如果在创建Popen对象是，参数stdin被设置为PIPE，Popen.stdin将返回一个文件对象用于策子进程发送指令。否则返回None。\nPopen.stdout 如果在创建Popen对象是，参数stdout被设置为PIPE，Popen.stdout将返回一个文件对象用于策子进程发送指令。否则返回 None。（类似于os.popen()方法）\nPopen.stderr 如果在创建Popen对象是，参数stdout被设置为PIPE，Popen.stdout将返回一个文件对象用于策子进程发送指令。否则返回 None。\nPopen.pid 获取子进程的进程ID。\nPopen.returncode 获取进程的返回值。如果进程还没有结束，返回None。\n实例一 :\n使用Popen的communicate()方法保存子进程输出信息，避免程序死锁\n[root@master python_learning]# vim sh-subprocess.py #!/usr/bin/env python36 #-*- coding:utf-8 -*- import os,sys import subprocess if len(sys.argv) \u0026lt; 3 : print (\u0026#39;2 arguments is needed\u0026#39;) sys.exit(1) cmd = \u0026#39;./test.sh %s %s\u0026#39;%(sys.argv[1],sys.argv[2]) p=subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE) #p.wait() #阻塞父进程，等待cmd执行完成 #print(p.stdout.read()) #从管道读出输出信息 output,errors=p.communicate() #输出信息缓存到内存中;进程结束后，返回元组(stdoutdata, stderrdata) print(output) #打印stdoutdata if p.returncode != 0: print (\u0026#34;execute shell script failed\u0026#34;) 实例二 :\nenv参数默认为None，子进程默认继承父进程的环境变量。但是，一旦你自定义了env的值（类型为字典），则子程序的环境变量全部由env参数决定，与父进程无关\n\u0026gt;\u0026gt;\u0026gt; p=subprocess.Popen(\u0026#39;echo this is a test \u0026#34;$case\u0026#34;\u0026#39;,env={\u0026#39;case\u0026#39;:\u0026#39;123456\u0026#39;},shell=True,stdout=subprocess.PIPE) \u0026gt;\u0026gt;\u0026gt; print(p.stdout.read()) b\u0026#39;this is 123456\\n\u0026#39; 方法五 # 5.sh模块\n参考 官方文档\n调用常用命令 \u0026gt;\u0026gt;\u0026gt; import sh \u0026gt;\u0026gt;\u0026gt; sh.ls() getIP.py sh-popen.py sh-system.py test.sh zip.py sh-commands.py sh-subprocess.py sub1.py zipdir \u0026gt;\u0026gt;\u0026gt; sh.ls(\u0026#39;-l\u0026#39;,\u0026#39;./getIP.py\u0026#39;) #命令参数以字符串形式传递 -rwxr-xr-x. 1 root root 229 Feb 22 10:24 ./getIP.py 或者 \u0026gt;\u0026gt;\u0026gt; from sh import ls \u0026gt;\u0026gt;\u0026gt; ls() getIP.py sh-popen.py sh-system.py test.sh zip.py sh-commands.py sh-subprocess.py sub1.py zipdir \u0026gt;\u0026gt;\u0026gt; ls(\u0026#39;-l\u0026#39;,\u0026#39;./getIP.py\u0026#39;) -rwxr-xr-x. 1 root root 229 Feb 22 10:24 ./getIP.py 未完待续。。。\n","date":"25 February 2018","permalink":"/2018/02/python-shell/","section":"博客","summary":"\u003ch5 class=\"relative group\"\u003e方法一 \n    \u003cdiv id=\"方法一\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e6%96%b9%e6%b3%95%e4%b8%80\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h5\u003e\n\u003cp\u003e\u003cstrong\u003e1. os.system(\u0026lsquo;cmd\u0026rsquo;)方法\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e使用system方法可以返回运行cmd命令的状态返回值，同时会在终端输出运行结果，但无法将执行的结果保存起来\u003c/p\u003e\n\u003cp\u003esystem方法比较适用于执行单个命令、通常没有输出结果的情况\u003c/p\u003e\n\u003c/blockquote\u003e","title":"python脚本中调用shell的几种方法"},{"content":"","date":"6 January 2018","permalink":"/tags/shell/","section":"Tags","summary":"","title":"shell"},{"content":"背景 # 目前，由于公司多个相互隔离环境并存，各个环境都部署有容器管理平台，当有docker image更新的时候，就需要在每个环境都修改Dockerfile并执行docker build \u0026amp; docker push到公共镜像仓库的操作，为了减少重复性的工作，我就写了一个自动构建镜像的脚本，只需要在一个环境中准备好构建镜像的Dockerfile及依赖文件，执行脚本完成构建和推送，然后打包构建镜像的文件夹并传输到另一环境，执行脚本就可以自动完成\n脚本运行示例 # 构建脚本的目录结构 # 01.20171229文件夹 \u0026lt;\u0026mdash;\u0026gt; 要构建镜像的tag\n构建镜像步骤 # 创建目录 # $ cd /data/dcos $ mkdir -p buildimages/01.20171229 log $ cd buildimages/01.20171229 #创建各个需要build镜像的文件夹 $ mkdir centos7.2_tomcat7.0.72-all rhel7.2_tomcat7.0.63-jdk1.6.0_45 ... 修改configfile # $ vim /data/dcos/buildimages/configfile TargetRegistry:192.168.196.128:5000 --\u0026gt; 当前环境镜像仓库地址 Registry_user:admin\t--\u0026gt; 仓库登录用户名 Registry_passwd:admin --\u0026gt; 仓库认证密码 准备Dockerfile和依赖文件 # #示例 $ ls centos7.2_tomcat7.0.72-all Dockerfile startServer.sh 构建镜像 # $ bash /data/dcos/buildimages/build.sh 01.20171229 --\u0026gt;脚本要跟参数01.20171229 日志文件 # $ less /data/dcos/buildimages/log/01.20171229.log --\u0026gt;以tag命名的日志文件 Trying to pull repository 192.168.196.128:5000/admin/centos7.2_tomcat7.0.72-all ... 01.20171221: Pulling from 192.168.196.128:5000/admin/centos7.2_tomcat7.0.72-all 4f4fb700ef54: Pulling fs layer d8405e7a6b27: Pulling fs layer 02389d9a455d: Pulling fs layer ...中间省略... a3eef13e30fb: Pull complete 2833ecd114e0: Pull complete f8b56d99c9b0: Pull complete adb966dc304e: Pull complete Digest: sha256:861302f6da8e083b112d92fa153fc4dcde677f6d3e598ab22eea4289e312b08c Sending build context to Docker daemon 5.12 kB^M Step 1 : FROM 192.168.196.128:5000/admin/centos7.2_tomcat7.0.72-all:01.20171221 ---\u0026gt; 09ce979d3860 Step 2 : USER root ---\u0026gt; Running in 9a072cdbee5e ---\u0026gt; cc3b6b1d9bf1 Removing intermediate container 9a072cdbee5e Step 3 : COPY startServer.sh /app/bin/startServer.sh ---\u0026gt; 0d743c01403c Removing intermediate container fbdd0d775d09 Step 4 : RUN chmod +x /app/bin/startServer.sh \u0026amp;\u0026amp; chown -R dcos:docker /app/bin/startServer.sh ---\u0026gt; Running in 03b6f5d1d467 ---\u0026gt; 0f6055d1b9c5 Removing intermediate container 03b6f5d1d467 Step 5 : USER dcos ---\u0026gt; Running in 6ac2a787ae71 ---\u0026gt; ae33c0899e2a Removing intermediate container 6ac2a787ae71 Successfully built ae33c0899e2a The push refers to a repository [192.168.196.128:5000/admin/centos7.2_tomcat7.0.72-all] e623fa631602: Preparing 473647768f24: Preparing 96bd6e1cc68c: Preparing ...中间省略... 473647768f24: Pushed e623fa631602: Pushed 01.20171229: digest: sha256:1a5c012f2d7b4a627c5dc1aca22350d652676adf53adfde0d471943f4d390d4b size: 6798 压缩构建镜像的目录 # $ tar cvf 01.20171229.tar /data/dcos/buildimages/01.20171229 跨环境自动构建镜像 # 传输01.20171229.tar文件并解压 # $ cd /data/dcos/buildimages $ tar xf 01.20171229.tar 查看并确认configfile # #示例 $ vim /data/dcos/buildimages/configfile TargetRegistry:192.168.196.128:5000 --\u0026gt; 当前环境镜像仓库地址 Registry_user:admin Registry_passwd:admin 构建镜像 # $ bash /data/dcos/buildimages/build.sh 01.20171229 shell脚本 # #!/bin/bash #Date:2018-01-03 #version:3.0 #Author:Feixiang Fu #Description:Auto-build images based on configfile \u0026amp;\u0026amp; Auto-push images . /etc/init.d/functions [ -z \u0026#34;$*\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;usage: \u0026#34;buildim.sh VERSIONDIR\u0026#34; (VERSIONDIR is the target image tag)\u0026#34; \u0026amp;\u0026amp; exit 1 dir=/data/dcos/buildimages build_dir=$dir/$1 if [ ! -d $build_dir ]; then action \u0026#34;目标目录${1}不存在\u0026#34; false exit 1 fi addr=`cat $dir/configfile | awk -F: \u0026#39;/^TargetRegistry/{print $2\u0026#34;:\u0026#34;$3}\u0026#39;` user=`cat $dir/configfile | awk -F: \u0026#39;/^Registry_user/{print $2}\u0026#39;` passwd=`cat $dir/configfile | awk -F: \u0026#39;/^Registry_passwd/{print $2}\u0026#39;` cd $build_dir imagesdir_num=`ls -d $build_dir/*/|wc -l` if [ $imagesdir_num -eq 0 ] ; then action \u0026#34;${1}目录下没有要构建的镜像文件夹\u0026#34; false exit 1 fi docker login -u $user -p $passwd $addr \u0026amp;\u0026gt; /dev/null if [ $? -eq 0 ] ; then action \u0026#34;登陆镜像仓库${addr}成功\u0026#34; else action \u0026#34;登陆镜像仓库${addr}失败\u0026#34; false exit 1 fi declare -a localim buildim(){ cd $build_dir/$1 \u0026amp;\u0026gt; /dev/null if [ $? -ne 0 ] ;then echo \u0026#34;${1}为普通文件，略过\u0026#34; continue fi if [ -f ./Dockerfile ] ;then sed -i -r \u0026#34;s#(FROM\\s+)[^/]+(\\/.*)#\\1${addr}\\2#\u0026#34; Dockerfile baseimage=`cat Dockerfile |grep FROM|cut -d\u0026#34; \u0026#34; -f2` baseimage_num=`docker images|awk \u0026#39;{print $1\u0026#34;:\u0026#34;$2}\u0026#39;|grep \u0026#34;$baseimage\u0026#34;|wc -l` baseimage_name=`echo \u0026#34;$baseimage\u0026#34; | awk -F/ \u0026#39;{print $3}\u0026#39; ` if [ $baseimage_num -eq 0 ] ; then echo -e \u0026#34;正在拉取\\e[33m${baseimage_name}\\e[0m\u0026#34; docker pull $baseimage \u0026gt;\u0026gt; $dir/log/${2}.log 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ]; then action \u0026#34;拉取${baseimage_name}成功\u0026#34; localim[${#localim[*]}]=$baseimage else action \u0026#34;拉取${baseimage_name}失败\u0026#34; false continue fi fi path=`echo $baseimage |cut -d: -f1,2` newimage_name=`echo $baseimage_name|sed -r \u0026#34;s/([^:]+:).*/\\1$2/\u0026#34;` echo -e \u0026#34;正在构建\\e[33m${newimage_name}\\e[0m\u0026#34; docker build -t $path:$2 ./ \u0026gt;\u0026gt; $dir/log/${2}.log 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ] ;then action \u0026#34;${newimage_name}构建成功\u0026#34; localim[${#localim[*]}]=$path:$2 else action \u0026#34;${newimage_name}构建失败\u0026#34; false continue fi echo -e \u0026#34;正在推送\\e[33m${newimage_name}\\e[0m\u0026#34; docker push $path:$2 \u0026gt;\u0026gt; $dir/log/${2}.log 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ] ;then action \u0026#34;${newimage_name}推送成功\u0026#34; else action \u0026#34;${newimage_name}推送失败\u0026#34; false fi else action \u0026#34;${1}目录下无Dockerfile\u0026#34; false\tfi } for i in `ls $build_dir` ; do echo -e \u0026#34;\\e[32m-------------------------------------------------------------------\\e[0m\u0026#34; buildim $i $1 done echo -e \u0026#34;\\e[32m-------------------------------------------------------------------\\e[0m\u0026#34; echo \u0026#34;正在清理本地镜像文件\u0026#34; for j in `seq $((${#localim[*]}-1)) -1 0` ;do localim_name=$(echo \u0026#34;${localim[$j]}\u0026#34;| awk -F/ \u0026#39;{print $3}\u0026#39;) docker rmi ${localim[$j]} \u0026gt;\u0026gt; $dir/log/${1}.log 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ] ;then action \u0026#34;${localim_name}清理成功\u0026#34; else action \u0026#34;${localim_name}清理失败\u0026#34; false fi done echo -e \u0026#34;\\e[32m-------------------------------------------------------------------\\e[0m\u0026#34; [ -f $dir/log/${1}.log ] \u0026amp;\u0026amp; echo \u0026#34;logfile: $dir/log/${1}.log\u0026#34; ","date":"6 January 2018","permalink":"/2018/01/build-docker-image/","section":"博客","summary":"背景 # 目前，由于公司多个相互隔离环境并存，各个环境都部署有容器管理平台，当有docker image更新的时候，就需要在每个环境都修改Dockerfile并执行docker build \u0026amp; docker push到公共镜像仓库的操作，为了减少重复性的工作，我就写了一个自动构建镜像的脚本，只需要在一个环境中准备好构建镜像的Dockerfile及依赖文件，执行脚本完成构建和推送，然后打包构建镜像的文件夹并传输到另一环境，执行脚本就可以自动完成\n脚本运行示例 # 构建脚本的目录结构 # 01.20171229文件夹 \u0026lt;\u0026mdash;\u0026gt; 要构建镜像的tag\n构建镜像步骤 # 创建目录 # $ cd /data/dcos $ mkdir -p buildimages/01.20171229 log $ cd buildimages/01.20171229 #创建各个需要build镜像的文件夹 $ mkdir centos7.","title":"批量自动构建docker镜像的shell脚本"},{"content":"​ Tomcat 服务器是一个免费的开放源代码的Web 应用服务器, 是编译JSP\\Servlet的容器，常用来处理动态请求。nginx和apache HTTP服务器是静态解析，擅长处理HTML及图片等静态请求，处理静态页面效率远高于tomcat。使用nginx的动静分离机制，可以将静态请求分发至静态服务器(nginx或者apache)，而将动态请求分发至后端tomcat服务器处理，从而提高服务器的并发处理性能。tomcat支持HTTP和AJP两种协议的连接器，AJP协议比HTTP更稳定和更快，但是nginx仅支持HTTP协议，所以本文使用nginx做负载均衡，在后端web服务器上配置apache+tomcat服务并使用ajp协议，提高响应速度。\n对于后端tomcat cluster的session会话管理，本文将使用MSM\u0026ndash;Memcached_Session_Manager搭建 session server cluster, 实现session会话保持和高可用(session共享)。Memcached是一款开源、高性能、分布式内存对象缓存系统，可应用各种需要缓存的场景，其主要目的是通过降低对Database的访问来加速web应用程序。它是一个基于内存的“键值对”存储，用于存储数据库调用、API调用或页面引用结果的直接数据，如字符串、对象等。但是，tomcat与memcached的结合并不是为了加速获取mysql数据的，而是仅仅把tomcat自己与客户端一侧维持的会话保存到memcached中，它与用户请求无关，是服务端主动记录client身份信息或者活动性的数据并存在memcached中，从而加速客户端会话访问，实现动态站点加速。\n​\t本文集群架构如下图： IP 地址分配:\nnginx server : 192.168.196.130 web server 1 : 192.168.196.129 web server 2: 192.168.196.132 session server 1: 192.168.196.131 session server 2: 192.168.196.133 配置nginx LB server # $ yum install -y nginx $ vim /etc/nginx/conf.d/tomcat.conf upstream appsrvs { server 192.168.196.132:80; server 192.168.196.129:80; } server { listen 80 default_server; server_name www.fufeixiang.com; index index.jsp index.html; location / { proxy_pass http://appsrvs/; } } $ service nginx start 本文重点是session server cluster的实现，这里只是简单配置nginx做负载均衡，没有做动静分离\n配置apache+tomcat web server # web server1 apache的配置 $ yum install -y httpd $ vim /etc/httpd/conf.d/vhost_tom_ajp.conf \u0026lt;Virtualhost *:80\u0026gt; ServerName www.vhost1.com ProxyRequests Off ProxyVia On \u0026lt;--响应报文中添加Proxyvia信息 ProxyPreserveHost On \u0026lt;--向后端转发，保留client主机名 \u0026lt;Proxy *\u0026gt; Require all granted \u0026lt;/Proxy\u0026gt; ProxyPass / ajp://192.168.196.129:8009/ ProxyPassReverse / ajp://192.168.196.129:8009/ \u0026lt;--使用ajp协议将请求转发至本机的tomcat \u0026lt;location /\u0026gt; Require all granted \u0026lt;/location\u0026gt; \u0026lt;/Virtualhost\u0026gt; tomcat server1 的配置 安装包：java-1.8.0-openjdk-devel; tomcat-admin-webapps; tomcat-webapps; tomcat-docs-webapp ​ 对于session集群的共享问题，参考GitHub项目https://github.com/magro/memcached-session-manager/wiki/SetupAndConfiguration ， 使用memcached+javolution-serializer(可序列化工具)的解决方案。依据其配置说明：\n添加tomcat所依赖的Memcached_Session_Manage的库文件：\njavolution-5.4.3.1.jar memcached-session-manager-tc7-2.1.1.jar memcached-session-manager-2.1.1.jar msm-javolution-serializer-2.1.1.jar spymemcached-2.11.1.jar\n$ mv javolution-5.4.3.1.jar memcached-session-manager-tc7-2.1.1.jar memcached-session-manager-2.1.1.jar msm-javolution-serializer-2.1.1.jar spymemcached-2.11.1.jar /usr/share/java/tomcat/\n修改tomcat主配置文件/etc/tomcat/server.xml $ vim /etc/tomcat/server.xml \u0026lt;Engine name=\u0026#34;Catalina\u0026#34; defaultHost=\u0026#34;localhost\u0026#34; jvmRoute=\u0026#34;tomcatA\u0026#34;\u0026gt; \u0026lt;--添加节点唯一标示 \u0026lt;Host name=\u0026#34;localhost\u0026#34; appBase=\u0026#34;webapps\u0026#34; unpackWARs=\u0026#34;true\u0026#34; autoDeploy=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;Context path=\u0026#34;/test\u0026#34; docBase=\u0026#34;/data/webapps/ROOT\u0026#34; reloadable=\u0026#34;true\u0026#34; \u0026gt; \u0026lt;--应用程序路径别名 \u0026lt;Manager className=\u0026#34;de.javakaffee.web.msm.MemcachedBackupSessionManager\u0026#34; memcachedNodes=\u0026#34;n1:192.168.196.131:11211,n2:192.168.196.133:11211\u0026#34; \u0026lt;--定义session server 节点 failoverNodes=\u0026#34;n2\u0026#34;\t\u0026lt;--n2节点为backup server requestUriIgnorePattern=\u0026#34;.*\\.(ico|png|gif|jpg|css|js)$\u0026#34; transcoderFactoryClass=\u0026#34;de.javakaffee.web.msm.serializer.javolution.JavolutionTranscoderFactory\u0026#34; /\u0026gt; \u0026lt;/Context\u0026gt; \u0026lt;Valve className=\u0026#34;org.apache.catalina.valves.AccessLogValve\u0026#34; directory=\u0026#34;logs\u0026#34; prefix=\u0026#34;localhost_access_log.\u0026#34; suffix=\u0026#34;.txt\u0026#34; pattern=\u0026#34;%h %l %u %t \u0026amp;quot;%r\u0026amp;quot; %s %b\u0026#34; /\u0026gt; \u0026lt;/Host\u0026gt; 创建测试程序 $ mkdir /data/webapps/ROOT/{classes,lib,WEB-INF} $ vim /data/webapps/ROOT/index.jsp \u0026lt;%@ page language=\u0026#34;java\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;TomcatA\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt;TomcatA.sflying.com\u0026lt;/font\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;table align=\u0026#34;centre\u0026#34; border=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Session ID\u0026lt;/td\u0026gt; \u0026lt;% session.setAttribute(\u0026#34;sflying.com\u0026#34;,\u0026#34;sflying.com\u0026#34;); %\u0026gt; \u0026lt;td\u0026gt;\u0026lt;%= session.getId() %\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Created on\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;%= session.getCreationTime() %\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; web server2 apache的配置 $ yum install -y httpd $ vim /etc/httpd/conf.d/vhost_tom_ajp.conf \u0026lt;Virtualhost *:80\u0026gt; ServerName www.vhost2.com ProxyRequests Off ProxyVia On ProxyPreserveHost On \u0026lt;Proxy *\u0026gt; Require all granted \u0026lt;/Proxy\u0026gt; ProxyPass / ajp://192.168.196.132:8009/ ProxyPassReverse / ajp://192.168.196.132:8009/ \u0026lt;location /\u0026gt; Require all granted \u0026lt;/location\u0026gt; \u0026lt;/Virtualhost\u0026gt; tomcat server2 的配置 参照tomcat server1的配置，下载依赖类库，并修改配置文件以下几项\nserver.xml \u0026ndash;\u0026gt; \u0026lt;Engine name=\u0026quot;Catalina\u0026quot; defaultHost=\u0026quot;localhost\u0026quot; jvmRoute=\u0026quot;tomcatB\u0026quot;\u0026gt; /data/webapps/ROOT/index.jsp :\n\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;TomcatA\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;font color=\u0026quot;red\u0026quot;\u0026gt;TomcatA.sflying.com\u0026lt;/font\u0026gt;\u0026lt;/h1\u0026gt; 配置memcached session server # $ yum install -y memcached $ vim /etc/sysconfig/memcached PORT=\u0026#34;11211\u0026#34; USER=\u0026#34;nobody\u0026#34; MAXCONN=\u0026#34;1024\u0026#34;\t\u0026lt;-- 最大并发数\tCACHESIZE=\u0026#34;64\u0026#34;\t\u0026lt;-- 根据实际情况调整，eg：2048M\tOPTIONS=\u0026#34;\u0026#34; 测试会话保持 # 启动服务，在浏览器中访问 刷新页面 从测试结果可以看出，session ID 与 session 储存节点位置都保持不变，至此基于session server cluster的会话保持已经实现。\n","date":"19 September 2017","permalink":"/2017/09/nginx-tomcat-seesion-cluster/","section":"博客","summary":"​ Tomcat 服务器是一个免费的开放源代码的Web 应用服务器, 是编译JSP\\Servlet的容器，常用来处理动态请求。nginx和apache HTTP服务器是静态解析，擅长处理HTML及图片等静态请求，处理静态页面效率远高于tomcat。使用nginx的动静分离机制，可以将静态请求分发至静态服务器(nginx或者apache)，而将动态请求分发至后端tomcat服务器处理，从而提高服务器的并发处理性能。tomcat支持HTTP和AJP两种协议的连接器，AJP协议比HTTP更稳定和更快，但是nginx仅支持HTTP协议，所以本文使用nginx做负载均衡，在后端web服务器上配置apache+tomcat服务并使用ajp协议，提高响应速度。\n对于后端tomcat cluster的session会话管理，本文将使用MSM\u0026ndash;Memcached_Session_Manager搭建 session server cluster, 实现session会话保持和高可用(session共享)。Memcached是一款开源、高性能、分布式内存对象缓存系统，可应用各种需要缓存的场景，其主要目的是通过降低对Database的访问来加速web应用程序。它是一个基于内存的“键值对”存储，用于存储数据库调用、API调用或页面引用结果的直接数据，如字符串、对象等。但是，tomcat与memcached的结合并不是为了加速获取mysql数据的，而是仅仅把tomcat自己与客户端一侧维持的会话保存到memcached中，它与用户请求无关，是服务端主动记录client身份信息或者活动性的数据并存在memcached中，从而加速客户端会话访问，实现动态站点加速。\n​\t本文集群架构如下图： IP 地址分配:\nnginx server : 192.168.196.130 web server 1 : 192.168.196.129 web server 2: 192.168.196.132 session server 1: 192.","title":"nginx负载均衡apache+tomcat集群及session server cluster的实现"},{"content":"","date":"19 September 2017","permalink":"/tags/tomcat/","section":"Tags","summary":"","title":"tomcat"},{"content":"","date":"10 September 2017","permalink":"/tags/varnish/","section":"Tags","summary":"","title":"varnish"},{"content":"Varnish简介 # Varnish是一款高性能且开源的反向代理服务器和 HTTP 加速器，支持采用基于linux系统内存的缓存服务器。\nvarnish的系统架构如下图：\nvarnish是主要运行两个进程：Management进程和Child进程(也叫Cache进程)，基于 epoll机制的单进程多线程架构 Management进程类似于nginx中的master,主要实现 编译VCL并应用新配置(通知子进程);监控varnish格子进程运行状态，初始化varnish; 提供一个CLI接口(命令行接口)指挥管理进程 等。Management进程会每隔几秒钟探测一下Child进程以判断其是否正常运行，如果在指定的时长内未得到Child进程的回应，Management将会重启此Child进程。 Child进程包含多种类型的线程，常见的如： Acceptor线程：接收新的连接请求并响应 Worker线程：child进程会为每个会话启动一个worker线程，因此，在高并发的场景中可能会出现数百个worker线程甚至更多 Expiry线程：从缓存中清理过期内容 varnish通过使用VCL(Varnish Configuration Language )配置缓存系统的缓存策略，VCL编译器调用C编译器,C编译器编译配置文件为二进制文件并连接至child进程。 VCL语法格式\n(1)//、#或/* comment */用于注释 (2)sub $name 定义函数 (3)不支持循环，有内置变量 (4)使用终止语句，没有返回值 (5)域专用 (6)操作符：=(赋值)、==(等值比较)、~(模式匹配)、!(取反)、\u0026amp;\u0026amp;(逻辑与)、||(逻辑或)\n三类主要语法：\nsub subroutine { ... } if CONDITION { ... } else { ... } return(), hash_data() Varnish状态引擎(state engine) varnish内部有几个所谓的状态(state)，在这些状态上可以附加通过VCL定义的策略以完成相应的缓存处理机制，因此VCL也经常被称作“域专用”语言或状态引擎，“域专用”指的是有些数据仅出现于特定的状态中。在VCL状态引擎中，状态之间具有相关性，但彼此间互相隔离，每个引擎使用return(x)来退出当前状态并联至哪个下一级引擎；每个状态引擎对应于vcl文件(default.vcl)中的一个配置段，即为subroutine。例如：\nvcl_recv \u0026ndash; 处理客户端请求的第一步，分析是否为标准HTTP请求，是否为可缓存资源等\nvcl_synth \u0026ndash; 用于合成响应报文\nvcl_purge \u0026ndash; 用于删除某条缓存记录\nvcl_backend_fetch \u0026ndash; 根据服务器端的响应作出缓存决策（eg:是否先缓存再响应给客户端）\nvcl_pass \u0026ndash; 不能缓存的请求跳转到vcl_backend_fetch状态\n两个特殊的引擎： vcl_init \u0026ndash; 在处理任何请求之前要执行的vcl代码：主要用于初始化加载额外模块；vcl配置文件中要先于vcl_recv定义 vcl_fini \u0026ndash; 所有的请求都已经结束，在vcl配置被丢弃时调用；主要用于清理VMODs；\n变量 在vcl配置文件中指定缓存规则时，可以引用变量，包括内建变量与自定义变量\n内建变量： ​ req.*：request，表示由客户端发来的请求报文相关\n​ req.http.*\n​ req.http.User-Agent 客户端设备类型\n​ req.http.Referer 超链接跳转地址\n​ bereq.*：由varnish发往BE主机请求报文相关\n​ bereq.http.*\n​ beresp.*：由BE主机响应给varnish的响应报文相关\n​ beresp.http.*\n​ resp.*：由varnish响应给client相关\n​ obj.*：存储在缓存空间中的缓存对象的属性；只读\n​ server.*\n​ server.ip：varnish主机的IP\n​ server.hostname：varnish主机的Hostname\n​ client.*\n​ client.ip：发请求至varnish主机的客户端IP\n自定义变量 ​ set/unset\n详情请参考https://varnish-cache.org/docs/4.0/\nvarnish安装与配置 # 使用yum源安装varnish$ yum install -y varnish\n配置文件：/etc/varnish/varnish.params \u0026ndash;\u0026gt; 配置varnish服务进程的工作特性\nVARNISH_LISTEN_PORT=6081 \u0026ndash; varnish 服务进程监听的端口\nVARNISH_ADMIN_LISTEN_ADDRESS=127.0.0.1\nVARNISH_ADMIN_LISTEN_PORT=6082\n​ varnishadm命令行工具连入CLI接口的地址与端口\nVARNISH_STORAGE=\u0026ldquo;malloc,1024M\u0026rdquo;\n​ varnish的缓存存储机制( Storage Types)：\n​ malloc[,size] \u0026mdash; 内存存储，[,size]用于定义空间大小；重启后所有缓存项失\n​ file[,path[,size[,granularity]]] \u0026mdash; 磁盘文件存储，黑盒；重启后所有缓存项失效\n​ persistent,path,size \u0026mdash; 文件存储，黑盒；重启后所有缓存项有效；实验阶段\nDAEMON_OPTS=\u0026quot;-P thread_pools=2 -p thread_pool_max=1000 thread_pool_timeout=300\u0026quot;\n​ 指定varnish的运行时参数thread、timer相关\n配置文件：/etc/varnish/default.vcl \u0026ndash;\u0026gt;配置各Child/Cache线程的缓存策略 结合一下示例来熟悉vcl配置：\n使用内建变量obj.hits(用于保存某缓存项的从缓存中命中的次数)在响应报文中添加缓存标记 $ vim /etc/varnish/default.vcl backend default { \u0026lt;-- 定义后端服务器监听的IP与端口 .host = \u0026#34;192.168.196.132\u0026#34;; .port = \u0026#34;80\u0026#34;; } sub vcl_deliver { if (obj.hits\u0026gt;0) { set resp.http.X-Cache = \u0026#34;HIT via \u0026#34; + server.ip; } else { set resp.http.X-Cache = \u0026#34;MISS from \u0026#34; + server.ip; } } $ varnish_reload_vcl \u0026lt;--自动加载cmd Loading vcl from /etc/varnish/default.vcl Current running config name is Using new config name reload_2017-09-07T16:02:29 VCL compiled. VCL \u0026#39;reload_2017-09-07T16:02:29\u0026#39; now active available 0 boot active 0 reload_2017-09-07T16:02:29 \u0026lt;--当期使用的版本 Done 浏览器中访问http://192.168.196.133:6081，刷新之后，从响应报文的X-Cache字段可以看出请求的资源是否被缓存\n强制对某类资源的请求不检查缓存（eg:用户私有数据） $ vim /etc/varnish/default.vcl vcl_recv { if (req.url ~ \u0026#34;(?i)^/(login|admin)\u0026#34;) { \u0026lt;-- /login与/admin目录下均不能缓存 return(pass); } } $ varnishadm -S /etc/varnish/secret -T 127.0.0.1:6082 \u0026lt;-- 使用varnishadm工具手动reload配置文件 vcl.list 200 available 0 boot active 0 reload_2017-09-07T16:02:29 vcl.load test1 default.vcl 200 VCL compiled. vcl.use test1 200 VCL \u0026#39;test1\u0026#39; now active 浏览器中访问http://192.168.196.133:6081/login，无论如何刷新，X-Cache字段均显示\u0026rsquo;MISS from 192.168.196.133'\n缓存对象的修剪 \u0026ndash;\u0026gt; 在缓存有限期内，后端服务端资源更新时就需要手动删除缓存 purge 一次删除一条缓存记录 $ vim /etc/varnish/default.vcl acl purgers { \u0026lt;--只允许acl内的IP能使用purge方法 \u0026#34;127.0.0.0\u0026#34;/8; \u0026lt;--必须以127.0.0.1为client发出请求才符合此acl \u0026#34;192.168.196.130\u0026#34;; } vcl_recv { if (req.method == \u0026#34;PURGE\u0026#34;) { \u0026lt;--自定义请求方法PURGE时转到vcl_purge if (!client.ip ~ purgers) { return(synth(405,\u0026#34;Purging not allowed for \u0026#34; + client.ip)); } return(purge); } } reload之后在客户端192.168.196.130上使用curl命令测试\n$ curl -I http://192.168.196.133:6081 HTTP/1.1 200 OK Server: nginx/1.10.2 Date: Thu, 07 Sep 2017 12:57:43 GMT Content-Type: text/html Content-Length: 17 Last-Modified: Tue, 05 Sep 2017 07:38:56 GMT ETag: \u0026#34;59ae5490-11\u0026#34; X-Varnish: 32819 32817 Age: 2 Via: 1.1 varnish-v4 X-Cache: HIT via 192.168.196.133 \u0026lt;--已缓存 Connection: keep-alive $ curl -X PURGE http://192.168.196.133:6081 \u0026lt;--定义请求方法为PURGE \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;200 Purged\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Error 200 Purged\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Purged\u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt;Guru Meditation:\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;XID: 131101\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;Varnish cache server\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@Centos7 varnish]# curl -I http://192.168.196.133:6081 HTTP/1.1 200 OK Server: nginx/1.10.2 Date: Thu, 07 Sep 2017 13:01:10 GMT Content-Type: text/html Content-Length: 17 Last-Modified: Tue, 05 Sep 2017 07:38:56 GMT ETag: \u0026#34;59ae5490-11\u0026#34; X-Varnish: 32821 Age: 0 Via: 1.1 varnish-v4 X-Cache: MISS from 192.168.196.133 \u0026lt;--未到过期时间，已经没有缓存 Connection: keep-alive $ curl -X PURGE 192.168.196.133:6081 \u0026lt;--客户端192.168.196.132中访问 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;405 Purging not allowed for 192.168.196.132\u0026lt;/title\u0026gt; \u0026lt;--132不在acl定义中，没有权限 \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Error 405 Purging not allowed for 192.168.196.132\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Purging not allowed for 192.168.196.132\u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt;Guru Meditation:\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;XID: 32833\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;Varnish cache server\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Banning\n使用varnishadm工具删除 ​ ban $ varnishadm -S /etc/varnish/secret -T 127.0.0.1:6082 ban req.url ~ ^/images \u0026lt;-- 将images开头的请求的缓存，全部删除 在配置文件中定义，使用ban()函数 $ vim /etc/varnish/default.vcl acl bans { \u0026#34;127.0.0.0\u0026#34;/8; \u0026#34;192.168.196.130\u0026#34;; } if (req.method == \u0026#34;BAN\u0026#34; ){ if (!client.ip ~ bans){ return(synth(405,\u0026#34;Ban not allowed for \u0026#34; + client.ip)); } #将请求的资源删除。请求什么，删什么 ban(\u0026#34;req.http.host == \u0026#34; + req.http.host + \u0026#34; \u0026amp;\u0026amp; req.url == \u0026#34; + req.url); return(synth(200,\u0026#34;Ban succeed\u0026#34;)); } reload之后在客户端192.168.196.130上使用curl命令测试\n$ curl -I 192.168.196.133:6081 HTTP/1.1 200 OK Server: nginx/1.10.2 Date: Thu, 07 Sep 2017 13:41:13 GMT Content-Type: text/html Content-Length: 17 Last-Modified: Tue, 05 Sep 2017 07:38:56 GMT ETag: \u0026#34;59ae5490-11\u0026#34; X-Varnish: 131127 32840 Age: 2 Via: 1.1 varnish-v4 X-Cache: HIT via 192.168.196.133 $ curl -X BAN 192.168.196.133:6081 \u0026lt;--定义请求方法为BAN \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;200 Ban succeed\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Error 200 Ban succeed\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Ban succeed\u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt;Guru Meditation:\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;XID: 131129\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;Varnish cache server\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ curl -X BAN 192.168.196.133:6081 \u0026lt;--客户端192.168.196.132中访问 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;405 Ban not allowed for 192.168.196.132\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Error 405 Ban not allowed for 192.168.196.132\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Ban not allowed for 192.168.196.132\u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt;Guru Meditation:\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;XID: 131144\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;Varnish cache server\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 调度多个后端主机的设定 varnish服务器的设定 $ vim /etc/varnish/default.vcl import directors; \u0026lt;--需要先导入directors模块 backend imgsrv1 { .host = \u0026#34;192.168.196.129\u0026#34;; .port = \u0026#34;80\u0026#34;; } backend imgsrv2 { .host = \u0026#34;192.168.196.130\u0026#34;; .port = \u0026#34;80\u0026#34;; } backend default { .host = \u0026#34;192.168.196.132\u0026#34;; .port = \u0026#34;80\u0026#34;; } sub vcl_init { \u0026lt;-- 初始化directors设定,需要在vcl_recv之前 new imgsrvs = directors.round_robin(); \u0026lt;-- 自定义后端集群名字、调度算法 imgsrvs.add_backend(imgsrv1); \u0026lt;-- 指定要调度的后端主机 imgsrvs.add_backend(imgsrv2); } sub vcl_recv { if (req.url ~ \u0026#34;(?i)\\.(jpg|png|txt)$\u0026#34;){ set req.backend_hint = imgsrvs.backend(); \u0026lt;-- 对jpg/png/txt访问调度到后端imgsrvs集群 }else{ set req.backend_hint = default; } if (req.url ~ \u0026#34;(?i)\\.txt$\u0026#34;){ \u0026lt;-- 为看出轮询效果，不缓存.txt文件 return(pass); } } 在imgsrv1/imgsrv2根目录上分别创建test.txt\n192.168.196.129 \u0026ndash;\u0026gt; echo \u0026ldquo;Backend imgsrv1\u0026rdquo; \u0026gt; /usr/share/nginx/html/test.txt\n192.168.196.130 \u0026ndash;\u0026gt; echo \u0026ldquo;Backend imgsrv2\u0026rdquo; \u0026gt; /usr/share/nginx/html/test.txt\n为了看出后端服务器轮询的调度效果，在vcl配置文件中定义了不缓存.txt结尾的文件，使用curl命令访问\n$ while : ; do curl 192.168.196.133:6081/test.txt; sleep 1; done Backend imgsrv1 Backend imgsrv2 Backend imgsrv1 Backend imgsrv2 Backend imgsrv1 后端主机健康检测设置 方法一：直接在Backend 下定义.probe 健康状态检测方法\nbackend BE_NAME { .host = .port = .probe = { .url= \u0026lt;--检测时要请求的URL，默认为”/\u0026#34; .timeout= \u0026lt;--超时时长 .interval= \u0026lt;--检测频率 .window= \u0026lt;--基于最近的多少次检查来判断其健康状态 .threshold= \u0026lt;--最近.window次检查中多少次成功才算健康 } } 方法二：定义probe,然后在各个backend中调用\nprobe PB_NAME { } \u0026lt;---定义一个probe 再调用 backend NAME = { .probe = PB_NAME; ... } 在各个后端主机Web服务根目录创建.healthchk.html文件，用作健康检测 修改vcl配置文件 $ vim /etc/varnish/default.vcl probe healthchk{ .url = \u0026#34;/.healthchk.html\u0026#34;; .timeout = 1s; .interval = 2s; .window = 5; .threshold =4; } backend imgsrv1 { .host = \u0026#34;192.168.196.129\u0026#34;; .port = \u0026#34;80\u0026#34;; .probe = healthchk; } backend imgsrv2 { .host = \u0026#34;192.168.196.130\u0026#34;; .port = \u0026#34;80\u0026#34;; .probe = healthchk; } backend default { .host = \u0026#34;192.168.196.132\u0026#34;; .port = \u0026#34;80\u0026#34;; .probe = healthchk; } 在reload之后使用varnishadm工具查看backend状态 $ varnishadm -S secret -T 127.0.0.1:6082 backend.list 200 Backend name Refs Admin Probe imgsrv1(192.168.196.129,,80) 1 probe Healthy 5/5 imgsrv2(192.168.196.130,,80) 1 probe Healthy 5/5 default(192.168.196.132,,80) 1 probe Healthy 5/5 手动停止imgsrv1的nginx web服务，curl命令测试 $ while : ; do curl 192.168.196.133:6081/test.txt;sleep 1; done Backend imgsrv1 Backend imgsrv2 Backend imgsrv1 Backend imgsrv2 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;503 Backend fetch failed\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Error 503 Backend fetch failed\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Backend fetch failed\u0026lt;/p\u0026gt; \u0026lt;h3\u0026gt;Guru Meditation:\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;XID: 32828\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;Varnish cache server\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Backend imgsrv2 Backend imgsrv2 Backend imgsrv2 varnishadm工具查看backend状态, imgsrv1显示为Sick；重启imgsrv1 nginx服务即可恢复 $ varnishadm -S secret -T 127.0.0.1:6082 backend.list 200 Backend name Refs Admin Probe imgsrv1(192.168.196.129,,80) 1 probe Sick 0/5 imgsrv2(192.168.196.130,,80) 1 probe Healthy 5/5 default(192.168.196.132,,80) 1 probe Healthy 5/5 手动设定BE主机的状态 sick \u0026ndash; 手动标记为down healthy \u0026ndash; 管理up, 永久健康 auto \u0026ndash; probe auto $ varnishadm -S secret -T 127.0.0.1:6082 backend.set_health imgsrv1 sick 200 backend.list 200 Backend name Refs Admin Probe imgsrv1(192.168.196.129,,80) 1 sick Healthy 5/5 imgsrv2(192.168.196.130,,80) 1 probe Healthy 5/5 default(192.168.196.132,,80) 1 probe Healthy 5/5 后端主机其它属性设置 backend BE_NAME { ... .connect_timeout = 0.5s; \u0026lt;--注意是TCP连接，不是http连接 .first_byte_timeout = 20s; \u0026lt;--后端主机响应首字节传输超时时间 .between_bytes_timeout = 5s; \u0026lt;--字节传输中间间隔时长 .max_connections = 50; \u0026lt;--最大并发连接数 } varnish日志 # varnishlog \u0026ndash;Display Varnish logs 实时获取varnish服务器详细的接收请求及响应日志，常用于debug\nvarnishncsa - Display Varnish logs in Apache / NCSA combined log format ncsa格式的日志,使用命令行cmd实时获取\n$ varnishncsa 192.168.196.130 - - [09/Sep/2017:13:05:10 +0800] \u0026#34;GET http://192.168.196.133:6081/ HTTP/1.1\u0026#34; 200 25 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; 或者使此服务工作于后台记录日志\n$ /usr/bin/varnishncsa -a -w /var/log/varnish/varnishncsa.log -D 在Centos7中varnishncsa为单独的服务，启动即可$systemctl start varnishncsa\n","date":"10 September 2017","permalink":"/2017/09/varnish/","section":"博客","summary":"Varnish简介 # Varnish是一款高性能且开源的反向代理服务器和 HTTP 加速器，支持采用基于linux系统内存的缓存服务器。\nvarnish的系统架构如下图：\nvarnish是主要运行两个进程：Management进程和Child进程(也叫Cache进程)，基于 epoll机制的单进程多线程架构 Management进程类似于nginx中的master,主要实现 编译VCL并应用新配置(通知子进程);监控varnish格子进程运行状态，初始化varnish; 提供一个CLI接口(命令行接口)指挥管理进程 等。Management进程会每隔几秒钟探测一下Child进程以判断其是否正常运行，如果在指定的时长内未得到Child进程的回应，Management将会重启此Child进程。 Child进程包含多种类型的线程，常见的如： Acceptor线程：接收新的连接请求并响应 Worker线程：child进程会为每个会话启动一个worker线程，因此，在高并发的场景中可能会出现数百个worker线程甚至更多 Expiry线程：从缓存中清理过期内容 varnish通过使用VCL(Varnish Configuration Language )配置缓存系统的缓存策略，VCL编译器调用C编译器,C编译器编译配置文件为二进制文件并连接至child进程。 VCL语法格式\n(1)//、#或/* comment */用于注释 (2)sub $name 定义函数 (3)不支持循环，有内置变量 (4)使用终止语句，没有返回值 (5)域专用 (6)操作符：=(赋值)、==(等值比较)、~(模式匹配)、!","title":"varnish缓存服务器的配置"},{"content":"","date":"7 August 2017","permalink":"/tags/dns/","section":"Tags","summary":"","title":"DNS"},{"content":"\u0026ldquo;就近\u0026quot;解析的优点 # ​ 假设有一个面向全网络的web服务，并且服务器都集中在一个地方，而访问请求是来自全国各地的，网络传输线路的长短势必会影响访问响应质量和速度。因此，为了提高客户端访问速度、提供容错能力等，公司可能会按区域部署服务器，比如说在北京、杭州、广州等地，每台服务器刚好能优先响应离自己最近的访问请求，从而提高服务器响应速度。\n​ 实现\u0026quot;就近\u0026quot;解析的两种方法：\nweb调度器 仅提供请求转发功能，转发给区域web服务器，把离请求主机\u0026quot;最近\u0026quot;的服务器IP返回给dns服务器，然后解析\ndns服务器 由dns服务器根据源地址直接解析到离请求主机\u0026quot;最近\u0026quot;的web服务器,本文将详细介绍此方法的实现\ndns服务器据源地址解析的实现 # 实验环境 # 系统：CentOS 7 dns服务程序：bind dns服务器IP: 192.168.196.168 定义acl和启用view # acl: 把一个或多个地址归并为一个集合，并通过一个统一的名称调用 只能先定义，后使用,一般定义在配置文件中， 处于options的前面\n​ 格式：\nacl acl_name { ip; PREFIX; …… }; view:视图，一个bind服务器可定义多个view，每个view中可定义一个或多个zone 每个view用来匹配一组客户端 客户端请求到达时，是自上而下检查每个view所服务的客户端列表\n​ 格式：\nview shanghaiview{ match-clients { shanghai; }; include \u0026#34;/etc/named.shanghaiview.zones\u0026#34;; \u0026lt;--指定对应区域配置文件 }; 修改配置文件/etc/named.conf\nacl beijing { 192.168.196.130 ;}; \u0026lt;--假设beijing区域只有IP.130 acl shanghai { 192.168.196.128 ;}; \u0026lt;--假设shanghai区域只有IP.128 acl other { any;}; \u0026lt;--假设其它包含在other区域 options{ listen-on port 53 { localhost; }; \u0026lt;--localhost为关键字，代表本机所有IP allow-query { any ; }; \u0026lt;--可提供外部服务，允许其它主机查询 }; 创建view对应的区域配置文件 # zone \u0026ldquo;.\u0026rdquo; 把name.conf文件定义的根区移动到/etc/named.rfc1912.zones中\n一旦启用了 view，所有的zone都只能定义在view中\nzone \u0026#34;.\u0026#34; IN { type hint; file \u0026#34;named.ca\u0026#34;; }; 创建/etc/named.beijingview.zones zone \u0026#34;ffu.com\u0026#34; IN { type master; file \u0026#34;ffu.com.zone.bj\u0026#34;; \u0026lt;--对应各view的区域库文件名 }; 创建/etc/named.shanghaiview.zones zone \u0026#34;ffu.com\u0026#34; IN { type master; file \u0026#34;ffu.com.zone.sh\u0026#34;; }; 创建各view的区域库文件 # /var/named/ffu.com.zone.bj\n$TTL 86400\t; 1 day @\tIN SOA\tdns1.ffu.com. dns1admin.ffu.com. ( 1 ; serial 86400 ; refresh (1 day) 3600 ; retry (1 hour) 604800 ; expire (1 week) 10800 ; minimum (3 hours) ) NS\tdns1.ffu.com. A\t1.1.1.1 dns1\tA\t192.168.196.168 test\tA\t8.8.8.8 websrv\tA\t10.10.10.10 www\tCNAME\twebsrv /var/named/ffu.com.zone.sh\n$TTL 86400\t; 1 day @\tIN SOA\tdns1.ffu.com. dns1admin.ffu.com. ( 1 ; serial 86400 ; refresh (1 day) 3600 ; retry (1 hour) 604800 ; expire (1 week) 10800 ; minimum (3 hours) ) NS\tdns1.ffu.com. A\t2.2.2.2 dns1\tA\t192.168.196.168 test\tA\t9.9.9.9 websrv\tA\t13.13.13.13 www\tCNAME\twebsrv /var/named/ffu.com.zone\n$ORIGIN . $TTL 86400\t; 1 day ffu.com\tIN SOA\tdns1.ffu.com. dns1admin.ffu.com. ( 2 ; serial 86400 ; refresh (1 day) 3600 ; retry (1 hour) 604800 ; expire (1 week) 10800 ; minimum (3 hours) ) NS\tdns1.ffu.com. A\t1.1.1.1 $ORIGIN ffu.com. dns1\tA\t192.168.196.168 test\tA\t8.8.8.8 websrv\tA\t1.1.1.1 www\tCNAME\twebsrv 访问测试 # 192.168.196.130 主机\n$ dig www.ffu.com @192.168.196.168 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.4-RedHat-9.9.4-37.el7 \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.ffu.com @192.168.196.168 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 60401 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;www.ffu.com. IN A ;; ANSWER SECTION: www.ffu.com. 86400 IN CNAME websrv.ffu.com. websrv.ffu.com. 86400 IN A 10.10.10.10 \u0026lt;-- .130属于beijingview,解析到10.10.10.10服务器 ;; AUTHORITY SECTION: ffu.com. 86400 IN NS dns1.ffu.com. ;; ADDITIONAL SECTION: dns1.ffu.com. 86400 IN A 192.168.196.168 ;; Query time: 2 msec ;; SERVER: 192.168.196.168#53(192.168.196.168) ;; WHEN: Fri Jul 28 09:55:47 CST 2017 ;; MSG SIZE rcvd: 112 192.168.196.128 主机\n$ dig test.ffu.com @192.168.196.168 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.8.2rc1-RedHat-9.8.2-0.62.rc1.el6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; test.ffu.com @192.168.196.168 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 57430 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 1, ADDITIONAL: 1 ;; QUESTION SECTION: ;test.ffu.com. IN A ;; ANSWER SECTION: test.ffu.com. 86400 IN A 9.9.9.9 \u0026lt;-- .128属于shanghaiview,解析到9.9.9.9服务器 ;; AUTHORITY SECTION: ffu.com. 86400 IN NS dns1.ffu.com. ;; ADDITIONAL SECTION: dns1.ffu.com. 86400 IN A 192.168.196.168 ;; Query time: 2 msec ;; SERVER: 192.168.196.168#53(192.168.196.168) ;; WHEN: Fri Jul 28 00:05:52 2017 ;; MSG SIZE rcvd: 81 192.168.196.155 主机\n$dig www.ffu.com @192.168.196.168 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.8.2rc1-RedHat-9.8.2-0.62.rc1.el6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.ffu.com @192.168.196.168 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 62993 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 2 ;; QUESTION SECTION: ;www.ffu.com. IN A ;; ANSWER SECTION: www.ffu.com. 86400 IN CNAME websrv.ffu.com. websrv.ffu.com. 86400 IN A 1.1.1.1 \u0026lt;-- .155属于otherview,解析到1.1.1.1服务器 ;; AUTHORITY SECTION: ffu.com. 86400 IN NS dns1.ffu.com. ;; ADDITIONAL SECTION: dns1.ffu.com. 86400 IN A 192.168.196.168 ;; Query time: 2 msec ;; SERVER: 192.168.196.168#53(192.168.196.168) ;; WHEN: Thu Jul 27 02:29:47 2017 ;; MSG SIZE rcvd: 152 ","date":"7 August 2017","permalink":"/2017/08/dns-acl/","section":"博客","summary":"\u003ch3 class=\"relative group\"\u003e\u0026ldquo;就近\u0026quot;解析的优点 \n    \u003cdiv id=\"就近解析的优点\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e5%b0%b1%e8%bf%91%e8%a7%a3%e6%9e%90%e7%9a%84%e4%bc%98%e7%82%b9\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e​          假设有一个面向全网络的web服务，并且服务器都集中在一个地方，而访问请求是来自全国各地的，网络传输线路的长短势必会影响访问响应质量和速度。因此，为了提高客户端访问速度、提供容错能力等，公司可能会按区域部署服务器，比如说在北京、杭州、广州等地，每台服务器刚好能优先响应离自己最近的访问请求，从而提高服务器响应速度。\u003c/p\u003e","title":"DNS服务器据源地址解析的实现"},{"content":"","date":"8 December 2016","permalink":"/tags/mysql/","section":"Tags","summary":"","title":"Mysql"},{"content":"","date":"8 December 2016","permalink":"/tags/xtrabackup/","section":"Tags","summary":"","title":"xtrabackup"},{"content":"Xtrabackup是由percona提供的mysql数据库备份工具，支持对Innodb存储引擎的数据库在线热备份（备份时不影响数据读写), 它有以下特点：\n(1)备份过程快速、可靠； (2)备份过程不会打断正在执行的事务； (3)能够基于压缩等功能节约磁盘空间和流量； (4)自动实现备份检验； (5)还原速度快；\n本文使用的软件包为 percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm ，最新版的软件可从 http://www.percona.com/software/percona-xtrabackup/ 获得\n为解决依赖性关系使用yum源安装\n$ yum install -y ./percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm 本文的备份策略： 全量备份+增量备份+binlog\n使用root用户进行一次全量备份 $ cd /var/lib/mysql $ innobackupex --user=root --host=localhost --password=123456 /data/backup/ ...中间省略... xtrabackup: Transaction log of lsn (310138946) to (310138946) was copied. 170915 20:31:03 completed OK! \u0026lt;-- 显示备份成功 $ ls /data/backup 2017-09-16_08-22-55 \u0026lt;-- 全量备份生成的文件夹 $ cd 2017-09-16_08-22-55/ $ ls backup-my.cnf ibdata1 performance_schema xtrabackup_checkpoints xtrabackup_logfile database mysql testdb xtrabackup_info $ testdb库中创建新表tb3并添加数据 $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 11 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; show tables; +------------------+ | Tables_in_testdb | +------------------+ | tb1 | | tb2 | +------------------+ 2 rows in set (0.00 sec) MariaDB [testdb]\u0026gt; create table tb3 (stuid int unsigned primary key,name varchar(200),age tinyint); Query OK, 0 rows affected (0.07 sec) MariaDB [testdb]\u0026gt; insert into tb3 values(1,\u0026#39;stu1\u0026#39;,22),(2,\u0026#39;stu2\u0026#39;,24),(3,\u0026#39;stu3\u0026#39;,28); Query OK, 3 rows affected (0.00 sec) Records: 3 Duplicates: 0 Warnings: 0 MariaDB [testdb]\u0026gt; select * from tb3; +-------+------+------+ | stuid | name | age | +-------+------+------+ | 1 | stu1 | 22 | | 2 | stu2 | 24 | | 3 | stu3 | 28 | +-------+------+------+ 3 rows in set (0.00 sec) 进行第一次增量备份 每个InnoDB的页面都会包含一个LSN(日志序列号)信息，每当相关的数据发生改变，相关的页面的LSN就会自动增长。这正是InnoDB表可以进行增量备份的基础，即innobackupex通过备份上次完全备份之后发生改变的页面来实现。增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。\n$ cd /var/lib/mysql $ innobackupex --user=root --host=localhost --password=123456 --incremental /data/backup --incremental-basedir=/data/backup/2017-09-16_08-22-55 $ ls /data/backup 2017-09-16_08-22-55 2017-09-16_08-27-37 \u0026lt;-- 增量备份生成的文件夹 $ cat 2017-09-16_08-22-55/xtrabackup_checkpoints backup_type = full-backuped from_lsn = 0 to_lsn = 310144589 last_lsn = 310144589 compact = 0 recover_binlog_info = 0 $ cat 2017-09-16_08-27-37/xtrabackup_checkpoints backup_type = incremental from_lsn = 310144589 \u0026lt;-- 日志序列号承接全量备份的 to_lsn = 310148850 last_lsn = 310148850 compact = 0 recover_binlog_info = 0 删除testdb库中tb2 $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 8 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement MariaDB [(none)]\u0026gt; use testdb Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; drop table tb2; MariaDB [testdb]\u0026gt; show tables; +------------------+ | Tables_in_testdb | +------------------+ | tb1 | | tb3 | +------------------+ 2 rows in set (0.00 sec) 进行第二次增量备份 $ innobackupex --user=root --host=localhost --password=123456 --incremental /data/backup --incremental-basedir=/data/backup/2017-09-16_08-27-37 ...中间省略... xtrabackup: Transaction log of lsn (310149698) to (310149698) was copied. 170916 08:42:46 completed OK! $ ls /data/backup 2017-09-16_08-22-55 2017-09-16_08-27-37 2017-09-16_08-42-43 $ cat 2017-09-16_08-42-43/xtrabackup_checkpoints backup_type = incremental from_lsn = 310148850 \u0026lt;-- 日志序列号承接第一次增量备份的 to_lsn = 310149698 last_lsn = 310149698 compact = 0 recover_binlog_info = 0 删除testdb库中tb1 $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 11 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use testdb Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; drop table tb1; Query OK, 0 rows affected (0.03 sec) MariaDB [testdb]\u0026gt; show tables; +------------------+ | Tables_in_testdb | +------------------+ | tb3 | +------------------+ 1 row in set (0.00 sec) 备份最近一次增量备份之后的二进制日志 $ cat 2017-09-16_08-42-43/xtrabackup_info uuid = f5036e11-9a77-11e7-abfa-00505626a77f name = tool_name = innobackupex tool_command = --user=root --host=localhost --password=... --incremental /data/backup --incremental-basedir=/data/backup/2017-09-16_08-27-37 tool_version = 2.4.7 ibbackup_version = 2.4.7 server_version = 5.5.52-MariaDB start_time = 2017-09-16 08:42:43 end_time = 2017-09-16 08:42:46 lock_time = 0 binlog_pos = filename \u0026#39;master-log.000001\u0026#39;, position \u0026#39;828\u0026#39; \u0026lt;--最后一次增量备份对应二进制日志位置 innodb_from_lsn = 310148850 innodb_to_lsn = 310149698 partial = N incremental = Y format = file compact = N compressed = N encrypted = N $ mysqlbinlog -j 828 /var/lib/mysql/master-log.000001 \u0026gt; /data/backup/2017-09-16_08-42-43-binlog 8 ) 模拟服务器故障并恢复\n模拟故障并删除数据 $ service mariadb stop $ rm -rf /var/lib/mysql/* prepare增量备份 需要在每个备份(包括完全和各个增量备份)上，将已经提交的事务进行“重放”。“重放”之后，所有的备份数据将合并到完全备份上。 基于所有的备份将未提交的事务进行“回滚”。\n$ cd 2017-09-16_08-22-55/ $ innobackupex --apply-log --redo-only ./ \u0026lt;-- 合并完成之前只重放不回滚 ...中间省略... InnoDB: Shutdown completed; log sequence number 310150212 170916 10:10:24 completed OK! $ innobackupex --apply-log --redo-only ./ --incremental-dir=/data/backup/2017-09-16_08-27-37 $ cat 2017-09-16_08-22-55/xtrabackup_checkpoints backup_type = log-applied from_lsn = 0 to_lsn = 310148850 \u0026lt;-- 完全备份的lsn已经合并到第一次增量备份的lsn last_lsn = 310148850 compact = 0 recover_binlog_info = 0 $ innobackupex --apply-log --redo-only ./ --incremental-dir=/data/backup/2017-09-16_08-42-43 $ innobackupex --apply-log ./ \u0026lt;-- 合并完成后进行回滚操作 从完全备份中恢复数据 $ cd 2017-09-16_08-22-55/ $ innobackupex --copy-back ./ $ ls -l /var/lib/mysql total 40984 drwxr-x--- 2 root root 4096 Sep 16 10:15 database -rw-r----- 1 root root 18874368 Sep 16 10:15 ibdata1 -rw-r----- 1 root root 5242880 Sep 16 10:15 ib_logfile0 -rw-r----- 1 root root 5242880 Sep 16 10:15 ib_logfile1 -rw-r----- 1 root root 12582912 Sep 16 10:15 ibtmp1 drwxr-x--- 2 root root 4096 Sep 16 10:15 mysql drwxr-x--- 2 root root 4096 Sep 16 10:15 performance_schema drwxr-x--- 2 root root 4096 Sep 16 10:15 testdb -rw-r----- 1 root root 24 Sep 16 10:15 xtrabackup_binlog_pos_innodb -rw-r----- 1 root root 569 Sep 16 10:15 xtrabackup_info $ chown -R mysql.mysql /var/lib/mysql/* \u0026lt;-- 修改数据目录属主属组 二进制日志还原 $ service mariadb start $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 3 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | database | | mysql | | performance_schema | | testdb | +--------------------+ 5 rows in set (0.00 sec) MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; show tables; \u0026lt;-- 已经恢复到最后一次增量备份的状态 +------------------+ | Tables_in_testdb | +------------------+ | tb1 | | tb3 | +------------------+ 2 rows in set (0.00 sec) MariaDB [testdb]\u0026gt; set @@session.sql_log_bin=OFF; \u0026lt;-- 二进制日志重放无需记录日志 Query OK, 0 rows affected (0.01 sec) MariaDB [testdb]\u0026gt; \\. /data/backup/2017-09-16_08-42-43-binlog \u0026lt;-- 根据故障前备份的二进制日志重放 Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Database changed Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Charset changed Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.01 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) Query OK, 0 rows affected (0.00 sec) MariaDB [testdb]\u0026gt; show tables; \u0026lt;-- 最后一次增量备份后的操作(drop tb2)已经重放 +------------------+ | Tables_in_testdb | +------------------+ | tb3 | +------------------+ 1 row in set (0.00 sec) MariaDB [testdb]\u0026gt; set @@session.sql_log_bin=ON; \u0026lt;-- 二进制日志记录开启 Query OK, 0 rows affected (0.00 sec) 至此，mysql服务已经实现数据恢复，可以上线；接下来应该立即更新备份即进行一次全量备份。\n","date":"8 December 2016","permalink":"/2016/12/xtrabackup/","section":"博客","summary":"Xtrabackup是由percona提供的mysql数据库备份工具，支持对Innodb存储引擎的数据库在线热备份（备份时不影响数据读写), 它有以下特点：\n(1)备份过程快速、可靠； (2)备份过程不会打断正在执行的事务； (3)能够基于压缩等功能节约磁盘空间和流量； (4)自动实现备份检验； (5)还原速度快；\n本文使用的软件包为 percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm ，最新版的软件可从 http://www.percona.com/software/percona-xtrabackup/ 获得\n为解决依赖性关系使用yum源安装\n$ yum install -y ./percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm 本文的备份策略： 全量备份+增量备份+binlog\n使用root用户进行一次全量备份 $ cd /var/lib/mysql $ innobackupex --user=root --host=localhost --password=123456 /data/backup/ .","title":"使用XtraBackup工具实现MySQL数据备份及恢复"},{"content":"mysqldump是一款mysql服务自带的功能强大的逻辑备份工具，通常使用mysqldump对 MySQL 数据库进行逻辑全量备份(所有数据集)。 mysqldump可以把要备份的数据库装载到一个单独的文本文件中，这个文件包含有所有重建该数据库所需要的SQL命令，这个文本文件可以用一个简单的批处理和一个合适SQL语句导回到MySQL中，完成恢复。\n备份单个数据库或单个数据库中的指定表： ​ mysqldump [OPTIONS] database [tables]\n备份多个数据库： ​ mysqldump [OPTIONS] \u0026ndash;databases [OPTIONS] DB1 [DB2 DB3\u0026hellip;]\n备份所有数据库： ​ mysqldump [OPTIONS] \u0026ndash;all-databases [OPTIONS]\n对于使用MyISAM存储引擎的mysql只支持温备, Innodb存储引擎还支持热备\n热备：读写操作均可进行的状态下所做的备份；\n温备：可读但不可写状态下进行的备份\n冷备：读写操作均不可进行的状态下所做的备份； 服务不在线\nmysqldump命令行常用参数：\n-x, \u0026ndash;lock-all-tables：锁定所有库的所有表，读锁 \u0026lt;\u0026ndash; 温备，备份时要锁定表； -l, \u0026ndash;lock-tables：锁定指定库所有表 \u0026lt;\u0026ndash;温备，备份时要锁定表；\n\u0026ndash;single-transaction：创建一个事务，基于此快照执行备份 \u0026lt;\u0026ndash;InnoDB存储引擎支持热备\n-R, \u0026ndash;routines：备份指定库的存储过程和存储函数\n\u0026ndash;triggers：备份指定库的触发器\n-E, \u0026ndash;events ：备份指定库的事件调度器\n\u0026ndash;master-data[=#] \u0026lt;\u0026ndash; 记录备份时二进制日志的position\n​ 1：记录为CHANGE MASTER TO语句，此语句不被注释\n​ 2：记录为CHANGE MASTER TO语句，此语句被注释\n\u0026ndash;flush-logs：锁定表完成后，即进行日志刷新操作； \u0026lt;\u0026ndash; 滚动日志，方便按备份时间点恢复\n备份恢复单个数据库 # 查看数据库testdb，数据库下的表引擎均为Innodb $\tmysql -uroot -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 29 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use testdb Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; show table status\\G; *************************** 1. row *************************** Name: test1 Engine: InnoDB Version: 10 Row_format: Compact Rows: 3 Avg_row_length: 5461 Data_length: 16384 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2017-09-13 16:22:53 Update_time: NULL Check_time: NULL Collation: latin1_swedish_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec) MariaDB [testdb]\u0026gt; show table status where Engine!=\u0026#39;Innodb\u0026#39;; Empty set (0.00 sec) 使用mysqldump进行备份 # 热备 $ mysqldump -uroot -p --single-transaction --databases testdb --master-data=2 --flush-logs \u0026gt; /tmp/testdb-fullbackup-$(date +%F-%H-%M-%S) Enter password: $ ls /tmp testdb-fullbackup-2017-09-14-19-12-45 # 温备 $ mysqldump -uroot -p -x -R -E --triggers --databases testdb --master-data=2 --flush-logs \u0026gt; /tmp/testdb-fullbackup-$(date +%F-%H-%M-%S) Enter password: $ ls /tmp testdb-fullbackup-2017-09-14-19-42-54 删除数据库testdb，并恢复 [root@Centos7 ~]# mysql -uroot -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 35 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; drop database testdb; Query OK, 1 row affected (0.09 sec) MariaDB [(none)]\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mydb | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.00 sec) $ mysql -uroot -p123456 \u0026lt; /tmp/testdb-fullbackup-2017-09-14-19-12-45 $ mysql -uroot -p123456 -e \u0026#39;show databases\u0026#39; +--------------------+ | Database | +--------------------+ | information_schema | | mydb | | mysql | | performance_schema | | testdb | +--------------------+ 上例只是简单演示数据库的备份恢复，实际生产中数据是在一直更新的。所有只是恢复备份是不够的，我们还需要重放执行备份后更新的二进制日志\n备份恢复所有数据库 # 使用mysqldump进行备份 $ mysqldump -uroot -p123456 -x -R -E --triggers --databases testdb --master-data=2 --flush-logs \u0026gt; /tmp/testdb-fullbackup-$(date +%F-%H-%M-%S) $ ls /tmp testdb-fullbackup-2017-09-14-20-23-01 testdb库中创建新表test2 $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 43 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; create table test2 (id int unsigned primary key,name varchar(200)); Query OK, 0 rows affected (0.03 sec) MariaDB [testdb]\u0026gt; show tables; +------------------+ | Tables_in_testdb | +------------------+ | test1 | | test2 | +------------------+ 2 rows in set (0.00 sec) MariaDB [testdb]\u0026gt; insert into test2 values(1,\u0026#39;tom\u0026#39;),(2,\u0026#39;jerry\u0026#39;); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 MariaDB [testdb]\u0026gt; select * from test2; +----+-------+ | id | name | +----+-------+ | 1 | tom | | 2 | jerry | +----+-------+ 2 rows in set (0.00 sec) 备份二进制日志文件（执行备份操作后的） $ mysqlbinlog master-log.000005 \u0026gt; /tmp/testdb-backup-binlog 删除/var/lib/mysql目录下所有数据库文件,模拟服务器崩溃 $ service mariadb stop $ rm -rf /var/lib/mysql/* 关闭mysql服务记录二进制日志功能(恢复数据无需记录) $ vim /etc/my.cnf.d/server.cnf ... [mysqld] server_id = 1 #log_bin = master-log skip_name_resolve = ON innodb_file_per_table = ON ... 启动mysql服务并恢复备份数据 $ service mariadb start $ mysql \u0026lt; /tmp/testdb-fullbackup-2017-09-14-20-23-01 $ mysql Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 3 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mydb | | mysql | | performance_schema | | table | | test | | testdb | +--------------------+ 7 rows in set (0.00 sec) MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; show tables; \u0026lt;-- 连入mysql，备份后的testdb.test2没有恢复 +------------------+ | Tables_in_testdb | +------------------+ | test1 | +------------------+ 1 row in set (0.00 sec) 重放二进制文件，恢复备份时间点后的数据 $ mysql \u0026lt; /tmp/testdb-backup-binlog $ mysql Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 5 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; show tables; \u0026lt;-- test2表数据也恢复了 +------------------+ | Tables_in_testdb | +------------------+ | test1 | | test2 | +------------------+ 2 rows in set (0.00 sec) MariaDB [testdb]\u0026gt; select * from test2; +----+-------+ | id | name | +----+-------+ | 1 | tom | | 2 | jerry | +----+-------+ 2 rows in set (0.00 sec) 由于mysqldump是基于mysql客户端协议的备份工具，所以它也支持远程恢复指定数据库。还有一点mysqldump工具是将数据转换为特定格式的字符串并显示出来，所以使用mysqldump工具备份对于浮点型数据会丢失精度。\n","date":"18 November 2016","permalink":"/2016/11/mysqldump/","section":"博客","summary":"mysqldump是一款mysql服务自带的功能强大的逻辑备份工具，通常使用mysqldump对 MySQL 数据库进行逻辑全量备份(所有数据集)。 mysqldump可以把要备份的数据库装载到一个单独的文本文件中，这个文件包含有所有重建该数据库所需要的SQL命令，这个文本文件可以用一个简单的批处理和一个合适SQL语句导回到MySQL中，完成恢复。\n备份单个数据库或单个数据库中的指定表： ​ mysqldump [OPTIONS] database [tables]\n备份多个数据库： ​ mysqldump [OPTIONS] \u0026ndash;databases [OPTIONS] DB1 [DB2 DB3\u0026hellip;]\n备份所有数据库： ​ mysqldump [OPTIONS] \u0026ndash;all-databases [OPTIONS]\n对于使用MyISAM存储引擎的mysql只支持温备, Innodb存储引擎还支持热备\n热备：读写操作均可进行的状态下所做的备份；\n温备：可读但不可写状态下进行的备份\n冷备：读写操作均不可进行的状态下所做的备份； 服务不在线","title":"使用mysqldump工具实现MySQL数据备份"},{"content":"MySQL主从复制原理 # 从节点的I/O线程向主节点请求binlog事件，主节点Dump线程读取binlog事件并发送事件至从节点I/O线程，从节点将事件写到relay log（中继日志） 文件中；之后从节点的SQL 线程，会读取relay log文件中的日志，重放事件来实现主从数据一致。\n事实上，主节点是可以并行写的(mysql的并发)，而binlog是串行写(每次写一个事件) ，所以主从复制中，mysql是单线程复制，也就不可避免的造成了主从数据同步有延迟，所以主从复制也成为异步复制\n主从复制配置 # 本文以一主一从为例。\n在配置之前确保时间同步，并且从节点的版本号与主节点相同或者高于主节点的版本号\n配置主服务器 开启二进制日志记录binlog\n$ vim /etc/my.cnf.d/server.cnf [mysqld] server_id = 1 log_bin = master-log skip_name_resolve = ON \u0026lt;-- Centos6不支持 innodb_file_per_table = ON 创建用于复制的账号并授权\n$ systemctl start mariadb $ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 2 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO \u0026#39;rpuser\u0026#39;@\u0026#39;192.168.196.%\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; Query OK, 0 rows affected (0.01 sec) MariaDB [(none)]\u0026gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]\u0026gt; SHOW MASTER STATUS; \u0026lt;-- 查看binlog日志position +-------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +-------------------+----------+--------------+------------------+ | master-log.000001 | 495 | | | +-------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) 配置从服务器 开启中继日志relay-log\n$ vim /etc/my.cnf.d/server.cnf [mysqld] server_id = 2 relay_log = relay-log skip_name_resolve = ON innodb_file_per_table = ON read_only = ON 配置slave连接至主节点\n$ mysql -uroot -p123456 Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 3 Server version: 5.5.52-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;192.168.196.129\u0026#39;,MASTER_USER=\u0026#39;rpuser\u0026#39;,MASTER_PASSWORD=\u0026#39;123456\u0026#39;,MASTER_LOG_FILE=\u0026#39;master-log.000001\u0026#39;,MASTER_LOG_POS=495; Query OK, 0 rows affected (0.09 sec) MariaDB [(none)]\u0026gt; START SLAVE; Query OK, 0 rows affected (0.01 sec) MariaDB [(none)]\u0026gt; SHOW SLAVE STATUS\\G; *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.196.129 Master_User: rpuser Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-log.000001 Read_Master_Log_Pos: 495 Relay_Log_File: relay-log.000002 Relay_Log_Pos: 530 Relay_Master_Log_File: master-log.000001 Slave_IO_Running: Yes \u0026lt;-- I/O线程开启 Slave_SQL_Running: Yes \u0026lt;-- SQL线程开启 Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 495 Relay_Log_Space: 818 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 1 row in set (0.00 sec) 查看从节点的两个文件master.info 、relay-log.info $ cat /var/lib/mysql/master.info \u0026lt;-- 储存有master信息，重启也会生效 18 master-log.000001 495 192.168.196.129 rpuser 123456 3306 60 0 0 1800.000 0 $ cat /var/lib/mysql/relay-log.info \u0026lt;-- 记录有当前数据库对应日志位置 ./relay-log.000002 530 master-log.000001 495 更新主节点服务器数据 $ mysql -uroot -p123456 MariaDB [(none)]\u0026gt; create database testdb; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]\u0026gt; use testdb Database changed MariaDB [testdb]\u0026gt; create table tb1 (id int); Query OK, 0 rows affected (0.02 sec) MariaDB [testdb]\u0026gt; insert into tb1 values(1),(2); Query OK, 2 rows affected (0.00 sec) Records: 2 Duplicates: 0 Warnings: 0 MariaDB [testdb]\u0026gt; SHOW MASTER STATUS; +-------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +-------------------+----------+--------------+------------------+ | master-log.000003 | 863 | | | +-------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) 从节点查看数据已经同步 $ mysql -uroot -p123456 MariaDB [(none)]\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | test | | testdb | +--------------------+ 5 rows in set (0.00 sec) MariaDB [(none)]\u0026gt; use testdb; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [testdb]\u0026gt; select * from tb1; +------+ | id | +------+ | 1 | | 2 | +------+ 2 rows in set (0.00 sec) MariaDB [testdb]\u0026gt; show slave status\\G; *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.196.129 Master_User: rpuser Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-log.000003 Read_Master_Log_Pos: 863 Relay_Log_File: relay-log.000002 Relay_Log_Pos: 898 Relay_Master_Log_File: master-log.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 \u0026lt;-- 参考 1) Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 863 Relay_Log_Space: 1186 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 1 row in set (0.01 sec) 如果出现error SQL线程会停止运行，执行set global sql_slave_skip_counter=# 语句后start slave 即可解决 ","date":"6 November 2016","permalink":"/2016/11/mysql-replication/","section":"博客","summary":"MySQL主从复制原理 # 从节点的I/O线程向主节点请求binlog事件，主节点Dump线程读取binlog事件并发送事件至从节点I/O线程，从节点将事件写到relay log（中继日志） 文件中；之后从节点的SQL 线程，会读取relay log文件中的日志，重放事件来实现主从数据一致。\n事实上，主节点是可以并行写的(mysql的并发)，而binlog是串行写(每次写一个事件) ，所以主从复制中，mysql是单线程复制，也就不可避免的造成了主从数据同步有延迟，所以主从复制也成为异步复制\n主从复制配置 # 本文以一主一从为例。\n在配置之前确保时间同步，并且从节点的版本号与主节点相同或者高于主节点的版本号\n配置主服务器 开启二进制日志记录binlog\n$ vim /etc/my.cnf.d/server.cnf [mysqld] server_id = 1 log_bin = master-log skip_name_resolve = ON \u0026lt;-- Centos6不支持 innodb_file_per_table = ON 创建用于复制的账号并授权","title":"MySQL主从复制的实现"},{"content":"Nginx是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。它是基于事件驱动模型，能承受万级别的高并发http请求。由于其负载性能很好，它合适做前端服务器，而高速处理静态类文件也是它的优点。nginx通过使用ngx_http_fastcgi_module模块也可以将客户端动态请求反向代理到后端的php-fpm，两者之间遵循FastCGI协议。\nphp-fpm(php-Fastcgi Process Manager)也是FastCGI的具体实现，可以独立运行并提供服务也提供了进程管理的功能。它的工作模式类似于prefork，包含 master 和 worker两种进程。 master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个(具体数量根据实际需要配置)，每个进程响应一个请求，而且内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方。nginx与php-fpm可以配置在同一台服务器上，也可以独立部署，在并发访问量不是很大时，建议采用后者的结合方式。\n本文将分离部署nginx与php-fpm服务，拓扑模型如下：\nphp-fpm服务器上部署wordpress,并开启nfs服务，导出wordpress所在目录到nginx服务器，将nginx根目录挂载到导出目录，配置nginx实现只将php动态请求交由php-fpm服务器处理，其它静态文件则有nginx直接响应。\n配置nginx服务器 # 使用yum源安装$ yum install -y nginx\n创建虚拟服务器配置文件\n$ vim /etc/nginx/conf.d/vhost.conf #http上下文中定义压缩；server中单独定义gzip on|off; gzip_min_length 64; gzip_comp_level 2; gzip_types text/xml text/plain text/css application/json application/x-javascript application/xml application/xml+rss text/javascript; gzip_proxied any; #http上下文中定义fastcgi_cache,server|location中调用 fastcgi_cache_path /data/nginx/fastcgi_cache levels=1:2:1 keys_zone=fcgi:20m inactive=120s; server{ server_name www.ffu.com; listen 80 ; #实现https跳转 rewrite /(.*)$ https://www.ffu.com/$1; } server{ server_name www.ffu.com; location / { root /data/www/web1; index index.php index.html; } #实现https listen 443 ssl; ssl on; ssl_certificate /etc/nginx/nginx.crt; ssl_certificate_key /etc/nginx/nginx.key; ssl_session_cache shared:sslcache:20m; #定义访问策略 allow 192.168.196.0/24; deny all; #启用压缩功能 gzip on; #文件操作优化 aio on; open_file_cache max=1000 inactive=20s; open_file_cache_valid 50s; open_file_cache_min_uses 2; open_file_cache_errors off; #防盗链,只允许指定域名能引用资源 valid_referers none block server_names *.ffu.com ffu.* ~\\.ffu\\.; if ($invalid_referer) { return http://www.ffu.com/invalid.jpg; #return 403; } location ~* \\.php$ { #反代php动态请求到后端php-fpm服务器上应用所在的目录 fastcgi_pass 192.168.196.131:9000; fastcgi_param SCRIPT_FILENAME /data/www/app$fastcgi_script_name; fastcgi_index index.php; include fastcgi_params; #fastcgi缓存设置 fastcgi_cache fcgi; fastcgi_cache_key $request_uri; fastcgi_cache_valid 200 302 10m; fastcgi_cache_valid 301 1h; fastcgi_cache_valid any 1m; } #对php-fpm的内嵌状态页请求直接反代到后端php-fpm服务器 location ~* ^/(status|ping) { fastcgi_pass 192.168.196.131:9000; fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; include fastcgi_params; } #指定错误页面 error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 实现ssl加密的私钥及证书申请，这里不再单独介绍，详情参考另一篇 关于使用openssl工具创建私有CA的博客\n配置php-fpm服务器 # 使用yum源安装$ yum install -y php-fpm $ vim /etc/php-fpm.d/www.conf listen = 192.168.196.131:9000 \u0026lt;-- 监听的套接字 user = nginx group = nginx pm = dynamic pm = static \u0026lt;--静态配置 pm.max_children = 256 \u0026lt;--开启的最大php进程数 pm.max_requests = 1024 \u0026lt;--在执行了1024个请求后重启worker进程 pm.status_path = /status ping.path = /ping 服务器内存较大时建议直接计算后配置静态资源池，可以减少频繁prefork进程所带来的开销，提高服务质量\n配置nfs服务 $ yum install nfs-utils rpcbind $ useradd -r -s /sbin/nologin nginx \u0026lt;-- 创建nginx用户 $ id nginx uid=996(nginx) gid=994(nginx) groups=994(nginx) $ vim /etc/exports #设定导出目录，只允许nginx服务器挂载，压缩所有远程用户为特定用户(nginx)的UID与GID /data/www/app 192.168.196.129(rw,sync,all_squash,anonuid=996,anongid=994) 配置mysql $ yum install mariadb-server $ mysql_secure_installation \u0026lt;-- 安全初始化root账户密码 $ mysql -uroot -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 18 Server version: 10.2.7-MariaDB-log MariaDB Server Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; create database blogdb; \u0026lt;-- 创建wordpress的数据库 MariaDB [(none)]\u0026gt; grant all on blogdb.* to wpuser@\u0026#39;192.168.196.%\u0026#39; identified by \u0026#39;123456\u0026#39;; \u0026lt;-- 创建wordpress连接mysql的账户 安装wordpress $ mkdir /data/www $ tar xf wordpress-4.8-zh_CN.tar.gz -C /data/www/ $ cd /data/www $ ln -sv wordpress app \u0026lt;--创建指向wordpress的软链接 $ chown -R nginx.nginx app/* $ mv wp-config-sample.php wp-config.php $ vim wp-config.php \u0026lt;-- 配置接口连接至mysql服务器 /** WordPress数据库的名称 */ define(\u0026#39;DB_NAME\u0026#39;, \u0026#39;blogdb\u0026#39;); /** MySQL数据库用户名 */ define(\u0026#39;DB_USER\u0026#39;, \u0026#39;wpuser\u0026#39;); /** MySQL数据库密码 */ define(\u0026#39;DB_PASSWORD\u0026#39;, \u0026#39;123456\u0026#39;); /** MySQL主机 */ define(\u0026#39;DB_HOST\u0026#39;, \u0026#39;192.168.196.131\u0026#39;); nginx服务器根目录挂载到nfs导出目录 # $ vim /etc/fstab 192.168.196.131:/data/www/app /data/www/web1 nfs defaults 0 0 $ mount -a $ mount 192.168.196.131:/data/www/app on /data/www/web1 type nfs (rw,relatime,vers=3,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=192.168.196.131,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=192.168.196.131) 客户端浏览器访问wordpress # 开启各服务之后，在浏览器中访问并创建账户，即完成wordpress部署\n","date":"15 September 2016","permalink":"/2016/09/nginx-php-fpm/","section":"博客","summary":"\u003cp\u003eNginx是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。它是基于事件驱动模型，能承受万级别的高并发http请求。由于其负载性能很好，它合适做前端服务器，而高速处理静态类文件也是它的优点。nginx通过使用ngx_http_fastcgi_module模块也可以将客户端动态请求反向代理到后端的php-fpm，两者之间遵循FastCGI协议。\u003c/p\u003e","title":"Nginx+php-fpm分离部署搭建wordpress"},{"content":"","date":"15 September 2016","permalink":"/tags/wordpress/","section":"Tags","summary":"","title":"wordpress"},{"content":"MariaDB 是一个采用Maria 存储引擎的MySQL分支版本，是由原来 MySQL 的作者Michael Widenius创办的公司所开发的免费开源的数据库服务器。MariaDB是目前最受关注的MySQL数据库衍生版，也被视为开源数据库MySQL的替代品。除了使用Linux 各发行版供应商的程序包安装，也可以选择基于二进制格式的程序包进行安装。具体安装步骤如下：\n1.下载二进制源码 # 官网下载地址http://downloads.mariadb.org\n2.创建系统用户 # [root@Centos6 ~]# groupadd -r -g 36 mysql [root@Centos6 ~]# useradd -r -u 36 -g 36 mysql 3.准备二进制程序 # 解压缩包到/usr/local\n[root@Centos6 ~]# tar xf /root/mariadb-5.5.57-linux-x86_64.tar.gz -C /usr/local/ 创建软链接并修改目录属组为mysql\n[root@Centos6 local]#cd /usr/local [root@Centos6 local]#ln -sv mariadb-5.5.57-linux-x86_64/ mysql `mysql\u0026#39; -\u0026gt; `mariadb-5.5.57-linux-x86_64/\u0026#39; [root@Centos6 local]# chown -R root:mysql /usr/local/mysql/ 4.准备mysql数据存储目录 # 建议把mysql数据存在基于逻辑卷的单独分区\n[root@Centos6 local]#lvcreate -L 20G -n mydata vg_centos6 [root@Centos6 local]#mkfs.ext4 /dev/vg_centos6/mydata 设置开机自动挂载逻辑卷mydata到/mydata\n[root@Centos6 local]mkdir /mydata [root@Centos6 local]vim /etc/fstab /dev/vg_centos6/mydata /mydata ext4 defaults 0 0 [root@Centos6 local]mount -a 创建存储目录/mydata/data并修改属主属组为mysql\n[root@Centos6 local]chown mysql:mysql /mydata/data 5.创建数据库文件 # 安装包提供了自动生成数据库的脚本/usr/local/mysql/scripts/mysql_install_db,在/usr/local/mysql目录下运行该脚本\n[root@Centos6 mysql]# ./scripts/mysql_install_db --user=mysql --datadir=/mydata/data [root@Centos6 mysql]# ./scripts/mysql_install_db --help \u0026lt;-- 可以查看脚本帮助 [root@Centos6 mysql]# ls /mydata/data aria_log.00000001 aria_log_control mysql performance_schema test 6.准备mysqld程序配置文件 # ​\t配置文件查找次序: /etc/my.cnf \u0026ndash; \u0026gt; /etc/mysql/my.cnf \u0026ndash; \u0026gt; -- default-extrafile=/PATH/TO/CONF_FILE(第5步中脚本选项指定的配置文件) \u0026ndash; \u0026gt; ~/. my.cnf\n安装包提供了几种不同配置的模板配置文件,位于目录/usr/local/mysql/suport-files/;可以根据数据库的大小及服务器配置等选择合适的模板进行修改\n[root@Centos6 mysql]# mkdir /etc/mysql [root@Centos6 mysql]# cp support-files/my-large.cnf /etc/mysql/my.cnf [root@Centos6 mysql]# vim /etc/mysql/my.cnf datadir = /mydata/data\t\u0026lt;--指定数据库文件存储目录 innodb_file_per_table = on\t\u0026lt;--数据库中各表格以单个文件存储 skip_name_resolve = on\t\u0026lt;--禁止主机名解析 7.准备日志文件 # Centos6\u0026ndash;\u0026gt;/var/log/mysqld.log\n注意在Centos7里自动生成，不用手动创建\u0026ndash;\u0026gt;/var/log/mariadb/mariadb.log\n[root@Centos6 mysql]# touch /var/log/mysqld.log [root@Centos6 mysql]# chown mysql:mysql /var/log/mysqld.log 8.准备服务脚本,并启动服务 # [root@Centos6 mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysqld [root@Centos6 mysql]# chkconfig --add mysqld [root@Centos6 mysql]# chkconfig --list mysqld [root@Centos6 ~]# vi /etc/profile.d/my.sh \u0026lt;--创建系统配置文件,将可执行程序mysql路径加入PATH变量 export PATH=/usr/local/mysql/bin/:$PATH [root@Centos6 mysql]#service mysqld start 9.运行mysql命令\u0026ndash;\u0026gt;交互式客户端程序 # [root@Centos6 ~]#mysql \u0026lt;-- 默认空密码登录 MariaDB [mysql]\u0026gt; use mysql Database changed MariaDB [mysql]\u0026gt; select user,host,password from user; +------+-----------+----------+ | user | host | password | +------+-----------+----------+ | root | localhost | | | root | centos6.9 | | | root | 127.0.0.1 | | | root | ::1 | | | | localhost | | \u0026lt;-- 表示允许匿名登录 | | centos6.9 | | +------+-----------+----------+ 6 rows in set (0.01 sec) mysql用户账号由两部分组成：\u0026lsquo;USERNAME\u0026rsquo;@\u0026lsquo;HOST\u0026rsquo;\nHOST用于限制此用户可通过哪些远程主机连接mysql服务(限制客户端)\nHOST支持CIDR IP表示法；也支持使用通配符：\n​\t% 匹配任意长度的任意字符 eg: 192.168.%.%\n​\t_ 匹配任意单个字符\n10.安全初始化 # 从文章第9步可以看出，数据库默认是允许匿名登录及无密码登录，这是非常不安全的，因此，我们还需要进行安全初始化\n[root@Centos6 ~]# /usr/local/mysql/bin/mysql_secure_installation NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, we\u0026#39;ll need the current password for the root user. If you\u0026#39;ve just installed MariaDB, and you haven\u0026#39;t set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] y\tNew password:\t\u0026lt;-- 设置root密码 Re-enter new password: Password updated successfully! Reloading privilege tables.. ... Success! By default, a MariaDB installation has an anonymous user, allowing anyone to log into MariaDB without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] y \u0026lt;-- 禁止匿名登录 ... Success! Normally, root should only be allowed to connect from \u0026#39;localhost\u0026#39;. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] y \u0026lt;-- 禁止root远程登录 ... Success! By default, MariaDB comes with a database named \u0026#39;test\u0026#39; that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] - Dropping test database... ... Success! - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] y\t\u0026lt;-- 生效权限 ... Success! Cleaning up... All done! If you\u0026#39;ve completed all of the above steps, your MariaDB installation should now be secure. Thanks for using MariaDB! [root@Centos6 ~]# mysql\t\u0026lt;-- 无密码登录已经禁止 ERROR 1045 (28000): Access denied for user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; (using password: NO) [root@Centos6 ~]# mysql -uroot -p\t\u0026lt;-- 正确登入 Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 17 Server version: 5.5.57-MariaDB MariaDB Server Copyright (c) 2000, 2017, Oracle, MariaDB Corporation Ab and others. Type \u0026#39;help;\u0026#39; or \u0026#39;\\h\u0026#39; for help. Type \u0026#39;\\c\u0026#39; to clear the current input statement. MariaDB [(none)]\u0026gt; use mysql Database changed MariaDB [mysql]\u0026gt; select user,host,password from user; +------+-----------+-------------------------------------------+ | user | host | password | +------+-----------+-------------------------------------------+ | root | localhost | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | | root | 127.0.0.1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | | root | ::1 | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 | +------+-----------+-------------------------------------------+ 3 rows in set (0.00 sec) ","date":"31 August 2016","permalink":"/2016/08/centos6-mariadb/","section":"博客","summary":"\u003cp\u003eMariaDB 是一个采用Maria 存储引擎的MySQL分支版本，是由原来 MySQL 的作者Michael Widenius创办的公司所开发的免费开源的数据库服务器。MariaDB是目前最受关注的MySQL数据库衍生版，也被视为开源数据库MySQL的替代品。除了使用Linux 各发行版供应商的程序包安装，也可以选择基于二进制格式的程序包进行安装。具体安装步骤如下：\u003c/p\u003e","title":"CentOS6 mariadb二进制安装"},{"content":"","date":"31 August 2016","permalink":"/tags/mariadb/","section":"Tags","summary":"","title":"mariadb"},{"content":"","date":"20 July 2016","permalink":"/tags/keepalived/","section":"Tags","summary":"","title":"keepalived"},{"content":"keepalived简介 # keepalived是一款解决负载均衡(调度器)的高可用问题的软件，可以实现负载均衡器的故障转移(failover)，原生设计的目的为了高可用ipvs服务，后来增加了脚本调用接口，使其一定程度上具有了管控服务进程的功能，基于此接口也可以支持nginx、haproxy负载均衡的高可用。keepalived是VRRP协议的软件实现，也基于VRRP协议完成地址流动，那什么是VRRP协议呢？\nVRRP协议 # VRRP(Virtual Routing Redundant Protocol)是解决路由器冗余的虚拟路由器冗余协议。\n在VRRP的拓扑模型中，主节点会周期性向多点传送地址(224.0.0.18)发送组播信息即VRRP报文，备用节点若一段时间内没有收到VRRP报文信息，备用节点就会认为主节点已经宕机，然后映射VIP(虚拟地址)，发生故障转移。\nVRRP报文包括以下三项数据：\nVIP 虚拟IP地址 Virtual Rtr ID 虚拟路由器ID ​ 一台物理路由器可能有多个接口，不同物理路由器的接口可以组合出多个虚拟路由器，以此ID区分\nPriority 优先顺序\nVIP优先漂移到高优先级的节点\nkeepalived特点 # 基于VRRP协议实现负载均衡器的故障转移 为集群内所有节点生成ipvs规则（在配置文件中预先定义） 为ipvs集群的各RS做健康状态检测 基于脚本调用接口通过执行脚本完成脚本中定义的功能，进而影响集群事务;正是基于此接口，keepalived才能实现nginx负载均衡器的故障转移 本文将介绍单主、双主模型下nginx负载均衡高可用集群的实现\n单主模型KA高可用集群实现 # 拓扑模型 # 在上图网络拓扑中，nginx server1和server2为安装keepalived的两台负载均衡器：\n主节点nginx server1正常工作时(active状态)，VIP映射在其网卡上，它负责将客户端请求分发至后端real server，此时nginx server2备用状态(passive) nginx server1发生故障后，备用节点nginx server2就会变成主节点，VIP漂移映射到sever2上，之后server2 代替server1响应客户端并分发请求，实现故障转移 一个节点处于active状态，另一个节点处于passive状态的KA高可用模型就称为单主模型\nIP地址分配 # nginx LB sever1:\nens33: 192.168.196.131/24\nens37: 172.16.253.93/16\nnginx LB sever2:\nens37: 192.168.196.133/24\nens33: 172.16.251.171/16\nWeb server1:\nens37: 192.168.196.129/24\nWeb server2:\nens37: 192.168.196.132/24\nVIP: 172.16.255.168/16 \u0026ndash;\u0026gt;对外公开提供服务地址\n服务配置 # 完成服务器准备及IP配置后，首先要同步时间，保证各服务器系统时间一致性，可以借助ntp或者chrony服务。\nWeb server 的配置 # server1: $yum install -y nginx $echo \u0026#34;backend server 1\u0026#34; /usr/share/nginx/html/index.html $systemctl start nginx server2: $yum install -y nginx $echo \u0026#34;backend server 2\u0026#34; /usr/share/nginx/html/index.html $systemctl start nginx nginx server 的配置 # server1 \u0026ndash;\u0026gt; 主节点\n安装配置keepalived $ yum install -y keepalived $ vim /etc/keepalived/keepalived.conf \u0026lt;--修改主配置文件 global_defs { notification_email {\t\u0026lt;--通知邮件相关设置 root@localhost \u0026lt;--邮件发送目标地址 } notification_email_from keepalived@localhost \u0026lt;--邮件发件人 smtp_server 127.0.0.1 \u0026lt;--使用本机邮件服务 smtp_connect_timeout 30 router_id node1 \u0026lt;--物理路由节点名称 vrrp_mcast_group4 224.3.10.67 \u0026lt;--发送VRRP报文的组播地址 } vrrp_script chk_down { \u0026lt;--参考注释[1]定义实现手动维护server的脚本 script \u0026#34;[[ -f /etc/keepalived/down ]] \u0026amp;\u0026amp; exit 1 || exit 0\u0026#34; interval 1 weight -10 } vrrp_script chk_nginx { \u0026lt;--参考注释[2]定义实现检查nginx服务健康检查周期的脚本 script \u0026#34;killall -0 nginx \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34; interval 1 \u0026lt;--检测间隔 weight -10 \u0026lt;--脚本执行状态结果为非0则该虚拟路由优先级-10 fall 2 \u0026lt;--失败检测两次 rise 1 \u0026lt;--成功检测一次 } vrrp_instance VI_1 { \u0026lt;--设置vrrp实例即虚拟路由 state MASTER \u0026lt;--设置虚拟路由为主节点 interface ens37 \u0026lt;--设置进行收发vrrp报文的网卡 virtual_router_id 51 \u0026lt;--虚拟路由器ID，唯一性 priority 100 advert_int \u0026lt;--vrrp通告的时间间隔 authentication { auth_type PASS auth_pass AhpaeQ9J \u0026lt;--VRRP报文合法性认证 } virtual_ipaddress { 172.16.255.168/16 dev ens37 \u0026lt;--设置VIP } track_script { \u0026lt;--调用已定义脚本 chk_down chk_nginx } notify_master \u0026#34;/etc/keepalived/notify.sh master\u0026#34; \u0026lt;--参考注释[3] notify_backup \u0026#34;/etc/keepalived/notify.sh backup\u0026#34; notify_fault \u0026#34;/etc/keepalived/notify.sh fault\u0026#34; } $ scp /etc/keepalived/notify.sh 192.168.196.133:/etc/keepalived/notify.sh [1]检测down文件存在，则脚本执行状态结果为1，调用该脚本的vrrp实例(即虚拟路由器)的优先级减10；本文中master优先级设为100，减10变为90，则小于backup节点的优先级95，backup节点变为主节点，实现故障转移\n[2]检测到nginx服务进程没有运行，则脚本执行状态结果为1，调用该脚本的vrrp实例的优先级减10\n[3]通用格式的通知触发机制，一个脚本可完成master、backup、fault三种状态的转换时的通知\n$ vim /etc/keepalived/notify.sh #!/bin/bash # contact=\u0026#39;root@localhost\u0026#39; notify() { local mailsubject=\u0026#34;$(hostname) to be $1, vip floating\u0026#34; local mailbody=\u0026#34;$(date +\u0026#39;%F %T\u0026#39;): vrrp transition, $(hostname) changed to be $1\u0026#34; echo \u0026#34;$mailbody\u0026#34; | mail -s \u0026#34;$mailsubject\u0026#34; $contact } case $1 in master) notify master ;; backup) notify backup ;; fault) notify fault ;; *) echo \u0026#34;Usage: $(basename $0) {master|backup|fault}\u0026#34; exit 1 ;; esac 配置nginx代理 $ vim /etc/nginx/nginx.conf upstream srvs { server 192.168.196.129; server 192.168.196.132; server 127.0.0.1:8080 backup; \u0026lt;--nginx热备实现sorry server,注意80端口已被占用 } sever{ ... location / { proxy_pass http://srvs; } ... } $ vim /etc/nginx/conf.d/web2.conf \u0026lt;--设置监听8080的虚拟主机做sorry server server{ server_name www.web2.com; listen 8080 ; root /var/www/html/web2; } } $ echo \u0026#39;nginx LB sorry server 1\u0026#39; \u0026gt; /var/www/html/web2/index.html $ systemctl start nginx server2 \u0026ndash;\u0026gt; 备用节点\n安装配置keepalived $ yum install -y keepalived $ vim /etc/keepalived/keepalived.conf global_defs { notification_email { root@localhost } notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id node2 vrrp_mcast_group4 224.3.10.67 \u0026lt;--相同的组播地址 } vrrp_script chk_down { script \u0026#34;[[ -f /etc/keepalived/down ]] \u0026amp;\u0026amp; exit 1 || exit 0\u0026#34; interval 1 weight -10 } vrrp_script chk_nginx { script \u0026#34;killall -0 nginx \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34; interval 1 weight -10 fall 2 rise 1 } vrrp_instance VI_1 { state BACKUP \u0026lt;--状态为backup interface ens33 virtual_router_id 51 priority 95 \u0026lt;--优先级要低于master advert_int 1 authentication { auth_type PASS auth_pass AhpaeQ9J } virtual_ipaddress { 172.16.255.168/16 dev ens33 \u0026lt;--相同的VIP } track_script { chk_down chk_nginx } notify_master \u0026#34;/etc/keepalived/notify.sh master\u0026#34; notify_backup \u0026#34;/etc/keepalived/notify.sh backup\u0026#34; notify_fault \u0026#34;/etc/keepalived/notify.sh fault\u0026#34; } 配置nginx代理 $ vim /etc/nginx/nginx.conf upstream srvs { server 192.168.196.129; server 192.168.196.132; server 127.0.0.1:8080 backup; } server{ ... location / { proxy_passhttp://srvs; } ... } $ vim /etc/nginx/conf.d/vhost.conf server{ server_name www.vhost.com; listen 8080; root \u0026#34;/usr/share/nginx/html\u0026#34;; } $ systemctl start nginx 启用keepalived并测试 # 1.node1\n$ systemctl start keepalived $ systemctl status keepalived ● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2017-09-05 16:24:29 CST; 4s ago Process: 3721 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 3722 (keepalived) CGroup: /system.slice/keepalived.service ├─3722 /usr/sbin/keepalived -D ├─3723 /usr/sbin/keepalived -D └─3724 /usr/sbin/keepalived -D Sep 05 16:24:30 centos7.ffu.com Keepalived_healthcheckers[3723]: Registering Kernel netlink reflector Sep 05 16:24:30 centos7.ffu.com Keepalived_healthcheckers[3723]: Registering Kernel netlink command channel Sep 05 16:24:30 centos7.ffu.com Keepalived_healthcheckers[3723]: Opening file \u0026#39;/etc/keepalived/keepalived.conf\u0026#39;. Sep 05 16:24:30 centos7.ffu.com Keepalived_healthcheckers[3723]: Configuration is using : 7553 Bytes Sep 05 16:24:30 centos7.ffu.com Keepalived_healthcheckers[3723]: Using LinkWatch kernel netlink reflector... Sep 05 16:24:30 centos7.ffu.com Keepalived_vrrp[3724]: VRRP_Instance(VI_1) Transition to MASTER STATE Sep 05 16:24:31 centos7.ffu.com Keepalived_vrrp[3724]: VRRP_Instance(VI_1) Entering MASTER STATE Sep 05 16:24:31 centos7.ffu.com Keepalived_vrrp[3724]: VRRP_Instance(VI_1) setting protocol VIPs. Sep 05 16:24:31 centos7.ffu.com Keepalived_vrrp[3724]: VRRP_Instance(VI_1) Sending gratuitous ARPs on ens37 f....168 Sep 05 16:24:31 centos7.ffu.com Keepalived_healthcheckers[3723]: Netlink reflector reports IP 172.16.255.168 added Hint: Some lines were ellipsized, use -l to show in full. 从状态信息可以看出，node1成为master节点，并映射了VIP到指定的物理网卡；ip a list ens37 命令查看\n$ ip a list ens37 3: ens37: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f1:c7:d5 brd ff:ff:ff:ff:ff:ff inet 172.16.253.93/16 brd 172.16.255.255 scope global dynamic ens37 valid_lft 62845sec preferred_lft 62845sec inet 172.16.255.168/16 scope global secondary ens37 valid_lft forever preferred_lft forever inet6 fe80::fce:4707:e290:f1e0/64 scope link valid_lft forever preferred_lft forever mail查看邮件，确认收到了node状态转换的邮件通知\n$ mail Heirloom Mail version 12.5 7/5/10. Type ? for help. \u0026#34;/var/spool/mail/root\u0026#34;: 1 message 1 new \u0026gt;N 1 root Tue Sep 5 17:26 18/681 \u0026#34;centos7.ffu.com to be master, vip floating\u0026#34; 2.node2\n$ start keepalived $ systemctl status keepalived -l ● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2017-09-05 16:38:24 CST; 19s ago Process: 8822 ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 8825 (keepalived) CGroup: /system.slice/keepalived.service ├─8825 /usr/sbin/keepalived -D ├─8826 /usr/sbin/keepalived -D └─8827 /usr/sbin/keepalived -D Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Netlink reflector reports IP fe80::d254:507:7c6e:2e23 added Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Netlink reflector reports IP fe80::250:56ff:fe20:3183 added Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Registering Kernel netlink reflector Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Registering Kernel netlink command channel Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Opening file \u0026#39;/etc/keepalived/keepalived.conf\u0026#39;. Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Configuration is using : 7807 Bytes Sep 05 16:38:24 Centos7 Keepalived_healthcheckers[8826]: Using LinkWatch kernel netlink reflector... Sep 05 16:38:24 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) Entering BACKUP STATE Sep 05 16:38:24 Centos7 Keepalived_vrrp[8827]: VRRP sockpool: [ifindex(2), proto(112), unicast(0), fd(10,11)] Sep 05 16:38:24 Centos7 Keepalived_vrrp[8827]: VRRP_Script(chk_down) succeeded Sep 05 16:38:24 Centos7 Keepalived_vrrp[8827]: VRRP_Script(chk_nginx) succeeded node2优先级95低于node1的100，从状态信息可以看出，chk_down/chk_nginx 脚本执行状态返回结果均为0显示success，node2成为backup节点，不会映射VIP\n$ mail Heirloom Mail version 12.5 7/5/10. Type ? for help. \u0026#34;/var/spool/mail/root\u0026#34;: 1 message 1 new \u0026gt;N 1 root Tue Sep 5 17:26 18/693 \u0026#34;Centos7 to be backup, vip floating\u0026#34; 3.client测试\n通过VIP访问Web服务，成功实现调度 $ while : ; do curl http://172.16.255.168 ;sleep 1 ;done backend server 1 backend server 2 backend server 1 backend server 2 手动停止node1 nginx进程，脚本chk_nginx状态返回值为1，priority减为90小于node2的95，node2节点则成为master节点，VIP漂移并完成故障转移。同样可以查看keepalived、IP、mail状态 \u0026ndash;\u0026gt; node1\n$ systemctl stop nginx $ systemctl status keepalived ● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2017-09-05 16:37:24 CST; 11min ago ... 中间省略... Sep 05 16:47:53 centos7.ffu.com Keepalived_vrrp[5171]: VRRP_Script(chk_nginx) failed Sep 05 16:47:55 centos7.ffu.com Keepalived_vrrp[5171]: VRRP_Instance(VI_1) Received higher prio advert Sep 05 16:47:55 centos7.ffu.com Keepalived_vrrp[5171]: VRRP_Instance(VI_1) Entering BACKUP STATE Sep 05 16:47:55 centos7.ffu.com Keepalived_vrrp[5171]: VRRP_Instance(VI_1) removing protocol VIPs. Sep 05 16:47:55 centos7.ffu.com Keepalived_healthcheckers[5170]: Netlink reflector reports IP 172.16.255.168 removed Hint: Some lines were ellipsized, use -l to show in full. $ ip a list ens37 3: ens37: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f1:c7:d5 brd ff:ff:ff:ff:ff:ff inet 172.16.253.93/16 brd 172.16.255.255 scope global dynamic ens37 valid_lft 61554sec preferred_lft 61554sec inet6 fe80::fce:4707:e290:f1e0/64 scope link valid_lft forever preferred_lft forever $ mail Heirloom Mail version 12.5 7/5/10. Type ? for help. \u0026#34;/var/spool/mail/root\u0026#34;: 2 messages 1 new 2 unread U 1 root Tue Sep 5 17:26 19/691 \u0026#34;centos7.ffu.com to be master, vip floating\u0026#34; \u0026gt;N 2 root Tue Sep 5 17:30 18/681 \u0026#34;centos7.ffu.com to be backup, vip floating\u0026#34; \u0026ndash;\u0026gt; node2\n[root@Centos7 keepalived]# systemctl status keepalived ● keepalived.service - LVS and VRRP High Availability Monitor Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2017-09-05 16:38:24 CST; 10min ago ... 中间省略... Sep 05 16:38:24 Centos7 Keepalived_vrrp[8827]: VRRP_Script(chk_nginx) succeeded Sep 05 16:47:55 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) forcing a new MASTER election Sep 05 16:47:56 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) Transition to MASTER STATE Sep 05 16:47:57 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) Entering MASTER STATE Sep 05 16:47:57 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) setting protocol VIPs. Sep 05 16:47:57 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) Sending gratuitous ARPs on ens33 for 172.1....168 Sep 05 16:47:57 Centos7 Keepalived_healthcheckers[8826]: Netlink reflector reports IP 172.16.255.168 added Sep 05 16:48:02 Centos7 Keepalived_vrrp[8827]: VRRP_Instance(VI_1) Sending gratuitous ARPs on ens33 for 172.1....168 Hint: Some lines were ellipsized, use -l to show in full. $ ip a list ens33 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:38:c5:21 brd ff:ff:ff:ff:ff:ff inet 172.16.251.171/16 brd 172.16.255.255 scope global dynamic ens33 valid_lft 55724sec preferred_lft 55724sec inet 172.16.255.168/16 scope global secondary ens33 valid_lft forever preferred_lft forever inet6 fe80::d254:507:7c6e:2e23/64 scope link valid_lft forever preferred_lft forever $ mail Heirloom Mail version 12.5 7/5/10. Type ? for help. \u0026#34;/var/spool/mail/root\u0026#34;: 2 messages 1 new 2 unread U 1 root Tue Sep 5 17:26 19/703 \u0026#34;Centos7 to be backup, vip floating\u0026#34; \u0026gt;N 2 root Tue Sep 5 17:30 18/693 \u0026#34;Centos7 to be master, vip floating\u0026#34; \u0026ndash;\u0026gt;访问测试也可以看出故障转移过程\nbackend server 1 backend server 2 curl: (7) Failed connect to 172.16.255.168:80; Connection refused curl: (7) Failed connect to 172.16.255.168:80; Connection refused curl: (7) Failed connect to 172.16.255.168:80; Connection refused backend server 1 backend server 2 重新启动node1 nginx服务，即可实现故障转回(failback)\n依次关闭Web server cluster 的所有nginx服务，启用sorry server，访问测试可以看出 backend server 1 backend server 2 backend server 1 backend server 1 nginx LB sorry server 2 nginx LB sorry server 2 重新启动Web server nginx 服务，即可向外部提供Web服务\n双主模型KA高可用集群实现 # 对比上文拓扑模型，双主模型不同的是，每台负载均衡器上配置两个VRRP实例，互为主备，VIP1和VIP2同时面向网络用户，此时两台负载均衡器均处于active状态\n沿用上文拓扑模型，在两个节点的keepalived配置文件中添加vrrp实例 \u0026ndash;\u0026gt; node1\nvrrp_instance VI_2 { state BACKUP interface ens37 virtual_router_id 61 \u0026lt;--注意同一台物理路由的虚拟路由ID唯一性 priority 95 advert_int 1 authentication { auth_type PASS auth_pass AhpaeQ0J } virtual_ipaddress { 172.16.255.68/16 dev ens37 \u0026lt;--指定VIP2 } track_script { chk_down chk_nginx } notify_master \u0026#34;/etc/keepalived/notify.sh master\u0026#34; notify_backup \u0026#34;/etc/keepalived/notify.sh backup\u0026#34; notify_fault \u0026#34;/etc/keepalived/notify.sh fault\u0026#34; } \u0026ndash;\u0026gt; node2\nvrrp_instance VI_2 { state MASTER interface ens33 virtual_router_id 61 priority 100 advert_int 1 authentication { auth_type PASS auth_pass AhpaeQ0J } virtual_ipaddress { 172.16.255.68/16 dev ens33 } track_script { chk_down chk_nginx } notify_master \u0026#34;/etc/keepalived/notify.sh master\u0026#34; notify_backup \u0026#34;/etc/keepalived/notify.sh backup\u0026#34; notify_fault \u0026#34;/etc/keepalived/notify.sh fault\u0026#34; } 启动node1、node2 keepalived服务 VIP1和VIP2分别映射在node1和node2上\n$ ip a list ens37 3: ens37: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:f1:c7:d5 brd ff:ff:ff:ff:ff:ff inet 172.16.253.93/16 brd 172.16.255.255 scope global dynamic ens37 valid_lft 79856sec preferred_lft 79856sec inet 172.16.255.168/16 scope global secondary ens37 valid_lft forever preferred_lft forever $ ip a list ens33 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:38:c5:21 brd ff:ff:ff:ff:ff:ff inet 172.16.251.171/16 brd 172.16.255.255 scope global dynamic ens33 valid_lft 39490sec preferred_lft 39490sec inet 172.16.255.68/16 scope global secondary ens33 valid_lft forever preferred_lft forever 访问测试，两台负载均衡器均处于active状态 $ while : ; do curl http://172.16.255.68 ;sleep 1 ;done backend server 1 backend server 2 backend server 1 backend server 2 $ while : ; do curl http://172.16.255.168 ;sleep 1 ;done backend server 1 backend server 2 backend server 1 backend server 2 手动停止node1 nginx服务，VIP1漂移到node2上，实现故障转移; \u0026ndash;\u0026gt; node2 同时映射VIP1、VIP2\n$ ip a list ens33 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:38:c5:21 brd ff:ff:ff:ff:ff:ff inet 172.16.251.171/16 brd 172.16.255.255 scope global dynamic ens33 valid_lft 38315sec preferred_lft 38315sec inet 172.16.255.68/16 scope global secondary ens33 valid_lft forever preferred_lft forever inet 172.16.255.168/16 scope global secondary ens33 valid_lft forever preferred_lft forever \u0026ndash;\u0026gt; 访问测试也可以看出故障转移过程\n$ while : ; do curl http://172.16.255.68 ;sleep 1 ;done backend server 2 backend server 1 curl: (7) Failed connect to 172.16.255.168:80; Connection refused curl: (7) Failed connect to 172.16.255.168:80; Connection refused backend server 2 backend server 1 同样的还可以测试node2的故障转移，至此单主、双主模型keepalived+nginx负载均衡高可用集群(两节点)已经实现。\n","date":"20 July 2016","permalink":"/2016/07/keepalived-nginx/","section":"博客","summary":"\u003ch3 class=\"relative group\"\u003ekeepalived简介 \n    \u003cdiv id=\"keepalived简介\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#keepalived%e7%ae%80%e4%bb%8b\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003ekeepalived是一款解决负载均衡(调度器)的高可用问题的软件，可以实现负载均衡器的故障转移(failover)，原生设计的目的为了高可用ipvs服务，后来增加了脚本调用接口，使其一定程度上具有了管控服务进程的功能，基于此接口也可以支持nginx、haproxy负载均衡的高可用。keepalived是VRRP协议的软件实现，也基于VRRP协议完成地址流动，那什么是VRRP协议呢？\u003c/p\u003e","title":"Keepalived+nginx LB高可用集群实现"},{"content":"","date":"3 June 2016","permalink":"/tags/lvs/","section":"Tags","summary":"","title":"LVS"},{"content":"","date":"3 June 2016","permalink":"/series/lvs/","section":"Series","summary":"","title":"LVS"},{"content":" 上篇关于 LVS-nat 模式的负载均衡实现博客中已经简单介绍了有关LVS集群的概念、四种IP负载均衡技术以及十种调度算法，本文将详细介绍LVS-DR(直接路由)模式的实现\n​ LVS-DR：Direct Routing, LVS默认模式, 通过为请求报文重新封装一个MAC首部进行转发，源MAC是DIP所在的接口的MAC，目标MAC是某挑选出的RS的RIP所在接口的MAC地址；源IP/PORT，以及目标IP/PORT均保持不变。请求报文同样要经由 Director，但与nat不同的是DR模式下的响应报文不经由 Director，而由RS直接发往Client\nDR模式调度流程 # 网络传输时，经过路由转发，会修改源mac为该转发路由的mac\nDirector和各RS都配置有VIP\n确保RS响应报文中源IP为VIP，进而走直接路由返回client\n解决地址冲突, 确保前端路由器将目标IP为VIP的请求报文发往Director,而不是RS，三种方法：\n在前端网关做静态绑定VIP和Director的MAC地址\n在RS上使用arptables工具\narptables -A IN -d VIP -j DROP arptables -A OUT -s VIP -j mangle \u0026ndash;mangle -ip -s RIP\n在各RS修改内核参数，来限制arp响应和通告的级别\n限制响应级别： arp_ignore\n1 \u0026ndash;\u0026gt; 仅在请求的目标IP配置在本地主机的接收到请求报文的接口上时，才给予响应\neg: echo 1 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_ignore\n限制通告级别：arp_announce\n2 \u0026ndash;\u0026gt; 必须避免将接口信息向非本网络进行通告\neg: echo 2 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_announce\nHTTP服务负载均衡实验 # 网络拓扑搭建 # 使用VMware搭建实验网络拓扑\nVIP与RIP可以在一个网段也可以不在同一网段。此实验中，VIP是与RIP在同一网段，前端路由临近的接口只用配一个地址，否则要额外配置一个vip网段的地址\n调度器Director要打开核心转发功能\n$ echo \u0026quot;net.ipv4.ip_forward = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf\n如果要启用sorry server, VS 的网关要指向前端路由临近的接口；否则，指向RIP所在网段任意IP皆可\nRS 网关要指向前端路由\n配置RS httpd服务并启动\nRS1 –\u0026gt;$ echo “RS1 SRV” \u0026gt; /var/www/html/index.html\nRS2 –\u0026gt;$ echo “RS2 SRV” \u0026gt; /var/www/html/index.html\n脚本实现RS配置\n$ bash lvs_dr_rs.sh start\n#!/bin/bash vip=192.168.196.186 dev=lo:1\t\u0026lt;-- 配置VIP在回环lo的别名上 case $1 in start) echo 1 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 \u0026gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 2 \u0026gt; /proc/sys/net/ipv4/conf/lo/arp_announce ifconfig $dev $vip netmask 255.255.255.255 broadcast $vip up ;; stop) ifconfig $dev down echo 0 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 \u0026gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 \u0026gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 0 \u0026gt; /proc/sys/net/ipv4/conf/lo/arp_announce ;; *) echo \u0026#34;Usage: $(basename $0) start|stop\u0026#34; exit 1 ;; esac DIRECTOR上管理集群服务并测试 # 脚本实现集群服务管理\n$ bash lvs_dr_vs.sh start\n#!/bin/bash vip=192.168.196.186 port=80 service=$vip:$port dev=ens33:1 RS1=192.168.196.155 RS2=192.168.196.196 scheduler=rr tp=\u0026#34;-g\u0026#34; case $1 in start) ifconfig $dev $vip/32 broadcast $vip up ipvsadm -A -t $service -s $scheduler ipvsadm -a -t $service -r $RS1 $tp -w 1 ipvsadm -a -t $service -r $RS2 $tp -w 1 ;; stop) ipvsadm -C ifconfig $dev down ;; *) echo \u0026#34;Usage $(basename $0) start|stopr\u0026#34;;exit 1 ;; esac 查看当前状态\n$ ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.196.186:80 rr -\u0026gt; 192.168.196.155:80 Route 1 0 0 -\u0026gt; 192.168.196.196:80 Route 1 0 1 客户端测试\n$ for i in {1..10}; do curl 192.168.196.186;sleep 1;done RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV ldirectord工具实现高可用的lvs # ldirectord：监控和控制LVS守护进程，可管理LVS规则 rpm包 \u0026ndash;\u0026gt; ldirectord-3.9.6-0rc1.1.1.x86_64.rpm\n$ rpm -ql ldirectord /etc/ha.d /etc/ha.d/resource.d /etc/ha.d/resource.d/ldirectord /etc/logrotate.d/ldirectord /usr/lib/ocf/resource.d/heartbeat/ldirectord /usr/lib/systemd/system/ldirectord.service /usr/sbin/ldirectord /usr/share/doc/ldirectord-3.9.6 /usr/share/doc/ldirectord-3.9.6/COPYING /usr/share/doc/ldirectord-3.9.6/ldirectord.cf \u0026lt;-- 配置文件模板 /usr/share/man/man8/ldirectord.8.gz $cp /usr/share/doc/ldirectord-3.9.6/ldirectord.cf /etc/ha.d 修改配置文件 # $ vim /etc/ha.d/ldirectord.cf checktimeout=3 \u0026lt;-- 响应超时时间 checkinterval=1 \u0026lt;-- 检查间隔时间 #fallback=127.0.0.1:80 #fallback6=[::1]:80 autoreload=yes \u0026lt;-- 自动读取加载配置文件 logfile=\u0026#34;/var/log/ldirectord.log\u0026#34; #logfile=\u0026#34;local0\u0026#34;\t\u0026lt;-- 指定rsyslog日志服务的facility名称 #emailalert=\u0026#34;admin@x.y.z\u0026#34; #emailalertfreq=3600 #emailalertstatus=all quiescent=no \u0026lt;-- no代表一旦宕机则从集群中移除RS；yes代表一旦宕机则将该RS权重设置为0 # Sample for an http virtual service \u0026lt;-- Director Server的配置(以上文实验配置为例) virtual=192.168.196.186:80 real=192.168.196.155:80 gate \u0026lt;-- gate 对应-g dr模式 real=192.168.196.196:80 gate fallback=127.0.0.1:80 gate \u0026lt;-- sorry server; RS全部宕机时的响应 service=http scheduler=rr #persistent=600 #netmask=255.255.255.255 protocol=tcp checktype=negotiate checkport=80 request=\u0026#34;index.html\u0026#34; receive=\u0026#34;SRV\u0026#34;\t\u0026lt;-- 抓取网页关键字，判断 $ echo \u0026#34;Sorry, server is down\u0026#34; \u0026gt; /var/www/html/index.html $ systemctl start httpd \u0026lt;-- 启动调度服务器的http服务 启动ldirector服务 # $ ifconfig ens33:1 192.168.196.186/32 broadcast 192.168.196.186 up \u0026lt;-- VS上手动添加VIP $ systemctl start ldirectord $ ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.196.186:80 rr -\u0026gt; 192.168.196.155:80 Route 1 0 0 -\u0026gt; 192.168.196.196:80 Route 1 0 0 客户端测试 # lvs集群已经实现轮询算法的调度 $ for i in {1..5}; do curl --connect-timeout 1 192.168.196.186;sleep 1; done RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV 停止RS1的http服务模拟宕机\nVS上查看集群服务状态\n$ ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.196.186:80 rr -\u0026gt; 192.168.196.196:80 Route 1 0 0 \u0026lt;--自动移除宕机的RS1 客户端测试显示RS1宕机后，DR将请求自动调度到RS2上\n$ for i in {1..5}; do curl --connect-timeout 1 192.168.196.186;sleep 1; done RS2 SRV RS1 SRV RS2 SRV RS2 SRV \u0026lt;-- 此时RS1 http服务停止 RS2 SRV 停止所有RS的http服务\nVS上查看集群服务状态\n$ ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.196.186:80 rr -\u0026gt; 127.0.0.1:80 Route 1 0 0 \u0026lt;--自动启用本机sorry server 客户端测试结果\n[root@Centos7 ~]#for i in {1..100}; do curl --connect-timeout 1 192.168.196.186;sleep 1; done RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS1 SRV \u0026lt;--RS2宕机 RS1 SRV sorry,server is down \u0026lt;--RS1同时宕机，启用sorry server sorry,server is down ","date":"3 June 2016","permalink":"/2016/06/lvs-dr/","section":"博客","summary":"上篇关于 LVS-nat 模式的负载均衡实现博客中已经简单介绍了有关LVS集群的概念、四种IP负载均衡技术以及十种调度算法，本文将详细介绍LVS-DR(直接路由)模式的实现\n​ LVS-DR：Direct Routing, LVS默认模式, 通过为请求报文重新封装一个MAC首部进行转发，源MAC是DIP所在的接口的MAC，目标MAC是某挑选出的RS的RIP所在接口的MAC地址；源IP/PORT，以及目标IP/PORT均保持不变。请求报文同样要经由 Director，但与nat不同的是DR模式下的响应报文不经由 Director，而由RS直接发往Client\nDR模式调度流程 # 网络传输时，经过路由转发，会修改源mac为该转发路由的mac\nDirector和各RS都配置有VIP\n确保RS响应报文中源IP为VIP，进而走直接路由返回client\n解决地址冲突, 确保前端路由器将目标IP为VIP的请求报文发往Director,而不是RS，三种方法：\n在前端网关做静态绑定VIP和Director的MAC地址\n在RS上使用arptables工具\narptables -A IN -d VIP -j DROP arptables -A OUT -s VIP -j mangle \u0026ndash;mangle -ip -s RIP","title":"LVS DR 负载均衡实现"},{"content":"什么是LVS # LVS: Linux Virtual Server，即Linux虚拟服务器，是一个虚拟的服务器集群系统\nLVS集群由VS(Virtual Server)负责调度,RS(Real Server)负责真正提供服务。VS根据请求报文的目标IP和目标协议及端口将其调度转发至某RS，根据调度算法来挑选RS，实现负载均衡，而且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。 lvs集群的类型：\nlvs-nat：多目标IP的DNAT(Network Address Translation)，通过将请求报文中的目标地址和目标端口修改为某挑出的RS的RIP和PORT实现转发\u0026mdash;\u0026ndash;本文将对nat模式及其实现做详细解释 lvs-dr：Direct Routing, LVS默认模式, 通过为请求报文重新封装一个MAC首部进行转发，源MAC是DIP所在的接口的MAC，目标MAC是某挑选出的RS的RIP所在接口的MAC地址；源IP/PORT，以及目标IP/PORT均保持不变 lvs-tun：不修改请求报文的IP首部, 在原请求IP报文之外封装一个IP首部, 将报文发往挑选出的目标RS,RS直接响应给客户端 lvs-fullnat：通过同时修改请求报文的源IP地址和目标IP地址进行转发,此类型kernel默认不支持 ipvsadm与ipvs：\nipvsadm是用户空间的命令行工具，规则管理器，实现用户管理集群服务及RealServer 管理集群服务：增、改、删 增、改：\nipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]]\n删除：\nipvsadm -D -t|u|f service-address\n管理集群上的RS：增、改、删 增、改：\nipvsadm -a|e -t|u|f service-address -r server-address [-g|i|m] [ -w weight] -g: gateway, dr类型，默认 -i: ipip, tun类型 -m: masquerade, nat类型\n删：\nipvsadm -d -t|u|f service-address -r serveraddress\nipvs是工作于内核空间netfilter的INPUT钩子上的框架，此框架使调度器具备调度转发功能，而调度算法又决定挑选哪个RS进行转发 ipvs 调度算法： 根据其调度时是否考虑各RS当前的负载状态，分为静态方法与动态方法\n静态方法：仅根据算法本身进行调度 动态方法：主要根据每RS当前的负载状态及调度算法进行调度Overhead=value 较小的RS将被调度 LVS-NAT 模式负载均衡的实现 # NAT模式IP调度流程 # 客户端向调度器(Director Server,VS)发送请求报文, Director通过将请求报文中的目标地址和目标端口修改为某挑出的RS的RIP和PORT实现转发。此模式下请求报文和响应报文都必须经由Director，调度器易于成为系统瓶颈\nhttp服务负载均衡实验\u0026ndash;网络拓扑搭建 # 使用VMware搭建实验网络拓扑\n调度器Director要打开核心转发功能\n$ echo \u0026quot;net.ipv4.ip_forward = 1\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf\nRS 网关要指向DIP\n配置RS httpd服务并启动\nRS1 \u0026ndash;\u0026gt;$ echo “RS1 SRV” \u0026gt; /var/www/html/index.html\nRS2 \u0026ndash;\u0026gt;$ echo “RS2 SRV” \u0026gt; /var/www/html/index.html\nDirector上管理集群服务并测试 # 添加集群服务\n$ ipvsadm -A -t 172.16.253.153:80 -s rr -- -s 指定算法为rr轮询 添加集群的RS\n$ ipvsadm -a -t 172.16.253.153:80 -r 192.168.196.155 -m -- -m指定为nat模式 $ ipvsadm -a -t 172.16.253.153:80 -r 192.168.196.196 -m $ ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.16.253.153:80 rr -\u0026gt; 192.168.196.155:80 Masq 1 0 0 -\u0026gt; 192.168.196.196:80 Masq 1 0 0 客户端测试\n$ for i in {1..10}; do curl 172.16.253.153;sleep 1;done RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV RS1 SRV RS2 SRV 从测试结果可以看出，客户端访问VS，调度器按照轮询算法依次转发给RS1、RS2\n本文的实验只演示了RR算法，还有其他算法可以选择，同时LVS-nat模式还支持端口映射，以httpd集群服务为例，RS的服务端口也可以是自定义端口(eg: 8080)。后续会更新 关于LVS-DR模式、集群服务合并及LVS高可用性的实现\n","date":"16 May 2016","permalink":"/2016/05/lvs-nat/","section":"博客","summary":"什么是LVS # LVS: Linux Virtual Server，即Linux虚拟服务器，是一个虚拟的服务器集群系统\nLVS集群由VS(Virtual Server)负责调度,RS(Real Server)负责真正提供服务。VS根据请求报文的目标IP和目标协议及端口将其调度转发至某RS，根据调度算法来挑选RS，实现负载均衡，而且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。 lvs集群的类型：\nlvs-nat：多目标IP的DNAT(Network Address Translation)，通过将请求报文中的目标地址和目标端口修改为某挑出的RS的RIP和PORT实现转发\u0026mdash;\u0026ndash;本文将对nat模式及其实现做详细解释 lvs-dr：Direct Routing, LVS默认模式, 通过为请求报文重新封装一个MAC首部进行转发，源MAC是DIP所在的接口的MAC，目标MAC是某挑选出的RS的RIP所在接口的MAC地址；源IP/PORT，以及目标IP/PORT均保持不变 lvs-tun：不修改请求报文的IP首部, 在原请求IP报文之外封装一个IP首部, 将报文发往挑选出的目标RS,RS直接响应给客户端 lvs-fullnat：通过同时修改请求报文的源IP地址和目标IP地址进行转发,此类型kernel默认不支持 ipvsadm与ipvs：\nipvsadm是用户空间的命令行工具，规则管理器，实现用户管理集群服务及RealServer 管理集群服务：增、改、删 增、改：\nipvsadm -A|E -t|u|f service-address [-s scheduler] [-p [timeout]]","title":"LVS NAT 负载均衡的实现"},{"content":"CA及证书 # 非对称加密是为了保证互联网中通讯信息安全使用的一种算法，密钥是成对出现(公钥和私钥），它的特点是发送方A使用接收方B的公钥加密数据，所有只有B拥有与之配对的私钥解密该数据，反之亦然。那么，A和B之间怎么交换得到对方的真实安全的公钥呢？此时就需要一个权威的机构来验证公钥的合法性，这个机构称之为CA(Certification Authority)。CA为每个使用公开密钥的客户发放数字证书，数字证书的作用是证明证书中列出的客户合法拥有证书中列出的公开密钥。\n获取证书两种方法 # 使用证书授权机构： # 生成签名请求(csr) –\u0026gt;将csr发送给CA –\u0026gt; 从CA处接收签名 。参考下图： CA证书颁发过程(假设只有一级CA) 很多权威的根CA会被内置到操作系统里面，用户安装系统之后也就会拥有根CA的公钥，所以可以获得上级CA的公钥，进而可以申请证书。参考下图：主机通过RootCA获得上级CA的公钥\n自签名的证书 # OpenSSL是一个免费开源的库，它提供了构建数字证书的命令行工具，其中一些可以用来自建RootCA并签发自己的公钥\n1.创建私有CA # 创建之前要了解一下openssl的配置文件： /etc/pki/tls/openssl.cnf\n[ ca ] default_ca = CA_default # The default ca section \u0026lt;--启用的CA名字 [ CA_default ] dir = /etc/pki/CA # Where everything is kept \u0026lt;--相关文件存放目录 certs = $dir/certs # Where the issued certs are kept \u0026lt;--存档颁发证书文件 crl_dir = $dir/crl # Where the issued crl are kept \u0026lt;--吊销证书列表 database = $dir/index.txt # database index file. \u0026lt;--证书索引数据库 #unique_subject = no # Set to \u0026#39;no\u0026#39; to allow creation of \u0026lt;--是否允许创建具有相同主题的多个证书 # several certificates with same subject. new_certs_dir = $dir/newcerts # default place for new certs. certificate = $dir/cacert.pem # The CA certificate \u0026lt;--自签名的证书 serial = $dir/serial # The current serial number \u0026lt;--当前可用的序列号(下一个要颁发证书的序列号) crlnumber = $dir/crlnumber # the current crl number \u0026lt;--吊销证书编号 # must be commented out to leave a V1 CRL crl = $dir/crl.pem # The current CRL private_key = $dir/private/cakey.pem# The private key \u0026lt;--CA的私钥文件 RANDFILE = $dir/private/.rand # private random number file default_days = 365 # how long to certify for \u0026lt;--证书有效期 default_crl_days= 30 # how long before next CRL \u0026lt;--发布吊销证书列表周期 default_md = sha256 # use SHA-256 by default \u0026lt;--算法 policy = policy_match \u0026lt;--使用哪个策略 # For the CA policy [ policy_match ] countryName = match \u0026lt;--CA与客户端的申请信息必须一致 stateOrProvinceName = match organizationName = match organizationalUnitName = optional \u0026lt;--可填可不填 commonName = supplied \u0026lt;--必须填 emailAddress = optional # For the \u0026#39;anything\u0026#39; policy # At this point in time, you must list all acceptable \u0026#39;object\u0026#39; # types. [ policy_anything ] countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional a.在CentOS7上创建CA的私钥 # [root@centos7 ~]#(umask 066;openssl genrsa -out /etc/pki/CA/private/cakey.pem 2048) \u0026lt;--私钥文件只对属主有权限 Generating RSA private key, 2048 bit long modulus ...+++ .............+++ e is 65537 (0x10001) [root@centos7 ~]#tree /etc/pki/CA /etc/pki/CA ├── certs ├── crl ├── newcerts └── private └── cakey.pem 4 directories, 1 file b.生成自签名证书 # [root@centos7 ~]#openssl req -new -x509 \\ \u0026lt;-- -x509 专用于CA生成自签证书 \u0026gt; -key /etc/pki/CA/private/cakey.pem \\ \u0026lt;-- 生成请求时用到的私钥文件 \u0026gt; -out /etc/pki/CA/cacert.pem \\ \u0026lt;-- 证书的保存路径 \u0026gt; -days 365 \u0026lt;-- 证书的有效期限 You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:BeiJing Locality Name (eg, city) [Default City]:BeiJing Organization Name (eg, company) [Default Company Ltd]:ffu Organizational Unit Name (eg, section) []:IT Common Name (eg, your name or your server\u0026#39;s hostname) []:ca.ffu.com Email Address []:ffu@outlook.com c.查看自签名证书信息 # [root@centos7 ~]#openssl x509 -in /etc/pki/CA/cacert.pem -noout -text Certificate: Data: Version: 3 (0x2) Serial Number: 14141409927417363425 (0xc440616792e4fbe1) Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=BeiJing, L=BeiJing, O=ffu, OU=IT, CN=ca.ffu.com/emailAddress=ffu@outlook.com Validity Not Before: Jul 16 08:57:27 2017 GMT Not After : Jul 16 08:57:27 2018 GMT Subject: C=CN, ST=BeiJing, L=BeiJing, O=ffu, OU=IT, CN=ca.ffu.com/emailAddress=ffu@outlook.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) ....后面省略.... d.创建所需数据库文件 # [root@centos7 CA]#touch /etc/pki/CA/index.txt \u0026lt;--生成证书索引数据库文件 [root@centos7 CA]#echo 01 \u0026gt; /etc/pki/CA/serial \u0026lt;--指定第一个颁发证书的序列号;十六进制，必须是两位数 2.颁发证书 # a.生成CentOS6主机的私钥 # [root@centos6 ~]#(umask 066;openssl genrsa -out /app/service.key 2048) Generating RSA private key, 2048 bit long modulus .............+++ .................................+++ e is 65537 (0x10001) b.生成证书申请文件 # [root@centos6 app]#openssl req -new -key /app/service.key -out /app/service.csr You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [XX]:CN \u0026lt;--按照所选policy，必须和申请CA的信息一致 State or Province Name (full name) []:BeiJing \u0026lt;--按照所选policy，必须和申请CA的信息一致 Locality Name (eg, city) [Default City]:Zhengzhou Organization Name (eg, company) [Default Company Ltd]:ffu \u0026lt;--按照所选policy，必须和申请CA的信息一致 Organizational Unit Name (eg, section) []:cs Common Name (eg, your name or your server\u0026#39;s hostname) []:*.ffu.com Email Address []: Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: An optional company name []: c.将证书请求文件传输给CA # [root@centos6 app]#scp service.csr 192.168.196.166:/etc/pki/CA/ d.CA签署证书，并将证书颁发给请求者 # [root@centos7 CA]#openssl ca -in /etc/pki/CA/service.csr -out /etc/pki/CA/certs/service.crt -days 100 Using configuration from /etc/pki/tls/openssl.cnf Check that the request matches the signature Signature ok Certificate Details: Serial Number: 1 (0x1) Validity Not Before: Jul 16 09:44:51 2017 GMT Not After : Oct 24 09:44:51 2017 GMT Subject: countryName = CN stateOrProvinceName = BeiJing organizationName = ffu organizationalUnitName = cs commonName = *.ffu.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE Netscape Comment: OpenSSL Generated Certificate X509v3 Subject Key Identifier: 89:01:83:51:84:C8:1F:A9:1F:E7:F5:60:6E:6E:5D:5A:2B:59:5A:F2 X509v3 Authority Key Identifier: keyid:A9:5F:1B:D6:F6:7E:99:5D:2F:EE:7D:40:F7:DA:61:AE:29:EE:D1:6F Certificate is to be certified until Oct 24 09:44:51 2017 GMT (100 days) Sign the certificate? [y/n]:y 1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated [root@centos7 CA]#ll certs/service.crt newcerts/01.pem -rw-r--r--. 1 root root 4456 Jul 16 17:45 certs/service.crt -rw-r--r--. 1 root root 4456 Jul 16 17:45 newcerts/01.pem \u0026lt;--自动生成以证书序列号命名的文件，内容与证书一致 [root@centos7 CA]#cat index.txt serial V 171024094451Z 01 unknown /C=CN/ST=BeiJing/O=ffu/OU=cs/CN=ffu \u0026lt;--自动生成数据库 02 \u0026lt;--自动更新下一个颁发证书的序列号 然后，CA就可以把证书发送给主机，主机相关Web服务就可以使用了\n3.如何吊销证书 # a.在客户端上先查看证书serial # openssl x509 -in /etc/pki/CA/service.crt -noout -text\nb.吊销证书 # 在CA上，根据客户提交的serial与subject信息，对比检验是否与index.txt文件中的信息一致，吊销证书\n[root@centos7 CA]#openssl ca -revoke /etc/pki/CA/newcerts/01.pem Using configuration from /etc/pki/tls/openssl.cnf Revoking Certificate 01. Data Base Updated [root@centos7 CA]#cat index.txt R 171024094451Z 170716112929Z 01 unknown /C=CN/ST=BeiJing/O=ffu/OU=cs/CN=ffu \u0026lt;--R代表removed c.指定第一个吊销证书的编号 # [root@centos7 CA]#echo 01 \u0026gt; /etc/pki/CA/crlnumber \u0026lt;--第一次更新证书吊销列表前，才需要执行 d.更新证书吊销列表 # [root@centos7 CA]#openssl ca -gencrl -out /etc/pki/CA/crl/crl.pem Using configuration from /etc/pki/tls/openssl.cnf [root@centos7 CA]#cat crlnumber 02 \u0026lt;--自动更新下一个吊销证书的序列号 [root@centos7 CA]#openssl crl -in /etc/pki/CA/crl/crl.pem -noout -text \u0026lt;--查看吊销证书文件详情 Certificate Revocation List (CRL): Version 2 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: /C=CN/ST=BeiJing/L=BeiJing/O=ffu/OU=IT/CN=ffu/emailAddress=ffu@outloo.co Last Update: Jul 16 11:35:48 2017 GMT Next Update: Aug 15 11:35:48 2017 GMT CRL extensions: X509v3 CRL Number: 1 Revoked Certificates: Serial Number: 01 Revocation Date: Jul 16 11:29:29 2017 GMT Signature Algorithm: sha256WithRSAEncryption .....后面省略..... ","date":"20 April 2016","permalink":"/2016/04/openssl-ca/","section":"博客","summary":"CA及证书 # 非对称加密是为了保证互联网中通讯信息安全使用的一种算法，密钥是成对出现(公钥和私钥），它的特点是发送方A使用接收方B的公钥加密数据，所有只有B拥有与之配对的私钥解密该数据，反之亦然。那么，A和B之间怎么交换得到对方的真实安全的公钥呢？此时就需要一个权威的机构来验证公钥的合法性，这个机构称之为CA(Certification Authority)。CA为每个使用公开密钥的客户发放数字证书，数字证书的作用是证明证书中列出的客户合法拥有证书中列出的公开密钥。\n获取证书两种方法 # 使用证书授权机构： # 生成签名请求(csr) –\u0026gt;将csr发送给CA –\u0026gt; 从CA处接收签名 。参考下图： CA证书颁发过程(假设只有一级CA) 很多权威的根CA会被内置到操作系统里面，用户安装系统之后也就会拥有根CA的公钥，所以可以获得上级CA的公钥，进而可以申请证书。参考下图：主机通过RootCA获得上级CA的公钥\n自签名的证书 # OpenSSL是一个免费开源的库，它提供了构建数字证书的命令行工具，其中一些可以用来自建RootCA并签发自己的公钥\n1.创建私有CA # 创建之前要了解一下openssl的配置文件： /etc/pki/tls/openssl.cnf\n[ ca ] default_ca = CA_default # The default ca section \u0026lt;--启用的CA名字 [ CA_default ] dir = /etc/pki/CA # Where everything is kept \u0026lt;--相关文件存放目录 certs = $dir/certs # Where the issued certs are kept \u0026lt;--存档颁发证书文件 crl_dir = $dir/crl # Where the issued crl are kept \u0026lt;--吊销证书列表 database = $dir/index.","title":"如何使用openssl工具创建私有CA"},{"content":"","date":"29 March 2016","permalink":"/tags/pxe/","section":"Tags","summary":"","title":"PXE"},{"content":"PXE介绍 # PXE： Preboot Excution Environment，由Intel公司研发，可以使没有任何操作系统的主机能够基于网络完成系统的安装工作，实现服务器的自动化安装系统\nPXE工作原理 # Client向PXE Server上的DHCP发送IP地址请求消息，DHCP检测Client是否合法（主要是检测Client的网卡MAC地址），如果合法则返回Client的IP地址，同时将启动文件pxelinux.0的位置信息一并传送给Client Client向PXE Server上的TFTP发送获取pxelinux.0请求消息， TFTP接收到消息之后再向Client发送pxelinux.0大小信息，试探Client是否满意，当TFTP收到Client发回的同意大小信息之后，正式向Client发送pxelinux.0 Client执行接收到的pxelinux.0文件 Client向TFTP Server发送针对本机的配置信息文件（在TFTP 服务的pxelinux.cfg目录下）， TFTP将配置文件发回Client，继而Client根据配置文件执行后续操作 Client向TFTP发送Linux内核请求信息， TFTP接收到消息之后将内核文件发送给Client Client向TFTP发送根文件请求信息， TFTP接收到消息之后返回Linux根文件系统 Client启动Linux内核 Client下载安装源文件，读取自动化安装脚本 PXE Server 的配置 # 1. DHCP服务的配置 # ​\t安装包：dhcp-4.2.5-47.el7.centos.x86_64\n[root@centos7 ~]#cp /usr/share/doc/dhcp*/dhcpd.conf.example /etc/dhcp/dhcpd.conf --\u0026gt; 使用模本配置文件 [root@centos7 ~]#vi /etc/dhcp/dhcpd.conf subnet 192.168.196.0 netmask 255.255.255.0 { --\u0026gt; 定义网段 range 192.168.196.10 192.168.196.40; --\u0026gt; 定义分配IP范围 option routers 192.168.196.1; --\u0026gt; 配置路由 filename \u0026#34;pxelinux.0\u0026#34;; --\u0026gt; 启动文件名称 next-server 192.168.196.188; --\u0026gt; dhcp服务器地址 } [root@centos7 ~]#systemctl start dhcpd [root@centos7 ~]#systemctl enable dhcpd 配置完成，并设置DHCP静态IP192.168.196.188。目标主机开机获取IP后，可以访问PXE Sever\n2. tftp、httpd服务的配置 # ​\t安装包：tftp-5.2-13.el7.x86_64 httpd-2.4.6-45.el7.centos.x86_64\n[root@Centos7 ~]#yum -q -y tftp [root@Centos7 ~]#yum -q -y httpd [root@Centos7 ~]#systemctl start tftpd.socket httpd [root@Centos7 ~]#systemctl enable tftp.socket [root@Centos7 ~]#systemctl enable httpd 注意：centos6里tftp是由xinetd服务代理的，需要先安装xinetd服务，再chkconfig tftp on 命令开启tftp服务\n3. 关闭SElinux、防火墙 # [root@Centos7 ~]#setenforce 0 [root@Centos7 ~]#vi /etc/selinux/config --\u0026gt; 修改配置文件 SELINUX=permissive [root@Centos7 ~]#systemctl stop firewalld [root@Centos7 ~]#systemctl disable firewalld 4.kickstart应答文件准备 # [root@centos7 ~]#mkdir /var/www/html/{centos7,ks} --\u0026gt; PXE Sever http服务会提供网络源于ks文件 [root@centos7 ~]#cp ~/anaconda-ks.cfg /var/www/html/ks/centos7-m.cfg --\u0026gt; 以系统安装ks文件为范本 [root@centos7 ks]#vi centos7-m.cfg #version=DEVEL # System authorization information auth --enableshadow --passalgo=sha512 url --url=http://192.168.196.188/centos7/ --\u0026gt; 网络源由PXE Sever http服务提供 text --\u0026gt; 文本界面安装 # Run the Setup Agent on first boot firstboot --disable reboot --\u0026gt; 安装完成后重启 ignoredisk --only-use=sda # Keyboard layouts keyboard --vckeymap=us --xlayouts=\u0026#39;us\u0026#39; # System language lang en_US.UTF-8 # Network information network --bootproto=dhcp --device=ens33 --onboot=on --ipv6=auto --activate --\u0026gt;网卡配置 network --hostname=centos7.ffu.com # Root password rootpw --iscrypted $6$8sIxlzOyC1RQ4g.X$UXgct3LHH67rVNH5StXAall/82K/5OkK6.QijHagr.DB4zw8IbnI0CYIUUAhTSDa.dfVfnZabRVZ8nFq5Cc1c1 # System services services --disabled=\u0026#34;chronyd\u0026#34; # System timezone timezone Asia/Shanghai --isUtc --nontp # System bootloader configuration bootloader --append=\u0026#34; crashkernel=auto\u0026#34; --location=mbr --boot-drive=sda # Partition clearing information clearpart --none --initlabel # Disk partitioning information part swap --fstype=\u0026#34;swap\u0026#34; --ondisk=sda --size=2048 part /app --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=50000 part / --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=100000 part /boot --fstype=\u0026#34;ext4\u0026#34; --ondisk=sda --size=1000 %packages --\u0026gt; 安装包选择 @^minimal @core %end --\u0026gt;各个模块要以%end结束 %addon com_redhat_kdump --enable --reserve-mb=\u0026#39;auto\u0026#39; %end %post --\u0026gt;安装完成后运行自定义脚本 useradd ffu echo 123456|passwd --stdin ffu rm -rf /etc/yum.repos.d/* cat \u0026gt; /etc/yum.repos.d/base.repo \u0026lt;\u0026lt; EOF [base] name=base baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/ gpgcheck=0 EOF %end %anaconda pwpolicy root --minlen=6 --minquality=50 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=50 --notstrict --nochanges --notempty pwpolicy luks --minlen=6 --minquality=50 --notstrict --nochanges --notempty %end [root@centos7 ks]#chmod a+r centos7-m.cfg --\u0026gt; 为other加读权限，以便能通过http访问 [root@centos7 ks]#ll -rw-r--r--. 1 root root 2042 Jul 24 15:00 centos7-m.cfg 5.安装源准备 # [root@centos7 ks]# mount /dev/sr0 /var/www/html/centos7/ 6.启动菜单、内核等相关文件准备 # a.相关文件准备 # [root@centos7 tftpboot]#mkdir /var/lib/tftpboot/pxelinux.cfg [root@centos7 tftpboot]#cp /var/www/html/centos7/isolinux/isolinux.cfg pxelinux.cfg/default --\u0026gt;复制安装光盘启动菜单并重命名 [root@centos7 tftpboot]#cp /usr/share/syslinux/menu.c32 /usr/share/syslinux/pxelinux.0 ./ --\u0026gt;复制syslinux的菜单及启动文件 [root@centos7 tftpboot]#cp /var/www/html/centos7/isolinux/vmlinuz /var/www/html/centos7/isolinux/initrd.img ./ [root@centos7 tftpboot]#tree --\u0026gt;目录结构 . ├── initrd.img ├── menu.c32 ├── pxelinux.0 ├── pxelinux.cfg │ └── default └── vmlinuz b.启动菜单定制 # [root@centos7 tftpboot]#vi pxelinux.cfg/default default menu.c32 --\u0026gt;所用菜单文件 timeout 600 menu title PXE CentOS Linux 7 Install Menu label linux-mini menu label ^Auto-install CentOS Linux 7 Mini kernel vmlinuz append initrd=initrd.img ks=http://192.168.196.188/ks/centos7-m.cfg --\u0026gt;ks文件由PXE Sever http服务提供 label linux-desktop menu label Test Auto-install CentOS Linux 7 ^Desktop kernel vmlinuz append initrd=initrd.img ks=http://192.168.196.188/ks/centos7.cfg --\u0026gt;可根据不同目的定制不同应答文件 label linux-manual menu label Test ^Manual-Install CentOS Linux 7 kernel vmlinuz append initrd=initrd.img label local menu label Boot from ^local drive menu default localboot 0xffff 实现主机自动化安装系统 # 可以使用VMware进行实验，根据以上步骤配置PXE Server,然后新建虚拟机，开机进入启动菜单，选择最小化安装，安装完成显示登录界面。\n注意需要把本地DHCP获取IP服务关闭\n","date":"29 March 2016","permalink":"/2016/03/pxe/","section":"博客","summary":"PXE介绍 # PXE： Preboot Excution Environment，由Intel公司研发，可以使没有任何操作系统的主机能够基于网络完成系统的安装工作，实现服务器的自动化安装系统\nPXE工作原理 # Client向PXE Server上的DHCP发送IP地址请求消息，DHCP检测Client是否合法（主要是检测Client的网卡MAC地址），如果合法则返回Client的IP地址，同时将启动文件pxelinux.0的位置信息一并传送给Client Client向PXE Server上的TFTP发送获取pxelinux.0请求消息， TFTP接收到消息之后再向Client发送pxelinux.0大小信息，试探Client是否满意，当TFTP收到Client发回的同意大小信息之后，正式向Client发送pxelinux.0 Client执行接收到的pxelinux.0文件 Client向TFTP Server发送针对本机的配置信息文件（在TFTP 服务的pxelinux.cfg目录下）， TFTP将配置文件发回Client，继而Client根据配置文件执行后续操作 Client向TFTP发送Linux内核请求信息， TFTP接收到消息之后将内核文件发送给Client Client向TFTP发送根文件请求信息， TFTP接收到消息之后返回Linux根文件系统 Client启动Linux内核 Client下载安装源文件，读取自动化安装脚本 PXE Server 的配置 # 1. DHCP服务的配置 # ​\t安装包：dhcp-4.","title":"PXE自动化安装CentOS7"},{"content":"","date":"2 March 2016","permalink":"/tags/yum/","section":"Tags","summary":"","title":"yum"},{"content":"什么是yum # 我们在Linux系统上安装处理软件，一般是使用RPM，它是通过预先编译完成并且把软件打包为RPM文件格式后，再加以安装的一种方式，使用者只要拿到这个打包好的软件，然后将里头的文件放置到应该摆放的目录，这样就完成了安装。但是，由于有些软件是有依赖于其他软件的，当你要安装某个RPM类型的软件时，RPM会检验RPM软件数据库，它所依赖的相关软件包是否都已存在，如果没有检索到，那么这个RPM文件默认就不能安装。甚至是有些包之间还会存在循环依赖，这时RPM就不能快速有效的进行软件安装了。 对于RPM的上述局限性，yum的出现就解决了包之间的依赖性的问题。文章前面提到了，RPM把软件依赖关系储存在本地数据库里，那我们在安装软件的时候，如果先到数据库里找到所有依赖包的列表，再检索哪些已经安装到本地，然后把剩下没安装的一起安装，这样就可以解决包依赖性的问题了，这就是yum机制的由来\nyum的运作流程 # 各版本发行商都会释放出软件并放置于yum服务器上，所以yum服务器储存有我们各种所需的软件。参考下图，yum服务器不仅存储了各种RPM包，还有包的相关的元数据文件（放置于特定目录repodata下），前面提到的包的依赖性关系就储存在元数据文件中。這些文件与RPM软件包所在的本地或网络位置就被称为yum仓库(yum repo)。当用户端有软件安装或升级的需求时，用户端会访问yum服务器下载或更新RPM软件列表并存在本机缓存列表中，然后通过缓存列表与本地RPM数据库相比对，筛选出缺少哪些RPM软件包并根据yum仓库储存的路径下载（可以是本地，也可以是网络），最后通过RPM机制一并进行安装。 如何配置yum客户端 # yum本身的配置文件，主要指向仓库的位置以及相关的各种配置信息。\n主配置文件 # /etc/yum.conf：为所有仓库提供公共配置\n各仓库指向的定义 # /etc/yum.repos.d/*.repo：为仓库的指向提供配置\n* [repositoryID] #仓库的名字，具有唯一性，标识repo的指向 * name=Some name for this repository #仓库描述信息 * baseurl=url://path/to/repository/ #指明repo的访问路径，通常为一个文件服务器上输出的某repo #文件服务器类型 http://SEVER/PATH/TO/REPOSITORY https://SEVER/PATH/TO/REPOSITORY ftp://SEVER/PATH/TO/REPOSITORY file:///PATH/TO/REPOSITORY * enabled={1|0} #此仓库是否可被使用 * gpgcheck={1|0} #是否对RPM包做检验 * gpgkey=url://path/to/keyfile/ #指明gpgkey文件路径 * enablegroups={1|0} #是否启用包组 * failovermethod={roundrobin|priority} #设置baseurl有多个时的优先级 roundrobin：随机挑选，默认值 priority:按顺序访问 * cost= #指明仓库的访问开销，默认为1000 创建新的配置文件 # 按照以上方式，我们就在/etc/yum.repos.d/目录下创建newbase.repo的配置文件（这里要注意目录下的配置文件只要有一个有问题就会影响yum的使用）\n[root@centos7 ~]#vi /etc/yum.repos.d/newbase.repo [NewBase] name=NewBase baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/ #使用阿里云CentOS系统yum源 file:///misc/cd/ #使用本地光盘作为yum源 enable=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 #使用本地文件，也可以使用文件服务器上的，https://mirrors.aliyun.com/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7 failovermethod={priority} #按顺序访问baseurl路径 对于CentOS来说，也支持由Fedora基金会发展的外加软件计划(Extra Packages for Enterprise Linux, EPEL)，包含了很多第三方软件。我们可以使用阿里云的Epel的yum源：https://mirrors.aliyun.com/epel/7/x86_64，只需在newbase.repo配置文件中创建另一个yum仓库：\n[NewEpel] name=NewEpel baseurl=https://mirrors.aliyun.com/epel/7/x86_64/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/rpm-gpg-key-epel-7 yum仓库的测试 # yum repolist: 列出已经配置的所有yum仓库\n用法：yum repolist [all|enabled|disabled]\n[root@centos7 ~]#yum repolist all Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile * NewBase: repo id repo name status !Centos7 Centos7 disabled NewBase NewBase enabled: 9,363 NewEpel NewEpel enabled: 11,787 !epel7 epel7 disabled repolist: 21,150 # NewBase和NewEpel两个yum仓库的status都为enabled，说明已经成功激活 yum命令的用法 # yum [options][command] [package …] Command:\n显示服务器提供的程序包： # yum list\nyum list [all | glob_exp1][glob_exp2] […] #默认为all,显示全部RPM包，也支持glob通配符\nyum list {available|installed|updates} [glob_exp1][…]\n[root@centos7 ~]#yum list|less Installed Packages #已安装软件 GConf2.x86_64 3.2.6-8.el7 @anaconda #随安装向导安装的 apr.x86_64 1.4.8-3.el7 @Centos7 #通过Centos7yum仓库安装 .....中间省略....... Available Packages #还可以安装的软件 0ad.x86_64 0.0.21-1.el7 NewEpel #位置 389-ds-base.x86_64 1.3.5.10-11.el7 NewBase #位置 .....(底下省略)..... yum list updates 目前服务器上可供本机进行升级的软件有哪些，与yum check-update 类似\n安装程序包 # yum install package1 [package2][…] (包名) ​\t如果一个包在不同仓库中有多个版本，默认会安装最新版本 ​\t如果要安装制定版本：install PACKAGE-VERSION… yum reinstall package1 [package2][…] (重新安装)\n升级程序包 # yum update [package1][package2] […]\nyum downgrade package1 [package2][…] (降级)\n卸载程序包 # yum remove | erase package1 [package2][…]\n此命令默认是不会卸载所依赖的包,但是依赖于正卸载的程序包的程序包会被一并卸载\n查看程序包information # yum info […]\n查看指定的特性(可以是某文件)是由哪个程序包所提供 # yum provides | whatprovides feature1 [feature2][…]\n[root@centos7 ~]#yum provides /usr/bin/bash Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile * NewBase: bash-4.2.46-20.el7_2.x86_64 : The GNU Bourne Again shell Repo : NewBase Matched from: Filename : /usr/bin/bash #bash的可执行程序文件是由bash-4.2.46-20.el7_2.x86_64这个RPM包提供的 缓存管理 # 清理本地缓存： 在yum运作流程中我们提到了客户端存有缓存列表，这些文件默认是存在/var/cache/yum/目录下\n[root@centos7 ~]#ls /var/cache/yum/x86_64/7/ #本机yum的缓存位置 base Centos7 epel7 extras NewBase NewEpel timedhosts timedhosts.txt updates yum clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]\n[root@centos7 ~]#yum clean all Loaded plugins: fastestmirror, langpacks Cleaning repos: NewBase NewEpel Cleaning up everything Cleaning up list of fastest mirrors 构建缓存： yum makecache #自动连接至每一个可用仓库，下载数据，创建为缓存\n[root@centos7 ~]#yum makecache Loaded plugins: fastestmirror, langpacks NewBase | 3.6 kB 00:00:00 NewEpel | 4.3 kB 00:00:00 (1/4): NewBase/other_db | 2.4 MB 00:00:00 (2/4): NewBase/filelists_db | 6.6 MB 00:00:00 (3/4): NewEpel/other_db | 2.2 MB 00:00:02 (4/4): NewEpel/filelists_db | 8.0 MB 00:00:23 Loading mirror speeds from cached hostfile * NewBase: Metadata Cache Created 搜索 # yum search string1 [string2][…] 搜索与以指定的关键字程序包名有关的软件列表\n查看yum事务历史 # yum history [info|list|packages- list|packages- info|summary|addon- info|redo|undo|rollback|new|sync|stats]\n#yum history列出yum的详细操作历史 [root@centos7 ~]#yum history Loaded plugins: fastestmirror, langpacks ID | Login user | Date and time | Action(s) | Altered ------------------------------------------------------------------------------- 6 | root \u0026lt;root\u0026gt; | 2017-06-11 19:39 | Install | 1 P\u0026lt; 5 | root \u0026lt;root\u0026gt; | 2017-06-11 18:10 | Install | 1 \u0026gt;\u0026lt; 4 | root \u0026lt;root\u0026gt; | 2017-06-11 17:40 | Install | 1 \u0026gt;\u0026lt; 3 | root \u0026lt;root\u0026gt; | 2017-06-10 09:28 | Install | 53 \u0026gt;\u0026lt; 2 | System \u0026lt;unset\u0026gt; | 2017-05-19 14:53 | Install | 1 \u0026gt; 1 | System \u0026lt;unset\u0026gt; | 2017-05-17 12:55 | Install | 1319 history list #yum history info 6 查看第6条历史的详细操作，从Conmand Lind可以看出是安装了tree软件包 [root@centos7 ~]#yum history info 6 Loaded plugins: fastestmirror, langpacks Transaction ID : 6 Begin time : Sun Jun 11 19:39:02 2017 Begin rpmdb : 1376:18487d64bcef03c85f0f14dca43701acce15328a End time : (0 seconds) End rpmdb : 1377:8e94811fa592da9c619827c4eb3a265af705bb45 User : root \u0026lt;root\u0026gt; Return-Code : Success Command Line : install tree Transaction performed with: Installed rpm-4.11.3-21.el7.x86_64 @anaconda Installed yum-3.4.3-150.el7.centos.noarch @anaconda Installed yum-plugin-fastestmirror-1.1.31-40.el7.noarch @anaconda Packages Altered: Install tree-1.6.0-10.el7.x86_64 @NewBase Rpmdb Problems: conflicts: ipa-client-4.4.0-12.el7.centos.x86_64 has installed conflicts freeipa-client: ipa-client-4.4.0-12.el7.centos.x86_64 Installed ipa-client-4.4.0-12.el7.centos.x86_64 @anaconda conflicts: ipa-client-common-4.4.0-12.el7.centos.noarch has installed conflicts freeipa-client-common: : ipa-client-common-4.4.0-12.el7.centos.noarch Installed ipa-client-common-4.4.0-12.el7.centos.noarch @anaconda conflicts: ipa-common-4.4.0-12.el7.centos.noarch has installed conflicts freeipa-common: ipa-common-4.4.0-12.el7.centos.noarch Installed ipa-common-4.4.0-12.el7.centos.noarch @anaconda history info yum history undo 6 撤消第6条的操作，即卸载tree包 [root@centos7 ~]#yum history undo 6 Loaded plugins: fastestmirror, langpacks Undoing transaction 6, from Sun Jun 11 19:39:02 2017 Install tree-1.6.0-10.el7.x86_64 @NewBase Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package tree.x86_64 0:1.6.0-10.el7 will be erased --\u0026gt; Finished Dependency Resolution Dependencies Resolved ==================================================================================================================================== Package Arch Version Repository Size ==================================================================================================================================== Removing: tree x86_64 1.6.0-10.el7 @NewBase 87 k Transaction Summary ==================================================================================================================================== Remove 1 Package Installed size: 87 k Is this ok [y/N]: y Downloading packages: Running transaction check Running transaction test Transaction test succeeded Running transaction Erasing : tree-1.6.0-10.el7.x86_64 1/1 Verifying : tree-1.6.0-10.el7.x86_64 1/1 Removed: tree.x86_64 0:1.6.0-10.el7 Complete! 这里要注意，在第四个卸载命令里提到了，remove命令是不会把其依赖的包卸载，而通过undo 命令是可以全部卸载的\n包组管理的相关命令 # yum grouplist\n列出所有可使用的软件群组,主要分为以下四类： Installed environment groups: 已经安装的系统环境包组 Available environment groups: 还可以安装的系统环境包组 Installed groups: 已经安装的包组 Available Groups: 还能额外安装的包组\nyum groupinstall group1 [group2][…] 安装包组\nyum groupupdate group1 [group2][…]升级包组\nyum groupremove group1 [group2][…] 卸载包组\nyum groupinfo group1 […]显示指定包组信息\nyum的命令行选项 # -y: 自动回答为“ yes”\n-q:静默模式\n\u0026ndash;nogpgcheck：禁止进行gpg check\n\u0026ndash;disablerepo=repoidglob：临时禁用此处指定的repo\n\u0026ndash;enablerepo=repoidglob：临时启用此处指定的repo\n\u0026ndash;noplugins：禁用所有插件\nyum命令的用法就简单介绍这么多，同时希望本文能让你对yum的使用有所帮助。\n","date":"2 March 2016","permalink":"/2016/03/yum/","section":"博客","summary":"什么是yum # 我们在Linux系统上安装处理软件，一般是使用RPM，它是通过预先编译完成并且把软件打包为RPM文件格式后，再加以安装的一种方式，使用者只要拿到这个打包好的软件，然后将里头的文件放置到应该摆放的目录，这样就完成了安装。但是，由于有些软件是有依赖于其他软件的，当你要安装某个RPM类型的软件时，RPM会检验RPM软件数据库，它所依赖的相关软件包是否都已存在，如果没有检索到，那么这个RPM文件默认就不能安装。甚至是有些包之间还会存在循环依赖，这时RPM就不能快速有效的进行软件安装了。 对于RPM的上述局限性，yum的出现就解决了包之间的依赖性的问题。文章前面提到了，RPM把软件依赖关系储存在本地数据库里，那我们在安装软件的时候，如果先到数据库里找到所有依赖包的列表，再检索哪些已经安装到本地，然后把剩下没安装的一起安装，这样就可以解决包依赖性的问题了，这就是yum机制的由来\nyum的运作流程 # 各版本发行商都会释放出软件并放置于yum服务器上，所以yum服务器储存有我们各种所需的软件。参考下图，yum服务器不仅存储了各种RPM包，还有包的相关的元数据文件（放置于特定目录repodata下），前面提到的包的依赖性关系就储存在元数据文件中。這些文件与RPM软件包所在的本地或网络位置就被称为yum仓库(yum repo)。当用户端有软件安装或升级的需求时，用户端会访问yum服务器下载或更新RPM软件列表并存在本机缓存列表中，然后通过缓存列表与本地RPM数据库相比对，筛选出缺少哪些RPM软件包并根据yum仓库储存的路径下载（可以是本地，也可以是网络），最后通过RPM机制一并进行安装。 如何配置yum客户端 # yum本身的配置文件，主要指向仓库的位置以及相关的各种配置信息。\n主配置文件 # /etc/yum.conf：为所有仓库提供公共配置\n各仓库指向的定义 # /etc/yum.repos.d/*.repo：为仓库的指向提供配置\n* [repositoryID] #仓库的名字，具有唯一性，标识repo的指向 * name=Some name for this repository #仓库描述信息 * baseurl=url://path/to/repository/ #指明repo的访问路径，通常为一个文件服务器上输出的某repo #文件服务器类型 http://SEVER/PATH/TO/REPOSITORY https://SEVER/PATH/TO/REPOSITORY ftp://SEVER/PATH/TO/REPOSITORY file:///PATH/TO/REPOSITORY * enabled={1|0} #此仓库是否可被使用 * gpgcheck={1|0} #是否对RPM包做检验 * gpgkey=url://path/to/keyfile/ #指明gpgkey文件路径 * enablegroups={1|0} #是否启用包组 * failovermethod={roundrobin|priority} #设置baseurl有多个时的优先级 roundrobin：随机挑选，默认值 priority:按顺序访问 * cost= #指明仓库的访问开销，默认为1000 创建新的配置文件 # 按照以上方式，我们就在/etc/yum.","title":"yum客户端的配置及yum命令用法"},{"content":"","date":"14 January 2016","permalink":"/tags/rsyslog/","section":"Tags","summary":"","title":"rsyslog"},{"content":"远程日志服务器的实现 # rsyslog客户端：CentOS 7 ; 192.168.196.168 rsyslog服务端：CentOS 6 ; 192.168.196.155 /etc/rsyslog.conf 客户端配置 #### RULES #### *.info;mail.none;authpriv.none;cron.none @@192.168.196.155 $ systemctl restart rsyslog 配置文件格式：由三部分组成\nMODULES：相关模块配置\nGLOBAL DIRECTIVES：全局配置\nRULES：日志记录相关的规则配置\nRULES配置格式：\nfacility： *: 所有的facility facility1,facility2,facility3,\u0026hellip;：指定的facility列表\npriority： *: 所有级别 none：没有级别，即不记录 PRIORITY：指定级别（含）以上的所有级别 =PRIORITY：仅记录指定级别的日志信息\ntarget： 文件路径：通常在/var/log/，文件路径前的-表示异步写入 用户：将日志事件通知给指定的用户， * 表示登录的所有用户 日志服务器：把日志送往至指定的远程服务器记录 @HOST \u0026mdash;\u0026mdash;- UDP传输 @@HOST \u0026mdash;\u0026mdash;- TCP传输 管道： | COMMAND，转发给其它命令处理\n/etc/rsyslog.conf 服务端配置 #### MODULES #### # Provides TCP syslog reception $ModLoad imtcp $InputTCPServerRun 514 $ service rsyslog restart 测试 [root@Centos7 ~]#logger \u0026#34;This is a test log from client~\u0026#34; [root@Centos6 ~]# tail -n1 /var/log/messages Aug 9 21:43:42 Centos7 root: This is a test log from client~ 日志服务器连接数据库MYSQL # mysql服务器：CentOS 7 ; 192.168.196.168 在mysql server上授权rsyslog能连接至当前服务器 $ mysql -uroot -p123456 mysql\u0026gt; grant all on Syslog.* to \u0026#39;syslog\u0026#39;@\u0026#39;192.168.196.%\u0026#39; identified by \u0026#39;123456\u0026#39;; 在rsyslog服务器上安装mysql模块相关的程序包 $ yum install rsyslog-mysql $ rpm -ql rsyslog-mysql /lib64/rsyslog/ommysql.so /usr/share/doc/rsyslog-mysql-5.8.10 /usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql \u0026lt;--用于创建rsyslog数据库及表的脚本文件 为rsyslog创建数据库及表 $ mysql -usyslog -h192.168.196.168 -p123456 \u0026lt; /usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql 配置rsyslog服务端将日志保存到mysql中 $ vim /etc/rsyslog.conf $ModLoad ommysql *.info;mail.none;authpriv.none;cron.none :ommysql:192.168.196.168,Syslog,syslog,123456 查看mysql数据库中日志数据 $ mysql -usyslog -h192.168.196.168 -p123456 MariaDB [(none)]\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | Syslog | | test | +--------------------+ 3 rows in set (0.00 sec) MariaDB [(none)]\u0026gt; use Syslog; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed MariaDB [Syslog]\u0026gt; show tables; +------------------------+ | Tables_in_Syslog | +------------------------+ | SystemEvents | | SystemEventsProperties | +------------------------+ 2 rows in set (0.00 sec) MariaDB [Syslog]\u0026gt; MariaDB [Syslog]\u0026gt; select * from SystemEvents \\G; *************************** 59. row ***************************\t\u0026lt;--截取客户端触发的最近一条log ID: 59 CustomerID: NULL ReceivedAt: 2017-08-09 22:06:04 DeviceReportedTime: 2017-08-09 22:06:25 Facility: 1 Priority: 5 FromHost: Centos7 Message: This is a test log from client~ NTSeverity: NULL Importance: NULL EventSource: NULL EventUser: NULL EventCategory: NULL EventID: NULL EventBinaryData: NULL MaxAvailable: NULL CurrUsage: NULL MinUsage: NULL MaxUsage: NULL InfoUnitID: 1 SysLogTag: root: EventLogType: NULL GenericFileName: NULL SystemID: NULL processid: checksum: 0 59 rows in set (0.00 sec) loganalyzer展示数据库中的日志 # 在rsyslog服务器上准备amp $ yum install httpd mysql-server php php-mysql php-gd\n安装LogAnalyzer\n$ tar xf loganalyzer-4.1.5.tar.gz $ cp -a loganalyzer-4.1.5/src /var/www/html/ $ cat loganalyzer-4.1.5/contrib/configure.sh \u0026lt;-- 源码文件中的configure脚本,手动执行所有命令 #!/bin/sh touch config.php chmod 666 config.php $ cd src $ touch config.php $ chmod 666 config.php $ service httpd start Web页面配置loganalyzer 浏览器访问http://192.168.196.155/src/install.php 按步骤配置loganalyzer 前三步直接点击next,跳转到配置mysq数据库的页面，根据上文mysql服务器的信息进行配置；点击next完成配置\n浏览器访问http://192.168.196.155/src,可以按事件或者报表查看数据库中的日志 ","date":"14 January 2016","permalink":"/2016/01/rsyslog-collect/","section":"博客","summary":"\u003ch3 class=\"relative group\"\u003e远程日志服务器的实现 \n    \u003cdiv id=\"远程日志服务器的实现\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e8%bf%9c%e7%a8%8b%e6%97%a5%e5%bf%97%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%9a%84%e5%ae%9e%e7%8e%b0\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ersyslog客户端：CentOS 7 ; 192.168.196.168\u003c/li\u003e\n\u003cli\u003ersyslog服务端：CentOS 6 ; 192.168.196.155\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e/etc/rsyslog.conf\u003c/code\u003e 客户端配置\u003c/li\u003e\n\u003c/ol\u003e","title":"如何实现rsyslog的远程日志存储"},{"content":"","date":"14 November 2015","permalink":"/tags/lvm/","section":"Tags","summary":"","title":"LVM"},{"content":"什么是LVM # LVM(Logical Volume Manager 逻辑卷管理)可以实现把多个实体硬盘分区整合在一起，当作一个硬盘来重新操作处理。最重要的是LVM不像传统分区一旦确定分区大小就不能再调整，它允许我们弹性的调整分区及文件系统容量\n通过几道练习题来说明LVM的实现 # 1、创建一个至少有两个PV组成的大小为20G的名为testvg的VG；要求PE大小为16MB, 而后在卷组中创建大小为5G的逻辑卷testlv；挂载至/users目录\n2、新建用户 archlinux，要求其家目录为/users/archlinux，而后su切换至archlinux用户，复制/etc/pam.d目录至自己的家目录\n3、扩展testlv至7G，要求archlinux用户的文件不能丢失\n4、收缩testlv至3G，要求archlinux用户的文件不能丢失5、对testlv创建快照，并尝试基于快照备份数据，验正快照的功能\n题1: PV、VG、LV的创建 # [root@centos6 ~]fdisk /dev/sda \u0026lt;--调整分区sda1 id为8e [root@centos6 ~]fdisk /dev/sdc \u0026lt;--调整分区sdc1 id为8e [root@centos6 ~]#pvcreate /dev/sd{a6,c1} \u0026lt;--指定分区为PV Physical volume \u0026#34;/dev/sda6\u0026#34; successfully created Physical volume \u0026#34;/dev/sdc1\u0026#34; successfully created [root@centos6 ~]#vgcreate -s 16M vg0 /dev/sd{a6,c1} \u0026lt;--创建vg；-s 指定PE大小 Volume group \u0026#34;vg0\u0026#34; successfully created [root@centos6 ~]#lvcreate -n testlv -L 5G vg0 \u0026lt;--创建lv; -n 指定lv名字；-L 按照容量指定lv大小[MGT]参考1） Wiping software RAID md superblock on /dev/vg0/testlv. Logical volume \u0026#34;testlv\u0026#34; created. [root@centos6 ~]#mkfs.ext4 /dev/vg0/testlv \u0026lt;--为lv指定文件系统 [root@centos6 ~]#mount /dev/vg0/testlv /app/lvm \u0026lt;--挂载lv; 在/etc/fstab添加可实现开机自动挂载 lv创建的时候也可以按照PE数量指定 为testlv分配3000 个PE：lvcreate -n testlv -l 3000 vg0\n把卷组所有PE分配给testlv：lvcreate -n testlv -l 100%vg vg0\nPV、VG、LV信息查询 [root@centos6 ~]#pvdisplay --- Physical volume --- PV Name /dev/sda6 VG Name vg0 PV Size 10.00 GiB / not usable 4.58 MiB Allocatable yes PE Size 16.00 MiB Total PE 640 \u0026lt;--按照指定PE Size,该PV划分的PE数量 Free PE 320 \u0026lt;--剩余没有分配给LV的PE数量 Allocated PE 320 \u0026lt;--已经分配给LV的PE数量 PV UUID rbsdXo-s39N-vGar-Oxuc-5E6Z-sIuD-rZP5el --- Physical volume --- PV Name /dev/sdc1 VG Name vg0 PV Size 10.00 GiB / not usable 4.54 MiB Allocatable yes PE Size 16.00 MiB Total PE 640 Free PE 640 Allocated PE 0 PV UUID T9ehfV-wSc8-ez1y-ZlNF-T9P2-PU87-2rFe7Q [root@centos6 ~]#vgdisplay --- Volume group --- VG Name vg0 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 14 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 20.00 GiB PE Size 16.00 MiB Total PE 1280 Alloc PE / Size 320 / 5.00 GiB Free PE / Size 960 / 15.00 GiB VG UUID YlqvPD-Vhsk-2vxR-G5SY-X1HM-3GUk-BNoUzq [root@centos6 ~]#lvdisplay --- Logical volume --- LV Path /dev/vg0/testlv LV Name testlv VG Name vg0 LV UUID nx4ENL-mE35-xlIf-DWQl-6f6j-m2t1-uznnNd LV Write Access read/write LV Creation host, time centos6.ffu.com, 2017-06-22 15:29:38 +0800 LV Status available # open 0 LV Size 5.00 GiB Current LE 320 \u0026lt;--等同于PE数量 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 题2：用户创建 # [root@centos6 app]#useradd -m -k /etc/skel -d /app/lvm/archlinux archlinux [root@centos6 app]#su archlinux [archlinux@centos6 app]$cp -r /etc/pam.d/ lvm/archlinux/ 题3：lv的扩展 # 逻辑卷的扩展是在线扩展，不用卸载lv,不影响用户使用\n[root@centos6 app]#lvextend -L +2G /dev/vg0/testlv \u0026lt;---L 按照容量指定扩展大小(Num增加到;+Num额外增加);参考1） [root@centos6 archlinux]#df -hTP Filesystem Type Size Used Avail Use% Mounted on /dev/sda2 ext4 96G 9.6G 82G 11% / tmpfs tmpfs 491M 76K 491M 1% /dev/shm /dev/sda3 ext4 48G 125M 46G 1% /app /dev/sda1 ext4 969M 35M 885M 4% /boot /dev/sdb7 ext4 2.0G 923M 1016M 48% /testdir /dev/sr0 iso9660 3.7G 3.7G 0 100% /media/CentOS_6.9_Final /dev/mapper/vg0-testlv ext4 4.8G 11M 4.6G 1% /app/lvm \u0026lt;--没有扩展文件系统,没有识别增加的2G [root@centos6 archlinux]#resize2fs /dev/vg0/testlv \u0026lt;--同步文件系统；参考2） [root@centos6 archlinux]#df -hTP Filesystem Type Size Used Avail Use% Mounted on /dev/sda2 ext4 96G 9.6G 82G 11% / tmpfs tmpfs 491M 76K 491M 1% /dev/shm /dev/sda3 ext4 48G 125M 46G 1% /app /dev/sda1 ext4 969M 35M 885M 4% /boot /dev/sdb7 ext4 2.0G 923M 1016M 48% /testdir /dev/sr0 iso9660 3.7G 3.7G 0 100% /media/CentOS_6.9_Final /dev/mapper/vg0-testlv ext4 6.8G 12M 6.5G 1% /app/lvm 同样可以按照PV数量指定 为testlv新增600个PE：lvextend -l 600 /dev/vg0/testlv\n把卷组剩余所有空闲PE分配给testlv：lvextend -l 100%free /dev/vg0/testlv\nresize2fs 只用于ext系统；对于xfs系统要用xfs_growfs +挂载点(注意不是设备名) 此例中是在挂载的状态下进行扩展，如果事先卸载的话，就需要先进行强制磁盘检查-\u0026gt;e2fsck -f /dev/vg0/testlv 其实在扩展lv时加上-r选项就可以一并扩展文件系统-\u0026gt; lvextend -r -l +100%free /dev/vg0/testlv\n题4：lv的缩减 # 逻辑卷的扩展会影响用户使用，不能在线扩展\n[root@centos6 archlinux]#cp /app/lvm /app/test -r \u0026lt;--缩减会影响用户使用，不能在线操作，缩减之前建议先备份 [root@centos6 ~]#umount /app/lvm \u0026lt;--必须先卸载 [root@centos6 ~]#e2fsck -f /dev/vg0/testlv \u0026lt;--强制磁盘检查;必要步骤 e2fsck 1.41.12 (17-May-2010) Pass 1: Checking inodes, blocks, and sizes Pass 2: Checking directory structure Pass 3: Checking directory connectivity` Pass 4: Checking reference counts Pass 5: Checking group summary information /dev/vg0/testlv: 80/458752 files (0.0% non-contiguous), 64511/1835008 blocks [root@centos6 ~]#resize2fs /dev/vg0/testlv 3G \u0026lt;--先缩减文件系统 resize2fs 1.41.12 (17-May-2010) Resizing the filesystem on /dev/vg0/testlv to 786432 (4k) blocks. The filesystem on /dev/vg0/testlv is now 786432 blocks long. [root@centos6 ~]#lvreduce -L 3G /dev/vg0/testlv \u0026lt;--再缩减逻辑卷 WARNING: Reducing active logical volume to 3.00 GiB. THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce vg0/testlv? [y/n]: y Size of logical volume vg0/testlv changed from 7.00 GiB (448 extents) to 3.00 GiB (192 extents). Logical volume testlv successfully resized. [root@centos6 ~]#mount /dev/vg0/testlv /app/lvm \u0026lt;--挂载 [root@centos6 ~]#ls /app/lvm \u0026lt;--数据并没有丢失 archlinux lost+found 题5：快照的使用 # 快照是一种特殊的逻辑卷，它是在生成快照时对存在同一VG下的逻辑卷的准确拷贝。 快照区有两部分组成：变更之前的数据和未变更的数据组成。当原来的逻辑卷中有所改变时，会将旧的数据复制到快照中，而没有被变动的数据依旧保持在原本的区块内。\n[root@centos6 ~]#df -hP \u0026lt;--查看要备份的lv上的数据大小 Filesystem Size Used Avail Use% Mounted on /dev/sda2 96G 9.6G 82G 11% / tmpfs 491M 76K 491M 1% /dev/shm /dev/sda3 48G 125M 46G 1% /app /dev/sda1 969M 35M 885M 4% /boot /dev/sdb7 2.0G 923M 1016M 48% /testdir /dev/sr0 3.7G 3.7G 0 100% /media/CentOS_6.9_Final /dev/mapper/vg0-testlv 2.9G 7.8M 2.7G 1% /app/lvm \u0026lt;--快照大小设置应不大于数据大小 [root@centos6 ~]#lvcreate -n testlv-snapshot -s -L 16M -p r /dev/vg0/testlv \u0026lt;--参考1) Logical volume \u0026#34;testlv-snapshot\u0026#34; created. [root@centos6 ~]#lvdisplay \u0026lt;--可以查看快照逻辑卷 [root@centos6 app]#mkdir snap [root@centos6 app]#mount /dev/vg0/testlv-snapshot /app/snap \u0026lt;--挂载快照 mount: block device /dev/mapper/vg0-testlv--snapshot is write-protected, mounting read-only [root@centos6 lvm]#rm -rf /app/lvm/archlinux/ \u0026lt;--删除逻辑卷testlv上的archlinux目录 [root@centos6 lvm]#ls ../snap \u0026lt;--可以看到快照中archlinux目录还在 archlinux lost+found 利用快照恢复testlv [root@centos6 app]#umount /app/lvm \u0026lt;--卸载逻辑卷 [root@centos6 app]#umount /app/snap/ \u0026lt;--卸载快照 [root@centos6 app]#lvconvert --merge /dev/vg0/testlv-snapshot \u0026lt;--利用快照testlv-snapshot恢复testlv Merging of volume testlv-snapshot started. testlv: Merged: 100.0% Merge of snapshot into logical volume testlv has finished. Logical volume \u0026#34;testlv-snapshot\u0026#34; successfully removed \u0026lt;--恢复后快照自动删除;参考2) [root@centos6 app]#mount /dev/vg0/testlv /app/lvm [root@centos6 app]#ls /app/lvm \u0026lt;--archlinux目录已经恢复 archlinux lost+found -s 指定创建的逻辑卷为快照；-n 指定快照名字；-L 指定快照大小(至少为设定的PE大小); -p r 设为只读属性,也可以挂载时候设置-\u0026gt;mount -o ro /dev/vg0/testlv-snapshot /app/snap 快照恢复也可手动把快照内数据复制到对应逻辑卷挂载目录下;手动删除快照命令为:lvremove /dev/vg0/testlv-snapshot 补充：如何移除一个PV # 沿用上例，移除sda6设备，首先要查看该设备上是否有分配给lv的PE(不是数据)。如果有要转移到对应vg下的其它pv上, 而且其PE数量要小于对应vg的剩余free PE数量\n[root@centos6 ~]#pvdisplay --- Physical volume --- PV Name /dev/sda6 VG Name vg0 PV Size 10.00 GiB / not usable 4.58 MiB Allocatable yes PE Size 16.00 MiB Total PE 640 Free PE 448 Allocated PE 192 \u0026lt;--需要把192个PE转移到/dev/sdc1 PV UUID rbsdXo-s39N-vGar-Oxuc-5E6Z-sIuD-rZP5el --- Physical volume --- PV Name /dev/sdc1 VG Name vg0 PV Size 10.00 GiB / not usable 4.54 MiB Allocatable yes PE Size 16.00 MiB Total PE 640 Free PE 640 Allocated PE 0 PV UUID T9ehfV-wSc8-ez1y-ZlNF-T9P2-PU87-2rFe7Q [root@centos6 ~]#pvmove /dev/sda6 /dev/sdc1 /dev/sda6: Moved: 0.5% /dev/sda6: Moved: 15.6% /dev/sda6: Moved: 30.7% /dev/sda6: Moved: 56.8% /dev/sda6: Moved: 78.6% /dev/sda6: Moved: 100.0% [root@centos6 ~]#vgreduce vg0 /dev/sda6 \u0026lt;--把pv/dev/sda6从vg0中移除 Removed \u0026#34;/dev/sda6\u0026#34; from volume group \u0026#34;vg0\u0026#34; [root@centos6 ~]#pvremove /dev/sda6 \u0026lt;--把设备/dev/sda6从pv中移除 Labels on physical volume \u0026#34;/dev/sda6\u0026#34; successfully wiped ","date":"14 November 2015","permalink":"/2015/11/logical-volume/","section":"博客","summary":"什么是LVM # LVM(Logical Volume Manager 逻辑卷管理)可以实现把多个实体硬盘分区整合在一起，当作一个硬盘来重新操作处理。最重要的是LVM不像传统分区一旦确定分区大小就不能再调整，它允许我们弹性的调整分区及文件系统容量\n通过几道练习题来说明LVM的实现 # 1、创建一个至少有两个PV组成的大小为20G的名为testvg的VG；要求PE大小为16MB, 而后在卷组中创建大小为5G的逻辑卷testlv；挂载至/users目录\n2、新建用户 archlinux，要求其家目录为/users/archlinux，而后su切换至archlinux用户，复制/etc/pam.d目录至自己的家目录\n3、扩展testlv至7G，要求archlinux用户的文件不能丢失\n4、收缩testlv至3G，要求archlinux用户的文件不能丢失5、对testlv创建快照，并尝试基于快照备份数据，验正快照的功能\n题1: PV、VG、LV的创建 # [root@centos6 ~]fdisk /dev/sda \u0026lt;--调整分区sda1 id为8e [root@centos6 ~]fdisk /dev/sdc \u0026lt;--调整分区sdc1 id为8e [root@centos6 ~]#pvcreate /dev/sd{a6,c1} \u0026lt;--指定分区为PV Physical volume \u0026#34;/dev/sda6\u0026#34; successfully created Physical volume \u0026#34;/dev/sdc1\u0026#34; successfully created [root@centos6 ~]#vgcreate -s 16M vg0 /dev/sd{a6,c1} \u0026lt;--创建vg；-s 指定PE大小 Volume group \u0026#34;vg0\u0026#34; successfully created [root@centos6 ~]#lvcreate -n testlv -L 5G vg0 \u0026lt;--创建lv; -n 指定lv名字；-L 按照容量指定lv大小[MGT]参考1） Wiping software RAID md superblock on /dev/vg0/testlv.","title":"逻辑卷LVM的实现"},{"content":"​\t以centos6为例，/boot目录下有最为关键的开机启动所必须的内核文件、根文件系统驱动文件已经引导加载程序(bootloader)grub。当我们清空此文件夹之后关机，机器就不能正常启动了，这种情况下，可以借助光盘启动进入救援模式解决。具体步骤如下：\n开机进入救援模式 # 这里不像正常情况下，显示根文件系统挂载在/mnt/sysimage目录，而是提示找不到分区。这是因为我把/etc/fstab文件删除了，即使救援模式下，系统启动也不会搜索挂载根目录了，此时我们只能查看本主机各个分区情况，尝试找到根目录并手动挂载。\n手动挂载根目录，恢复/etc/fstab文件 # 进入shell模式下，根据分区情况可以看出，为了使系统容量具有拓展性，根系统基于逻辑卷的。但是通过lvdisplay命令可以看出，逻辑卷的状态是not available，这是因为LVM及software Raid设备是在运行系统初始化脚本/etc/rc.d/rc.sysinit时才被激活。很显然，在本例中bootLoader都已经被损坏，lvm无法被自动激活，使用命令#vgchange -ay 手动激活\n激活lvm之后，通过逻辑卷名字看出根文件系统应该在dev/vg_www/lv_root逻辑卷设备上，挂载该设备到/mnt/tmp目录。参考下图ls结果可知，此设备确实是根文件系统，创建fstab文件并重启\n重启，进入救援模式，修复/boot # 1)如下图所示，救援模式下已显示找到根文件系统，并挂载在/mnt/sysimage目录。进入shell模式，在/boot目录下安装kernel和grub\n2)grub.conf文件可以在上述shell下直接编辑修复也可以在进入开机菜单时使用grub交互程序输入\n重启机器，系统直接进入grub交互界面，如下图所示\n分别输入kernel参数和伪根文件系统路径，并启动\n指定kernel和initrd的文件路径根为/boot所在的设备及分区;(hd0,0)代表着第一个硬盘中第一个分区\n重启之后，可以正常登陆了 # 登陆之后再去完善/etc/fstab文件及grub.conf文件，机器就恢复成功了。\n","date":"6 October 2015","permalink":"/2015/10/partion-recover/","section":"博客","summary":"​\t以centos6为例，/boot目录下有最为关键的开机启动所必须的内核文件、根文件系统驱动文件已经引导加载程序(bootloader)grub。当我们清空此文件夹之后关机，机器就不能正常启动了，这种情况下，可以借助光盘启动进入救援模式解决。具体步骤如下：\n开机进入救援模式 # 这里不像正常情况下，显示根文件系统挂载在/mnt/sysimage目录，而是提示找不到分区。这是因为我把/etc/fstab文件删除了，即使救援模式下，系统启动也不会搜索挂载根目录了，此时我们只能查看本主机各个分区情况，尝试找到根目录并手动挂载。\n手动挂载根目录，恢复/etc/fstab文件 # 进入shell模式下，根据分区情况可以看出，为了使系统容量具有拓展性，根系统基于逻辑卷的。但是通过lvdisplay命令可以看出，逻辑卷的状态是not available，这是因为LVM及software Raid设备是在运行系统初始化脚本/etc/rc.d/rc.sysinit时才被激活。很显然，在本例中bootLoader都已经被损坏，lvm无法被自动激活，使用命令#vgchange -ay 手动激活\n激活lvm之后，通过逻辑卷名字看出根文件系统应该在dev/vg_www/lv_root逻辑卷设备上，挂载该设备到/mnt/tmp目录。参考下图ls结果可知，此设备确实是根文件系统，创建fstab文件并重启\n重启，进入救援模式，修复/boot # 1)如下图所示，救援模式下已显示找到根文件系统，并挂载在/mnt/sysimage目录。进入shell模式，在/boot目录下安装kernel和grub\n2)grub.conf文件可以在上述shell下直接编辑修复也可以在进入开机菜单时使用grub交互程序输入\n重启机器，系统直接进入grub交互界面，如下图所示\n分别输入kernel参数和伪根文件系统路径，并启动\n指定kernel和initrd的文件路径根为/boot所在的设备及分区;(hd0,0)代表着第一个硬盘中第一个分区\n重启之后，可以正常登陆了 # 登陆之后再去完善/etc/fstab文件及grub.conf文件，机器就恢复成功了。","title":"/etc/fstab及/boot分区文件恢复"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"🤝感兴趣可以加个好友 ","date":"1 January 0001","permalink":"/me/","section":"Welcome to feixiang's blog 🎉","summary":"🤝感兴趣可以加个好友 ","title":"My Wechat"}]