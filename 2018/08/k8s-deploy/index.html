<!doctype html><html lang=zh-ch dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="zh-ch"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Kubernetes集群部署 &#183; FeixiangFu</title>
<meta name=title content="Kubernetes集群部署 &#183; FeixiangFu"><meta name=description content="打工人:tree:"><meta name=keywords content="kubernetes,"><link rel=canonical href=https://silentkeeping.github.io/2018/08/k8s-deploy/><link type=text/css rel=stylesheet href=/css/main.bundle.min.2b8b793b5350f23c027e845b237b267c85c08a4a6128dfad2b43b25f354c42eb4629bb7f88b75e09b52056c28e0d057594a8185bd540bd7bece42b3be6399cdf.css integrity="sha512-K4t5O1NQ8jwCfoRbI3smfIXAikphKN+tK0OyXzVMQutGKbt/iLdeCbUgVsKODQV1lKgYW9VAvXvs5Cs75jmc3w=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.82867bf47a612d7462e3555f4fccb5ab28e2bd4b18c37131afdf5ba4d21396ab08ac679cae8e0ae71b26835b657ce022e7aafd352b1ecf7ce672512e5cb25882.js integrity="sha512-goZ79HphLXRi41VfT8y1qyjivUsYw3Exr99bpNITlqsIrGecro4K5xsmg1tlfOAi56r9NSsez3zmclEuXLJYgg==" data-copy data-copied></script><script src=/js/zoom.min.js></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="Kubernetes集群部署"><meta property="og:description" content="k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。
集群节点规划及架构 # 节点 IP service master 192.168.196.134 etcd,flannel,docker,kubernetes-master,docker-distribution,nginx node1 192.168.196.135 flannel,docker,kubernetes-node node2 192.168.196.136 flannel,docker,kubernetes-node 对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群
服务安装 # etcd # 所有关于集群状态的配置信息都以key/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods 本例只是etcd单机部署，在master节点上yum安装etcd并修改etcd.conf，启动etcd
$ grep ^[^#] /etc/etcd/etcd.conf ETCD_DATA_DIR=&#34;/var/lib/etcd/default.etcd&#34; ETCD_LISTEN_CLIENT_URLS=&#34;http://192.168.196.134:2379&#34; ETCD_NAME=etcd1 ETCD_INITIAL_ADVERTISE_PEER_URLS=&#34;http://192."><meta property="og:type" content="article"><meta property="og:url" content="https://silentkeeping.github.io/2018/08/k8s-deploy/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-08-09T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes集群部署"><meta name=twitter:description content="k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。
集群节点规划及架构 # 节点 IP service master 192.168.196.134 etcd,flannel,docker,kubernetes-master,docker-distribution,nginx node1 192.168.196.135 flannel,docker,kubernetes-node node2 192.168.196.136 flannel,docker,kubernetes-node 对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群
服务安装 # etcd # 所有关于集群状态的配置信息都以key/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods 本例只是etcd单机部署，在master节点上yum安装etcd并修改etcd.conf，启动etcd
$ grep ^[^#] /etc/etcd/etcd.conf ETCD_DATA_DIR=&#34;/var/lib/etcd/default.etcd&#34; ETCD_LISTEN_CLIENT_URLS=&#34;http://192.168.196.134:2379&#34; ETCD_NAME=etcd1 ETCD_INITIAL_ADVERTISE_PEER_URLS=&#34;http://192."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"博客","name":"Kubernetes集群部署","headline":"Kubernetes集群部署","abstract":"k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。\n集群节点规划及架构 # 节点 IP service master 192.168.196.134 etcd,flannel,docker,kubernetes-master,docker-distribution,nginx node1 192.168.196.135 flannel,docker,kubernetes-node node2 192.168.196.136 flannel,docker,kubernetes-node 对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群\n服务安装 # etcd # 所有关于集群状态的配置信息都以key\/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods 本例只是etcd单机部署，在master节点上yum安装etcd并修改etcd.conf，启动etcd\n$ grep ^[^#] \/etc\/etcd\/etcd.conf ETCD_DATA_DIR=\u0026#34;\/var\/lib\/etcd\/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http:\/\/192.168.196.134:2379\u0026#34; ETCD_NAME=etcd1 ETCD_INITIAL_ADVERTISE_PEER_URLS=\u0026#34;http:\/\/192.","inLanguage":"zh-ch","url":"https:\/\/silentkeeping.github.io\/2018\/08\/k8s-deploy\/","author":{"@type":"Person","name":"FeixiangFu"},"copyrightYear":"2018","dateCreated":"2018-08-09T00:00:00\u002b00:00","datePublished":"2018-08-09T00:00:00\u002b00:00","dateModified":"2018-08-09T00:00:00\u002b00:00","keywords":["kubernetes"],"mainEntityOfPage":"true","wordCount":"1481"}]</script><meta name=author content="FeixiangFu"><link href=https://github.com/silentKeeping/silentkeeping.github.io/tree/gh-pages rel=me><link href=/me rel=me><script src=/lib/jquery/jquery.slim.min.js integrity></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span></a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div><a href=/ class=flex><span class=sr-only>FeixiangFu</span>
<img src=/img/avatar-logo_hu2551ff66a3dea2bebaba020a8603e044_243346_288x0_resize_box_3.png width=144 height=144 class="logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom" alt=FeixiangFu></a></div><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">FeixiangFu</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=博客>博客</p></a><a href=https://github.com/silentKeeping/silentkeeping.github.io/tree/gh-pages target=_blank class="flex items-center"><span><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium text-gray-500 hover:text-gray-900" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400 h-12" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher aria-label="Dark mode switcher" type=button><div class="flex items-center justify-center h-12 dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden h-12 dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button style=margin-right:5px><div class="flex items-center justify-center h-12 dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden h-12 dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=博客>博客</p></a></li><li class=mt-1><a href=https://github.com/silentKeeping/silentkeeping.github.io/tree/gh-pages target=_blank class="flex items-center"><div><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title></p></a></li></ul></div></label></div></div><script>(function(){var e=$(".main-menu"),t=window.location.pathname;e.find('a[href="'+t+'"]').each(function(e,t){$(t).children("p").addClass("active")})})()</script></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/img/ocean_hu3d03a01dcc18bc5be0e67db3d8d209a6_5097493_1200x0_resize_q75_box.jpg)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>Welcome to feixiang's blog :tada:</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>博客</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/2018/08/k8s-deploy/>Kubernetes集群部署</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Kubernetes集群部署</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div style=cursor:default class="flex flex-row flex-wrap items-center"><time datetime="2018-08-09 00:00:00 +0000 UTC">9 August 2018</time><span class="px-2 text-primary-500">&#183;</span><span>1481 字</span><span class="px-2 text-primary-500">&#183;</span><span title=预计阅读>7 分钟</span></div><div style=cursor:pointer class="flex flex-row flex-wrap items-center"></div><div style=cursor:pointer class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/t/kubernetes/","_self")'><span class=flex><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">kubernetes</span></span></span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first sm:max-w-prose lg:ml-auto px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"></summary><div class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#集群节点规划及架构>集群节点规划及架构</a></li><li><a href=#服务安装>服务安装</a><ul><li><a href=#etcd>etcd</a></li><li><a href=#flannel>flannel</a></li><li><a href=#registry>registry</a></li><li><a href=#docker>docker</a></li><li><a href=#kubernetes-master>kubernetes-master</a></li><li><a href=#kubernetes-node>kubernetes-node</a></li></ul></li><li><a href=#部署示例>部署示例</a></li></ul></li></ul></nav></div></details><details class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"></summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#集群节点规划及架构>集群节点规划及架构</a></li><li><a href=#服务安装>服务安装</a><ul><li><a href=#etcd>etcd</a></li><li><a href=#flannel>flannel</a></li><li><a href=#registry>registry</a></li><li><a href=#docker>docker</a></li><li><a href=#kubernetes-master>kubernetes-master</a></li><li><a href=#kubernetes-node>kubernetes-node</a></li></ul></li><li><a href=#部署示例>部署示例</a></li></ul></li></ul></nav></div></details><script>(function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=t.attr("id"))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active")}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){n()})}})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="max-w-prose mb-20"><blockquote><p>k8s是一个Google开源的容器集群管理平台，如今风靡一时，了解并掌握这门技术变得尤为重要。本文将介绍如何搭建一套简单的k8s集群并部署pod、kubernetes-dashboard。关于k8s架构及其组件的详细概念请自行搜索查阅。</p></blockquote><h3 class="relative group">集群节点规划及架构<div id=集群节点规划及架构 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e9%9b%86%e7%be%a4%e8%8a%82%e7%82%b9%e8%a7%84%e5%88%92%e5%8f%8a%e6%9e%b6%e6%9e%84 aria-label=锚点>#</a></span></h3><table><thead><tr><th style=text-align:center>节点</th><th style=text-align:center>IP</th><th style=text-align:center>service</th></tr></thead><tbody><tr><td style=text-align:center>master</td><td style=text-align:center>192.168.196.134</td><td style=text-align:center>etcd,flannel,docker,kubernetes-master,docker-distribution,nginx</td></tr><tr><td style=text-align:center>node1</td><td style=text-align:center>192.168.196.135</td><td style=text-align:center>flannel,docker,kubernetes-node</td></tr><tr><td style=text-align:center>node2</td><td style=text-align:center>192.168.196.136</td><td style=text-align:center>flannel,docker,kubernetes-node</td></tr></tbody></table><blockquote><p>对于高可用和容错的Kubernetes生产和部署，需要多个主节点和一个单独的etcd集群</p></blockquote><p><figure><img class="my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/selfflying/mypic@main/blog/20180809/k8s.jpg alt=k8s></figure></p><h3 class="relative group">服务安装<div id=服务安装 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e6%9c%8d%e5%8a%a1%e5%ae%89%e8%a3%85 aria-label=锚点>#</a></span></h3><h4 class="relative group">etcd<div id=etcd class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#etcd aria-label=锚点>#</a></span></h4><p>所有关于集群状态的配置信息都以key/value对的形式存储在etcd中，这些状态显示了集群中包含的节点和需要在其中运行的pods
本例只是etcd单机部署，在master节点上yum安装etcd并修改<code>etcd.conf</code>，启动etcd</p><pre tabindex=0><code>$ grep ^[^#] /etc/etcd/etcd.conf 
ETCD_DATA_DIR=&#34;/var/lib/etcd/default.etcd&#34;
ETCD_LISTEN_CLIENT_URLS=&#34;http://192.168.196.134:2379&#34;
ETCD_NAME=etcd1
ETCD_INITIAL_ADVERTISE_PEER_URLS=&#34;http://192.168.196.134:2380&#34;
ETCD_ADVERTISE_CLIENT_URLS=&#34;http://192.168.196.134:2379&#34;
ETCD_INITIAL_CLUSTER=&#34;etcd1=http://192.168.196.134:2380&#34;

$ ss -ntl|egrep &#34;2379|2380&#34;
LISTEN     0      128    192.168.196.134:2379                     *:*                  
LISTEN     0      128    127.0.0.1:2380                     *:*     

#etcdctl简单命令
$ etcdctl --endpoints http://192.168.196.134:2379 mkdir /testdir
$ etcdctl --endpoints http://192.168.196.134:2379 ls /testdir   
$ etcdctl --endpoints http://192.168.196.134:2379 ls /
/testdir
$ etcdctl --endpoints http://192.168.196.134:2379 mk /testdir/testkey testvalue  
testvalue
$ etcdctl --endpoints http://192.168.196.134:2379 get /testdir/testkey
testvalue
$ etcdctl --endpoints http://192.168.196.134:2379 rm /testdir/testkey
PrevNode.Value: testvalue
$ etcdctl --endpoints http://192.168.196.134:2379 rmdir /testdir
</code></pre><h4 class="relative group">flannel<div id=flannel class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#flannel aria-label=锚点>#</a></span></h4><p>overlay网络解决方案，实现pod到pod之间的网络通信，并为每个pod提供唯一的IP地址</p><ul><li>各节点yum安装flannel，修改<code>/etc/sysconfig/flanneld</code></li></ul><pre tabindex=0><code>$ grep ^[^#] /etc/sysconfig/flanneld 
FLANNEL_ETCD_ENDPOINTS=&#34;http://192.168.196.134:2379&#34;
FLANNEL_ETCD_PREFIX=&#34;/atomic.io/network&#34;                               &lt;--定义在etcd中存储的目录
FLANNEL_OPTIONS=&#34;-iface=ens34 -ip-masq=true&#34;
</code></pre><ul><li>在etcd中手动创建自定义flannel网段的地址池<code>10.55.0/16</code></li></ul><pre tabindex=0><code>$ etcdctl --endpoints http://192.168.196.134:2379 mkdir /atomic.io
$ etcdctl --endpoints http://192.168.196.134:2379 mkdir /atomic.io/network
$ etcdctl --endpoints http://192.168.196.134:2379 mk /atomic.io/network/config &#39;{&#34;Network&#34;: &#34;10.55.0.0/16&#34;}&#39;
{&#34;Network&#34;: &#34;10.55.0.0/16&#34;}
</code></pre><ul><li>各节点启动flanneld服务，可查看flannel0桥随机分配的地址</li></ul><pre tabindex=0><code># master节点
$ ifconfig flannel0 | grep -A1 flannel0                                  
flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 10.55.68.0  netmask 255.255.0.0  destination 10.55.68.0

# flannel配置信息存在subnet.env文件
$ cat /var/run/flannel/subnet.env 
FLANNEL_NETWORK=10.55.0.0/16
FLANNEL_SUBNET=10.55.68.1/24
FLANNEL_MTU=1472
FLANNEL_IPMASQ=true

# node1节点
$ ifconfig flannel0 | grep -A1 flannel0
flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 10.55.91.0  netmask 255.255.0.0  destination 10.55.91.0

# node2节点
$ ifconfig flannel0 | grep -A1 flannel
flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 10.55.2.0  netmask 255.255.0.0  destination 10.55.2.0       
</code></pre><h4 class="relative group">registry<div id=registry class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#registry aria-label=锚点>#</a></span></h4><p>​ 为集群搭建私有镜像仓库，可以从dockerHub上拉取镜像启动，也可以yum安装docker-distribution和nginx进行搭建，本文使用后一种方法</p><ul><li>在master节点上，yum安装docker-distribution并修改<code>config.yml</code>,启动服务</li></ul><pre tabindex=0><code>$ cat /etc/docker-distribution/registry/config.yml 
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /var/lib/registry                                &lt;--存储路径
http:
    addr: 127.0.0.1:5000                                                &lt;--监听地址和端口
</code></pre><ul><li>在master节点，yum安装nginx，新建一个代理到后端registry的nginx配置文件<code>registry.conf</code>,启动服务</li></ul><pre tabindex=0><code>$ cat /etc/nginx/conf.d/registry.conf 
server {
        listen 8088;
        server_name registry;
        client_max_body_size 0;

        location / {
            proxy_pass  http://127.0.0.1:5000;
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
            proxy_redirect off;
            proxy_buffering off;
            proxy_set_header        Host            $http_host;
            proxy_set_header        X-Real-IP       $remote_addr;
            proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
        }
}

# 8080和5000端口已监听
$ netstat -ntl
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 192.168.196.134:2379    0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.1:2380          0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:8088            0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.1:5000          0.0.0.0:*               LISTEN     
tcp6       0      0 :::80                   :::*                    LISTEN     
tcp6       0      0 :::22                   :::*                    LISTEN     
tcp6       0      0 ::1:25                  :::*                    LISTEN   
</code></pre><ul><li>修改各节点<code>/etc/hosts</code>文件，配置registry解析，以master节点为例</li></ul><pre tabindex=0><code>$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.196.134 registry
192.168.196.135 node1
192.168.196.136 node2

# 验证镜像仓库服务是否正常
$ curl -Lv  registry:8088/v2   
* About to connect() to registry port 8088 (#0)
*   Trying 192.168.196.134...
* Connected to registry (192.168.196.134) port 8088 (#0)
&gt; GET /v2 HTTP/1.1
&gt; User-Agent: curl/7.29.0
&gt; Host: registry:8088
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 301 Moved Permanently
&lt; Server: nginx/1.12.2
&lt; Date: Tue, 03 Jul 2018 14:45:49 GMT
&lt; Content-Type: text/html; charset=utf-8
&lt; Content-Length: 39
&lt; Connection: keep-alive
&lt; Docker-Distribution-Api-Version: registry/2.0
&lt; Location: /v2/
&lt; 
* Ignoring the response-body
* Connection #0 to host registry left intact
* Issue another request to this URL: &#39;HTTP://registry:8088/v2/&#39;
* Found bundle for host registry: 0x78fee0
* Re-using existing connection! (#0) with host registry
* Connected to registry (192.168.196.134) port 8088 (#0)
&gt; GET /v2/ HTTP/1.1
&gt; User-Agent: curl/7.29.0
&gt; Host: registry:8088
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Server: nginx/1.12.2
&lt; Date: Tue, 03 Jul 2018 14:45:49 GMT
&lt; Content-Type: application/json; charset=utf-8
&lt; Content-Length: 2
&lt; Connection: keep-alive
&lt; Docker-Distribution-Api-Version: registry/2.0
&lt; 
* Connection #0 to host registry left intact
{}
</code></pre><h4 class="relative group">docker<div id=docker class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#docker aria-label=锚点>#</a></span></h4><p>1.master节点yum安装 docker-engine-1.13.1-1</p><ul><li>修改/etc/docker/daemon.json</li></ul><pre tabindex=0><code>$ cat /etc/docker/daemon.json 
{
    &#34;storage-driver&#34;: &#34;devicemapper&#34;,
    &#34;storage-opts&#34;: [
    &#34;dm.thinpooldev=/dev/mapper/docker--vg-thinpool&#34;,
    &#34;dm.use_deferred_removal=true&#34;,
    &#34;dm.use_deferred_deletion=true&#34;
    ],
    &#34;graph&#34;: &#34;/docker&#34;,
    &#34;insecure-registries&#34;: [&#34;registry:8088&#34;]                                  &lt;--配置镜像仓库地址
}
</code></pre><ul><li>修改/usr/lib/systemd/system/docker.service<ul><li>加入<code>$DOCKER_NETWORK_OPTIONS</code>启动参数，否则docker无法根据flannel服务随机分配的网段配置docker0网桥</li><li>[service]下添加ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT；否则，docker启动后FORWARD链的默认策略为DROP</li></ul></li></ul><pre tabindex=0><code>$vi /usr/lib/systemd/system/docker.service
ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS
ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT

#flannel服务启动后会自动生成/var/run/flannel/docker文件，可用于指定docker的相关网络参数
$ cat /var/run/flannel/docker 
DOCKER_OPT_BIP=&#34;--bip=10.55.68.1/24&#34;
DOCKER_OPT_IPMASQ=&#34;--ip-masq=false&#34;
DOCKER_OPT_MTU=&#34;--mtu=1472&#34;
DOCKER_NETWORK_OPTIONS=&#34; --bip=10.55.68.1/24 --ip-masq=false --mtu=1472&#34;
</code></pre><ul><li>启动docker</li></ul><pre tabindex=0><code>$ systemctl daemon-reload
$ systemctl start docker
$ ifconfig|grep -A1 &#34;docker0\|flannel0&#34;
docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
        inet 10.55.68.1  netmask 255.255.255.0  broadcast 0.0.0.0
--
flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 10.55.68.0  netmask 255.255.0.0  destination 10.55.68.0
        
$ docker info | grep -A2 Insecure
Insecure Registries:
 registry:8088
 127.0.0.0/8
</code></pre><p>2.node1节点和node2节点yum安装docker-1.13.1</p><ul><li>添加镜像仓库地址，修改配置文件<code>/etc/sysconfig/docker</code></li></ul><pre tabindex=0><code>$ grep ^[^#] /etc/sysconfig/docker   
OPTIONS=&#39;--selinux-enabled --log-driver=journald --signature-verification=false -g /docker&#39;
if [ -z &#34;${DOCKER_CERT_PATH}&#34; ]; then
    DOCKER_CERT_PATH=/etc/docker
fi
ADD_REGISTRY=&#34;--add-registry registry&#34;
INSECURE_REGISTRY=&#34;--insecure-registry registry:8088&#34;
</code></pre><ul><li><p>修改/usr/lib/systemd/system/docker.service</p><p>[service]下添加ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT；否则，docker启动后FORWARD链的默认策略为DROP</p></li><li><p>启动docker服务</p></li></ul><pre tabindex=0><code>$ systemctl daemon-reload
$ systemctl start docker
$ ifconfig |grep -A1 &#34;docker0\|flannel&#34;
docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
        inet 10.55.2.1  netmask 255.255.255.0  broadcast 0.0.0.0
--
flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472
        inet 10.55.2.0  netmask 255.255.0.0  destination 10.55.2.0
        
$ docker info|grep -A2 Insecure
Insecure Registries:
 registry:8088
 127.0.0.0/8
</code></pre><h4 class="relative group">kubernetes-master<div id=kubernetes-master class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kubernetes-master aria-label=锚点>#</a></span></h4><ul><li>master节点yum安装kubernetes-master，修改/etc/kubernetes/下的配置文件</li></ul><pre tabindex=0><code>$ls /etc/kubernetes/
apiserver  config  controller-manager  scheduler

#config中的配置为apiserver、controller-manager及scheduler的通用配置项
$ grep ^[^#] /etc/kubernetes/config
KUBE_LOGTOSTDERR=&#34;--logtostderr=true&#34;
KUBE_LOG_LEVEL=&#34;--v=0&#34;
KUBE_ALLOW_PRIV=&#34;--allow-privileged=false&#34;
KUBE_MASTER=&#34;--master=http://192.168.196.134:8080&#34;                     &lt;--指定apiserver地址及端口

$ grep ^[^#] /etc/kubernetes/apiserver
KUBE_API_ADDRESS=&#34;--insecure-bind-address=0.0.0.0&#34;
KUBE_ETCD_SERVERS=&#34;--etcd-servers=http://192.168.196.134:2379&#34;            &lt;--etcd服务地址
KUBE_SERVICE_ADDRESSES=&#34;--service-cluster-ip-range=10.254.0.0/16&#34;         &lt;--k8s集群的service网段
KUBE_ADMISSION_CONTROL=&#34;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&#34;
KUBE_API_ARGS=&#34;&#34;
</code></pre><ul><li><p>master节点上启动kube-apiserver、kube-scheduler、kube-controller-manager服务</p><p>2.1 Kubernetes API Server: 通过kube-apiserver进程提供服务；用户通过Rest操作或kubectl cli与API Server交互；它用于所有与API对象（Pod、RC、Service等）相关的操作(如增、删、改、查及watch等)，是集群内各功能模块之间数据交互和通信的中心枢纽。
2.2 Controller Manager: 集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（serviceAccount）、资源定额（ResourceQuota）等的管理，提供自愈功能，确保集群始终处于预期的工作状态
2.3 Scheduler: 负责Pod的调度，将待调度的Pod(API新创建的Pod、Controller Manager为补足副本而创建的Pod等)按照特定的调度算法和调度策略绑定到集群中某个node上，并将绑定信息写入etcd</p></li></ul><pre tabindex=0><code>$ systemctl start kube-apiserver.service kube-scheduler.service kube-controller-manager.service

# 使用命令行工具测试
$ kubectl -s 192.168.196.134:8080 --version
Kubernetes v1.5.2
</code></pre><h4 class="relative group">kubernetes-node<div id=kubernetes-node class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kubernetes-node aria-label=锚点>#</a></span></h4><ul><li>node1/node2节点yum安装kubernetes-node，修改/etc/kubernetes/下的配置文件</li></ul><pre tabindex=0><code>#config中的配置为kubelet及kube-proxy的通用配置项
$ grep ^[^#] /etc/kubernetes/config 
KUBE_LOGTOSTDERR=&#34;--logtostderr=true&#34;
KUBE_LOG_LEVEL=&#34;--v=0&#34;
KUBE_ALLOW_PRIV=&#34;--allow-privileged=false&#34;
KUBE_MASTER=&#34;--master=lhttp://192.168.196.134:8080&#34;

$ grep ^[^#] /etc/kubernetes/kubelet 
KUBELET_ADDRESS=&#34;--address=0.0.0.0&#34;
KUBELET_HOSTNAME=&#34;--hostname-override=node1&#34;                          &lt;--注意node1/node2的区别
KUBELET_API_SERVER=&#34;--api-servers=http://192.168.196.134:8080&#34;
KUBELET_POD_INFRA_CONTAINER=&#34;--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest&#34;                                                &lt;--随pod一起启动的容器的镜像
KUBELET_ARGS=&#34;&#34;
</code></pre><ul><li><p>在node节点上启动kubelet、kube-proxy</p><p>2.1 kubelet: 处理Master节点下发到本节点的任务，管理Pod及Pod中的容器（通过API Server监听Scheduler产生的Pod绑定event）；每个kubelet进程会在API Server上注册节点自身信息，定期向Master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。
2.2 kube-proxy: kube-proxy服务进程，可以看做是service的透明代理兼负载均衡，其核心功能是将到某个service的访问请求转发到后端的多个Pod实例上，事实上是通过iptables的NAT转换来实现的。Service是对一组Pod的抽象，创建时可被分配一个虚拟的Cluster IP。</p></li></ul><pre tabindex=0><code>$ systemctl start kubelet.service kube-proxy.service

# 使用命令行工具测试
$ kubectl -s 192.168.196.134:8080 get node
NAME      STATUS    AGE
node1     Ready     2m
node2     Ready     2m
</code></pre><h3 class="relative group">部署示例<div id=部署示例 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%e9%83%a8%e7%bd%b2%e7%a4%ba%e4%be%8b aria-label=锚点>#</a></span></h3><p>1.查看当前版本的kubernetes服务支持的API类型及版本</p><pre tabindex=0><code>$ curl 192.168.196.134:8080
{
  &#34;paths&#34;: [
    &#34;/api&#34;,
    &#34;/api/v1&#34;,
    &#34;/apis&#34;,
    &#34;/apis/apps&#34;,
    &#34;/apis/apps/v1beta1&#34;,
    &#34;/apis/authentication.k8s.io&#34;,
    &#34;/apis/authentication.k8s.io/v1beta1&#34;,
    &#34;/apis/authorization.k8s.io&#34;,
    &#34;/apis/authorization.k8s.io/v1beta1&#34;,
    &#34;/apis/autoscaling&#34;,
    &#34;/apis/autoscaling/v1&#34;,
    &#34;/apis/batch&#34;,
    &#34;/apis/batch/v1&#34;,
    &#34;/apis/batch/v2alpha1&#34;,
    &#34;/apis/certificates.k8s.io&#34;,
    &#34;/apis/certificates.k8s.io/v1alpha1&#34;,
    &#34;/apis/extensions&#34;,
    &#34;/apis/extensions/v1beta1&#34;,
    &#34;/apis/policy&#34;,
    &#34;/apis/policy/v1beta1&#34;,
    &#34;/apis/rbac.authorization.k8s.io&#34;,
    &#34;/apis/rbac.authorization.k8s.io/v1alpha1&#34;,
    &#34;/apis/storage.k8s.io&#34;,
    &#34;/apis/storage.k8s.io/v1beta1&#34;,
    &#34;/healthz&#34;,
    &#34;/healthz/ping&#34;,
    &#34;/healthz/poststarthook/bootstrap-controller&#34;,
    &#34;/healthz/poststarthook/extensions/third-party-resources&#34;,
    &#34;/healthz/poststarthook/rbac/bootstrap-roles&#34;,
    &#34;/logs&#34;,
    &#34;/metrics&#34;,
    &#34;/swaggerapi/&#34;,
    &#34;/ui/&#34;,
    &#34;/version&#34;
  ]
}
</code></pre><p>2.部署一个nginx服务的deployment示例</p><pre tabindex=0><code>$ cat deployment-nginx.yml 
apiVersion: extensions/v1beta1                                    &lt;--指定APIversion
kind: Deployment                                                  &lt;--指定资源类型为deployment
metadata:
  name: deployment-nginx                                          &lt;--deployment的名称
spec:
  replicas: 2                                                     &lt;--期望pod的副本数
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:                                                     &lt;--pod标签
        app: nginx
    spec:
      containers:
      - name: nginx
        image: registry:8088/nginx:latest
        ports:
        - containerPort: 80
$ kubectl -s 192.168.196.134:8080 create -f deployment-nginx.yml
</code></pre><ul><li><p>查看deployment信息</p><p>get查询某个resource的详细信息，describe查询某个resource相关的状态信息</p></li></ul><pre tabindex=0><code>$ kubectl -s 192.168.196.134:8080 get deployment 
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment-nginx   2         2         2            2           7d

$ kubectl -s 192.168.196.134:8080 describe deployment
Name:                   deployment-nginx
Namespace:              default
CreationTimestamp:      Fri, 27 Jul 2018 13:37:02 +0800
Labels:                 app=nginx
Selector:               app=nginx
Replicas:               2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
OldReplicaSets: &lt;none&gt;
NewReplicaSet:  deployment-nginx-2481570099 (2/2 replicas created)
No events.

$ kubectl -s 192.168.196.134:8080 get deployment -o yaml            &lt;--支持导出为json和yaml格式
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: &#34;1&#34;
    creationTimestamp: 2018-07-27T05:37:02Z
    generation: 1
    labels:
      app: nginx
    name: deployment-nginx
    namespace: default
    resourceVersion: &#34;88336&#34;
    selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/deployment-nginx
    uid: 16ed87a7-915f-11e8-a083-000c29815d48
  spec:
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx
      spec:
        containers:
        - image: registry:8088/nginx:latest
          imagePullPolicy: Always
          name: nginx
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: 2018-07-30T05:36:05Z
      lastUpdateTime: 2018-07-30T05:36:05Z
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: &#34;True&#34;
      type: Available
    observedGeneration: 1
    replicas: 2
    updatedReplicas: 2
kind: List
metadata: {}
resourceVersion: &#34;&#34;
selfLink: &#34;&#34;
</code></pre><ul><li>查看pod的信息</li></ul><pre tabindex=0><code># pod所部署的节点及分配的IP
$ kubectl -s 192.168.196.134:8080 get pod -o wide
NAME                                READY     STATUS    RESTARTS   AGE       IP           NODE
deployment-nginx-2481570099-cfkhn   1/1       Running   4          7d        10.55.2.2   node2
deployment-nginx-2481570099-q6xv8   1/1       Running   3          7d        10.55.91.2   node1

#除了通过get/describe查询pod的全部信息，还可以通过template抓取指定key的值
$ kubectl -s 192.168.196.134:8080 get pod -o template \
deployment-nginx-2481570099-cfkhn --template={{.status.podIP}}
10.55.2.2
</code></pre><ul><li>验证flannel网络下pod的连通性</li></ul><pre tabindex=0><code>#登录到node1上
$ docker ps
CONTAINER ID        IMAGE                                   COMMAND                  CREATED             STATUS              PORTS               NAMES
17485262b0a6        registry:8088/nginx:latest              &#34;nginx -g &#39;daemon ...&#34;   5 hours ago         Up 5 hours                              k8s_nginx.4f504e9_deployment-nginx-2481570099-q6xv8_default_16f355db-915f-11e8-a083-000c29815d48_e2f3c24d
dc1833e50415        registry:8088/pod-infrastructure:v3.4   &#34;/pod&#34;                   5 hours ago         Up 5 hours                              k8s_POD.ee70020d_deployment-nginx-2481570099-q6xv8_default_16f355db-915f-11e8-a083-000c29815d48_7c54ed33
#访问本节点上的pod服务
$ curl 10.55.91.2:80   
Welcome to nginx!
# 访问node2节点上的pod服务
$ curl 10.55.2.2:80
Welcome to nginx!

#登录到node2上
$ docker ps
CONTAINER ID        IMAGE                                   COMMAND                  CREATED             STATUS              PORTS               NAMES
c087c12e8afd        registry:8088/nginx:latest              &#34;nginx -g &#39;daemon off&#34;   5 hours ago         Up 5 hours                              k8s_nginx.4f504e9_deployment-nginx-2481570099-cfkhn_default_16f36465-915f-11e8-a083-000c29815d48_80f894f4
195f5d3de8c1        registry:8088/pod-infrastructure:v3.4   &#34;/pod&#34;                   5 hours ago         Up 5 hours                              k8s_POD.ee70020d_deployment-nginx-2481570099-cfkhn_default_16f36465-915f-11e8-a083-000c29815d48_e4b4e2e1
#访问本节点上的pod服务
$ curl 10.55.2.2:80
Welcome to nginx!
# 访问node2节点上的pod服务
$ curl 10.55.91.2:80
Welcome to nginx!

#同样的master节点也配置flanne网络，可访问pod的应用
</code></pre><p>3.部署service示例</p><pre tabindex=0><code>#为部署的pod创建service 
$ cat service-nginx.yml 
kind: Service
apiVersion: v1
metadata:
  name: service-nginx
spec:
  ports:
    - port: 80                                         &lt;-- service的访问端口
      targetPort: 80                                   &lt;-- nginx容器服务端口
  selector:
      app: nginx                                       &lt;-- 以pod标签识别要代理的后端应用
  type: NodePort                                       &lt;-- 负载均衡为轮询

#NodePort模式为 绑定到pod所在Node节点的网卡，对外提供服务，端口为NodePort(不指定时随机分配)
$ kubectl -s 192.168.196.134:8080 create -f service-nginx.yml

#查看service-nginx的详细信息
$ kubectl -s 192.168.196.134:8080 get service -o wide service-nginx
NAME            CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE       SELECTOR
service-nginx   10.254.104.158   &lt;nodes&gt;       80:30284/TCP   7d        app=nginx
$ kubectl -s 192.168.196.134:8080 get service -o yaml service-nginx
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2018-07-27T07:01:04Z
  name: service-nginx
  namespace: default
  resourceVersion: &#34;17086&#34;
  selfLink: /api/v1/namespaces/default/services/service-nginx
  uid: d437faec-916a-11e8-a083-000c29815d48
spec:
  clusterIP: 10.254.104.158                                  &lt;-- 为service分配的随机clusterIP
  ports:
  - nodePort: 30284                                          &lt;--绑定到node节点物理网卡的随机端口
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None                                       &lt;-- 1）
  type: NodePort            
status:
  loadBalancer: {}
1)默认负载策略为轮询，可指定sessionAffinity的值为clientIP实现session绑定

$ kubectl -s 192.168.196.134:8080 describe service service-nginx           
Name:                   service-nginx
Namespace:              default
Labels:                 &lt;none&gt;
Selector:               app=nginx
Type:                   NodePort
IP:                     10.254.104.158
Port:                   &lt;unset&gt; 80/TCP
NodePort:               &lt;unset&gt; 30284/TCP
Endpoints:              10.55.2.2:80,10.55.91.2:80
Session Affinity:       None
No events.

#在任意node节点上访问service-nginx绑定的ClusterIP:Port
$ curl 10.254.104.158:80
Welcome to nginx!
#通过监测两个node的nginx访问日志可以看出是转发策略为轮询

#在任意节点上访问service-naginx绑定在主机网卡上的NodePort
[root@master ~]# curl 192.168.196.136:30284
Welcome to nginx!
[root@master ~]# curl 192.168.196.135:30284
Welcome to nginx!
[root@node1 ~]# curl 192.168.196.135:30284
Welcome to nginx!
[root@node1 ~]# curl 192.168.196.136:30284
Welcome to nginx!
[root@node2 ~]# curl 192.168.196.136:30284
Welcome to nginx!
[root@node2 ~]# curl 192.168.196.135:30284
Welcome to nginx!
</code></pre><p>4.部署kubernetes dashboard
​ github上项目地址https://github.com/kubernetes/dashboard</p><ul><li>由于安装的k8s服务版本为v1.5.2，这里选择dashboard release版本为v1.5.0的镜像，dockerhub上搜索并pull到本地( <a href=https://hub.docker.com/r/ist0ne/kubernetes-dashboard-amd64/ target=_blank>https://hub.docker.com/r/ist0ne/kubernetes-dashboard-amd64/</a> )，打标签推到私有仓库</li><li>获取github上dashboard的部署文件</li></ul><pre tabindex=0><code>$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.5.0/src/deploy/kubernetes-dashboard.yaml
</code></pre><ul><li><p>修改kubernetes-dashboard.yaml文件中</p><ul><li>image: registry:8088/kubernetes-dashboard-amd64:v1.5.0</li><li>&ndash;apiserver-host=http://192.168.196.134:8080</li></ul></li><li><p>部署dashboard</p></li></ul><pre tabindex=0><code>$ kubectl -s 192.168.196.134:8080 create -f kubernetes-dashboard.yaml           
deployment &#34;kubernetes-dashboard&#34; created
service &#34;kubernetes-dashboard&#34; created
</code></pre><blockquote><p>这里要注意，kubernetes-dashboard部署的namespace为kube-system而不是default，需在命令行指定 -n kube-system或者&ndash;namespace=kube-system才能获取到对应的资源信息</p></blockquote><ul><li>浏览器访问192.168.196.134:8080/ui</li></ul><p><figure><img class="my-0 rounded-md" src=https://cdn.jsdelivr.net/gh/selfflying/mypic@main/blog/20180809/dashboard.JPG alt=dashboard></figure></p></div></div><script>var oid="views_posts/Kubernetes集群部署.md",oid_likes="likes_posts/Kubernetes集群部署.md"</script><script type=text/javascript src=/js/page.min.5b1bad196a6075a1e11bae1dc95f24f67b610948a00525e347566c5a62338ea78f1c7224ab9a53873dcdb2a1a7d7d391b8a8ef45561297f68806c01315b0c4f6.js integrity="sha512-WxutGWpgdaHhG64dyV8k9nthCUigBSXjR1ZsWmIzjqePHHIkq5pThz3NsqGn19ORuKjvRVYSl/aIBsATFbDE9g=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/2018/08/docker-tls/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Docker配置TLS安全认证</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-08-08 00:00:00 +0000 UTC">8 August 2018</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/2018/12/docker-dragonfly/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">P2P文件与镜像分发开源解决方案Dragonfly安装部署</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-12-09 00:00:00 +0000 UTC">9 December 2018</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label title>&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" href=/tags/ title=Tags>标签</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2023
FeixiangFu</p><p class="text-xs text-neutral-500 dark:text-neutral-400">由 <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a> 强力驱动</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.62060bb247f4de2b6dde45903668fefb68d792f365587605177b1227c0cf43588701edaca0cb40e2c8e2789bd5ce67c1d2a215b9fb258c3496a7cd25e7cb5fdf.js integrity="sha512-YgYLskf03itt3kWQNmj++2jXkvNlWHYFF3sSJ8DPQ1iHAe2soMtA4sjieJvVzmfB0qIVufsljDSWp80l58tf3w=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://silentkeeping.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title>
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>